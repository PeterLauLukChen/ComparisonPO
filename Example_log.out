Initializing temp weights from base model...

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.76it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.72it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.81it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.79it/s]
Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
Copying scripts to worker nodes...
Starting with dataset offset: 0
===========================================
Processing iteration 0
===========================================
Iteration 0 is in valid list - performing full update
Finding noisy pairs for iteration 0...
2025-04-14 01:02:30,108 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.38it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
2025-04-14 01:02:30,655 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:02:30,908 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:02:30,932 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 01:02:30,933 - __main__ - INFO - Loading dataset
2025-04-14 01:02:31,996 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 01:02:53,511 - __main__ - INFO - Processing dataset
2025-04-14 01:02:53,758 - __main__ - INFO - Searching for noisy pairs starting from index 0
2025-04-14 01:02:54,387 - __main__ - INFO - Processed 0 samples without finding a noisy pair
2025-04-14 01:03:14,719 - __main__ - INFO - Processed 100 samples without finding a noisy pair
2025-04-14 01:03:32,735 - __main__ - INFO - Processed 200 samples without finding a noisy pair
2025-04-14 01:03:51,712 - __main__ - INFO - Processed 300 samples without finding a noisy pair
2025-04-14 01:03:58,412 - __main__ - INFO - Noisy pair found at index 337: chosen_logps=-90.0000, rejected_logps=-92.0000
2025-04-14 01:03:58,413 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 01:03:58,481 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_0
2025-04-14 01:03:58,482 - __main__ - INFO - New dataset offset: 338
Updated dataset offset to: 338 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 0
Worker node 2 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 01:04:14,309 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.21it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.53it/s]2025-04-14 01:04:14,682 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 01:04:14,715 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 01:04:14,858 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]2025-04-14 01:04:15,116 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:04:15,144 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 01:04:15,144 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]
2025-04-14 01:04:15,238 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:04:15,267 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:04:15,507 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:04:15,529 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 01:04:15,529 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:04:15,552 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:04:15,577 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 01:04:15,577 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:04:31,704 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 01:04:32,092 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 01:04:32,123 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 01:04:41,720 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 01:04:42,087 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 01:04:42,539 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 01:04:45,626 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 01:04:45,942 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 01:04:46,338 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 01:04:48,417 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 01:04:48,926 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 01:04:49,411 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 01:04:49,725 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 01:04:50,196 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 01:04:50,278 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 01:04:52,328 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 01:04:53,233 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 01:04:53,331 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 01:04:53,605 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 01:04:54,082 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 01:04:54,398 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 01:04:57,187 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 01:04:57,318 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 01:04:57,351 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 01:04:58,029 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 01:04:58,166 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 01:04:58,640 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 01:05:00,496 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 01:05:01,293 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 01:05:01,543 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 01:05:01,994 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 01:05:02,312 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 01:05:02,566 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 01:05:05,365 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 01:05:05,384 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 01:05:05,439 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 01:05:06,240 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 01:05:06,667 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 01:05:06,745 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 01:05:09,306 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 01:05:09,312 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 01:05:09,592 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 01:05:10,324 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 01:05:10,853 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 01:05:11,105 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 01:05:13,436 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 01:05:13,906 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 01:05:14,010 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 01:05:14,720 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 01:05:14,730 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 01:05:15,261 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 01:05:15,270 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 01:05:15,558 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 01:05:15,566 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 01:05:17,933 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 01:05:17,986 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 01:05:18,051 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 01:05:22,503 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 01:05:22,536 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 01:05:22,538 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 01:05:22,881 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 01:05:22,884 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 01:05:22,920 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 01:06:37,559 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 01:06:40,344 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 01:06:42,638 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 01:06:43,465 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 01:06:43,843 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 01:06:44,212 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 01:06:47,123 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 01:06:49,211 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 01:06:49,646 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 01:06:50,693 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 01:06:51,130 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 01:06:51,408 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 01:06:55,486 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 01:06:55,776 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 01:06:59,105 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 01:06:59,268 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 01:07:00,308 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 01:07:02,889 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 01:07:03,254 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 01:07:03,722 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 01:07:07,124 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 01:07:07,283 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 01:07:09,867 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 01:07:11,012 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 01:07:11,936 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 01:07:12,398 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 01:07:12,833 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 01:07:12,947 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 01:07:15,154 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 01:07:15,615 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 01:08:59.143713821 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x746443da9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7463f91cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7463f91cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7463f91cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7463f91d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7463f91d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x746443f105c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x746444894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x746444926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:08:59.157621788 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7aa6b6c1e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7aa66bfcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7aa66bfcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7aa66bfcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7aa66bfd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7aa66bfd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7aa6b6d855c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7aa6b7694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7aa6b7726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:08:59,521 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_1.npy
2025-04-14 01:08:59,521 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 155 negative comparisons
2025-04-14 01:08:59,702 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_0.npy
2025-04-14 01:08:59,703 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 138 negative comparisons
[E414 01:08:59.628589483 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e99cf468446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e99847cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e99847cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7e99847cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7e99847d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e99847d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e99cf5cf5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e99d0094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e99d0126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:09:02,274 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 01:09:02,989 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 01:09:06.956505790 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f885ad2a446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f880ffcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f880ffcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f880ffcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f880ffd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f880ffd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f885ae915c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f885b894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f885b926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:09:06.978853625 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75cce4b84446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75cc99fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75cc99fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x75cc99fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x75cc99fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75cc99fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x75cce4ceb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x75cce5894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x75cce5926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:09:06,248 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_2.npy
2025-04-14 01:09:06,248 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 144 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 0
2025-04-14 01:09:09,061 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 0
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 01:09:26,096 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.37it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.62it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
2025-04-14 01:09:26,643 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:09:26,898 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:09:26,921 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 01:09:43,392 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 01:09:46,363 - __main__ - INFO - Loaded gradients from node 0: 138 negative comparisons
2025-04-14 01:09:46,604 - __main__ - INFO - Loaded gradients from node 1: 155 negative comparisons
2025-04-14 01:09:46,846 - __main__ - INFO - Loaded gradients from node 2: 144 negative comparisons
2025-04-14 01:09:47,012 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 01:09:47,211 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 01:09:47,410 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133673
2025-04-14 01:09:47,609 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352333
2025-04-14 01:09:47,806 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84835843
2025-04-14 01:09:48,004 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47190464
2025-04-14 01:09:48,202 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22253964
2025-04-14 01:09:48,399 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804738
2025-04-14 01:09:48,597 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552239
2025-04-14 01:09:48,794 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554941
2025-04-14 01:09:49,152 - __main__ - INFO - Non-zero entries after thresholding: 47190464
2025-04-14 01:09:49,152 - __main__ - INFO - Testing stepsize: 0
2025-04-14 01:09:49,794 - __main__ - INFO -   chosen_logps: -90.05240, rejected_logps: -91.92721
2025-04-14 01:09:49,794 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 01:09:50,067 - __main__ - INFO -   chosen_logps: -89.51156, rejected_logps: -92.05190
2025-04-14 01:09:50,067 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 01:09:50,340 - __main__ - INFO -   chosen_logps: -89.03267, rejected_logps: -92.91888
2025-04-14 01:09:50,340 - __main__ - INFO - Testing stepsize: 1
2025-04-14 01:09:50,617 - __main__ - INFO -   chosen_logps: -87.89886, rejected_logps: -93.66566
2025-04-14 01:09:50,617 - __main__ - INFO - Testing stepsize: 2
2025-04-14 01:09:50,897 - __main__ - INFO -   chosen_logps: -85.55960, rejected_logps: -95.22470
2025-04-14 01:09:50,897 - __main__ - INFO - Testing stepsize: 5
2025-04-14 01:09:51,178 - __main__ - INFO -   chosen_logps: -79.63712, rejected_logps: -99.54552
2025-04-14 01:09:51,178 - __main__ - INFO - Testing stepsize: 10
2025-04-14 01:09:51,461 - __main__ - INFO -   chosen_logps: -70.64902, rejected_logps: -107.41225
2025-04-14 01:09:51,462 - __main__ - INFO - Testing stepsize: 15
2025-04-14 01:09:51,746 - __main__ - INFO -   chosen_logps: -62.99085, rejected_logps: -115.07786
2025-04-14 01:09:52,362 - __main__ - INFO - Update scale: 0.008497222222222224
2025-04-14 01:09:52,445 - __main__ - INFO - Model weights updated successfully
2025-04-14 01:09:53,677 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:09:53,713 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:09:54,070 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 01:09:54,080 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 01:09:54,080 - __main__ - INFO - Skipping full model checkpoint for iteration 0
2025-04-14 01:09:54,080 - __main__ - INFO - Iteration 0 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 0
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 0
Iteration 0 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 1
===========================================
Iteration 1 is in valid list - performing full update
Finding noisy pairs for iteration 1...
2025-04-14 01:10:15,563 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 01:10:16,112 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:10:16,251 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:10:16,265 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 01:10:16,265 - __main__ - INFO - Loading dataset
2025-04-14 01:10:17,323 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 01:10:39,131 - __main__ - INFO - Processing dataset
2025-04-14 01:10:39,394 - __main__ - INFO - Searching for noisy pairs starting from index 338
2025-04-14 01:10:46,161 - __main__ - INFO - Noisy pair found at index 369: chosen_logps=-153.0000, rejected_logps=-154.0000
2025-04-14 01:10:46,162 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 01:10:46,231 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_1
2025-04-14 01:10:46,232 - __main__ - INFO - New dataset offset: 370
Updated dataset offset to: 370 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 1
Worker node 2 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 01:11:00,830 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]2025-04-14 01:11:01,109 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 01:11:01,183 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 01:11:01,389 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.22it/s]2025-04-14 01:11:01,528 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:11:01,543 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 01:11:01,543 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 01:11:01,661 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:11:01,739 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:11:01,815 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:11:01,830 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 01:11:01,830 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:11:01,882 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:11:01,894 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 01:11:01,894 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:11:18,083 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 01:11:18,354 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 01:11:18,428 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 01:11:28,333 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 01:11:28,411 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 01:11:28,714 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 01:11:32,158 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 01:11:32,189 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 01:11:33,026 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 01:11:35,025 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 01:11:35,481 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 01:11:35,536 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 01:11:36,075 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 01:11:36,517 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 01:11:37,145 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 01:11:39,188 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 01:11:39,430 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 01:11:40,009 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 01:11:40,445 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 01:11:40,623 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 01:11:41,022 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 01:11:43,339 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 01:11:43,615 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 01:11:43,808 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 01:11:44,377 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 01:11:44,539 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 01:11:44,881 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 01:11:47,153 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 01:11:47,643 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 01:11:47,797 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 01:11:48,355 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 01:11:48,829 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 01:11:48,830 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 01:11:51,189 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 01:11:51,782 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 01:11:51,858 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 01:11:51,973 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 01:11:52,768 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 01:11:53,098 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 01:11:55,091 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 01:11:55,926 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 01:11:56,140 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 01:11:56,339 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 01:11:57,166 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 01:11:57,276 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 01:11:59,089 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 01:12:00,216 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 01:12:00,220 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 01:12:00,227 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 01:12:00,228 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 01:12:01,225 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 01:12:01,229 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 01:12:01,530 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 01:12:01,535 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 01:12:03,370 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 01:12:03,939 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 01:12:04,696 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 01:12:07,268 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 01:12:07,522 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 01:12:08,222 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 01:12:08,311 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 01:12:09,102 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 01:12:09,299 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 01:13:25,543 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 01:13:25,642 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 01:13:27,197 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 01:13:29,499 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 01:13:29,510 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 01:13:33,205 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 01:13:33,861 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 01:13:35,021 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 01:13:37,905 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 01:13:38,371 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 01:13:38,381 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 01:13:38,945 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 01:13:41,768 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 01:13:41,833 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 01:13:44,371 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 01:13:46,823 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 01:13:47,261 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 01:13:48,260 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 01:13:51,463 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 01:13:51,820 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 01:13:52,965 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 01:13:53,020 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 01:13:53,934 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 01:13:56,986 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 01:13:57,276 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 01:13:58,077 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 01:13:58,704 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 01:13:59,394 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 01:13:59,653 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 01:14:02,040 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 01:15:44.370311082 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78548cc64446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x785441fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x785441fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x785441fcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x785441fd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x785441fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78548cdcb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78548d894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78548d926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:15:44,653 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_0.npy
2025-04-14 01:15:44,653 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 121 negative comparisons
2025-04-14 01:15:47,756 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 01:15:48,025 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_1.npy
2025-04-14 01:15:48,025 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 128 negative comparisons
2025-04-14 01:15:50,843 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 01:15:53.940164690 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x723d1c05c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x723cd13cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x723cd13cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x723cd13cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x723cd13d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x723cd13d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x723d1c1c35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x723d1cc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x723d1cd26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:15:53,207 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_2.npy
2025-04-14 01:15:53,207 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 110 negative comparisons
Waiting for worker nodes to complete gradient computation...
2025-04-14 01:15:56,069 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 1
Worker node 2 successfully completed gradient computation for iteration 1
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 01:16:12,617 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 01:16:13,164 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:16:13,301 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:16:13,314 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 01:16:29,740 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 01:16:32,695 - __main__ - INFO - Loaded gradients from node 0: 121 negative comparisons
2025-04-14 01:16:32,932 - __main__ - INFO - Loaded gradients from node 1: 128 negative comparisons
2025-04-14 01:16:33,167 - __main__ - INFO - Loaded gradients from node 2: 110 negative comparisons
2025-04-14 01:16:33,329 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 01:16:33,521 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 01:16:33,714 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133023
2025-04-14 01:16:33,907 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107351090
2025-04-14 01:16:34,100 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84839441
2025-04-14 01:16:34,293 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47191674
2025-04-14 01:16:34,486 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22258420
2025-04-14 01:16:34,680 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806689
2025-04-14 01:16:34,873 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551130
2025-04-14 01:16:35,066 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553782
2025-04-14 01:16:35,416 - __main__ - INFO - Non-zero entries after thresholding: 47191674
2025-04-14 01:16:35,416 - __main__ - INFO - Using relaxed comparison (count_negative: 359)
2025-04-14 01:16:35,416 - __main__ - INFO - Testing stepsize: 0
2025-04-14 01:16:36,047 - __main__ - INFO -   chosen_logps: -152.86061, rejected_logps: -153.86920
2025-04-14 01:16:36,047 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 01:16:36,310 - __main__ - INFO -   chosen_logps: -152.68530, rejected_logps: -153.89931
2025-04-14 01:16:36,310 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 01:16:36,576 - __main__ - INFO -   chosen_logps: -152.07216, rejected_logps: -154.22185
2025-04-14 01:16:36,576 - __main__ - INFO - Testing stepsize: 1
2025-04-14 01:16:36,843 - __main__ - INFO -   chosen_logps: -151.47177, rejected_logps: -154.87210
2025-04-14 01:16:36,843 - __main__ - INFO - Testing stepsize: 2
2025-04-14 01:16:37,115 - __main__ - INFO -   chosen_logps: -150.59970, rejected_logps: -155.84952
2025-04-14 01:16:37,115 - __main__ - INFO - Testing stepsize: 5
2025-04-14 01:16:37,388 - __main__ - INFO -   chosen_logps: -147.30368, rejected_logps: -158.58670
2025-04-14 01:16:37,388 - __main__ - INFO - Testing stepsize: 10
2025-04-14 01:16:37,663 - __main__ - INFO -   chosen_logps: -142.13873, rejected_logps: -163.39264
2025-04-14 01:16:37,663 - __main__ - INFO - Testing stepsize: 15
2025-04-14 01:16:37,938 - __main__ - INFO -   chosen_logps: -138.47517, rejected_logps: -168.73531
2025-04-14 01:16:38,536 - __main__ - INFO - Update scale: 0.006980555555555557
2025-04-14 01:16:38,537 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 01:16:38,620 - __main__ - INFO - Model weights updated successfully
2025-04-14 01:16:39,453 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:16:39,489 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:16:39,868 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 01:16:39,877 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 01:16:39,877 - __main__ - INFO - Skipping full model checkpoint for iteration 1
2025-04-14 01:16:39,877 - __main__ - INFO - Iteration 1 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 1
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 1
Iteration 1 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 2
===========================================
Iteration 2 is in valid list - performing full update
Finding noisy pairs for iteration 2...
2025-04-14 01:17:00,730 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 01:17:01,281 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:17:01,425 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:17:01,439 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 01:17:01,439 - __main__ - INFO - Loading dataset
2025-04-14 01:17:02,724 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 01:17:24,576 - __main__ - INFO - Processing dataset
2025-04-14 01:17:24,839 - __main__ - INFO - Searching for noisy pairs starting from index 370
2025-04-14 01:17:31,073 - __main__ - INFO - Processed 30 samples without finding a noisy pair
2025-04-14 01:17:51,579 - __main__ - INFO - Processed 130 samples without finding a noisy pair
2025-04-14 01:18:13,046 - __main__ - INFO - Processed 230 samples without finding a noisy pair
2025-04-14 01:18:32,355 - __main__ - INFO - Processed 330 samples without finding a noisy pair
2025-04-14 01:18:52,619 - __main__ - INFO - Processed 430 samples without finding a noisy pair
2025-04-14 01:19:12,256 - __main__ - INFO - Processed 530 samples without finding a noisy pair
2025-04-14 01:19:30,887 - __main__ - INFO - Processed 630 samples without finding a noisy pair
2025-04-14 01:19:36,359 - __main__ - INFO - Noisy pair found at index 1030: chosen_logps=-280.0000, rejected_logps=-282.0000
2025-04-14 01:19:36,363 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 01:19:36,432 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_2
2025-04-14 01:19:36,433 - __main__ - INFO - New dataset offset: 1031
Updated dataset offset to: 1031 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 2
Worker node 2 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 01:19:51,832 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.68it/s]2025-04-14 01:19:52,223 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 01:19:52,244 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 01:19:52,379 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]2025-04-14 01:19:52,520 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:19:52,535 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 01:19:52,535 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
2025-04-14 01:19:52,779 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:19:52,798 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:19:52,921 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:19:52,933 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 01:19:52,933 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:19:52,952 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:19:52,967 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 01:19:52,967 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:20:09,052 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 01:20:09,478 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 01:20:09,549 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 01:20:19,132 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 01:20:19,367 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 01:20:19,534 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 01:20:22,962 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 01:20:23,231 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 01:20:23,369 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 01:20:26,115 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 01:20:26,163 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 01:20:26,225 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 01:20:26,918 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 01:20:27,207 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 01:20:27,319 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 01:20:30,223 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 01:20:30,593 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 01:20:30,623 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 01:20:30,911 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 01:20:31,021 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 01:20:31,120 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 01:20:34,072 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 01:20:34,344 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 01:20:34,464 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 01:20:34,822 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 01:20:34,897 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 01:20:35,000 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 01:20:38,156 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 01:20:38,184 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 01:20:38,406 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 01:20:38,975 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 01:20:39,032 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 01:20:39,289 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 01:20:41,747 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 01:20:41,972 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 01:20:42,240 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 01:20:42,990 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 01:20:43,115 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 01:20:43,245 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 01:20:46,248 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 01:20:46,265 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 01:20:46,307 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 01:20:47,033 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 01:20:47,460 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 01:20:47,565 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 01:20:50,246 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 01:20:50,269 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 01:20:50,523 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 01:20:51,465 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 01:20:51,469 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 01:20:51,693 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 01:20:51,698 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 01:20:52,458 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 01:20:52,465 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 01:20:54,460 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 01:20:54,671 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 01:20:55,075 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 01:20:58,674 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 01:20:58,681 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 01:20:58,880 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 01:20:59,133 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 01:21:00,363 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 01:21:00,726 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 01:22:15,492 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 01:22:19,044 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 01:22:19,237 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 01:22:20,444 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 01:22:20,625 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 01:22:20,645 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 01:22:23,506 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 01:22:24,866 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 01:22:26,884 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 01:22:28,204 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 01:22:28,448 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 01:22:31,164 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 01:22:31,675 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 01:22:32,321 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 01:22:34,958 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 01:22:35,809 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 01:22:36,227 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 01:22:38,771 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 01:22:39,686 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 01:22:40,019 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 01:22:40,742 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 01:22:43,342 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 01:22:44,484 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 01:22:44,638 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 01:22:47,463 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 01:22:47,730 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 01:22:47,771 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 01:22:48,212 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 01:22:50,072 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 01:22:52,983 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 01:24:34.752959065 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b41d3d6c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b41895cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b41895cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7b41895cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7b41895d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b41895d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b41d429f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b41d4c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b41d4d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:24:34.777287098 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e79de6fb446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e79939cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e79939cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7e79939cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7e79939d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e79939d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e79de8625c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e79df294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e79df326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:24:34.778051388 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7982cf484446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7982847cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7982847cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7982847cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7982847d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7982847d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7982cf5eb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7982d0094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7982d0126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:24:34.782569946 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76603d6a9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x765ff29cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x765ff29cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x765ff29cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x765ff29d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x765ff29d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x76603d8105c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x76603e294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x76603e326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:24:35,059 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_0.npy
2025-04-14 01:24:35,059 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 142 negative comparisons
2025-04-14 01:24:35,731 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_1.npy
2025-04-14 01:24:35,732 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 123 negative comparisons
2025-04-14 01:24:37,773 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 01:24:38,689 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 01:24:42.343035882 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b2c18693446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b2bcd9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b2bcd9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7b2bcd9cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7b2bcd9d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b2bcd9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b2c187ee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b2c19294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b2c19326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:24:42,536 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_2.npy
2025-04-14 01:24:42,536 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 139 negative comparisons
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 2
2025-04-14 01:24:45,532 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 2
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 01:25:01,663 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 01:25:02,211 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:25:02,358 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:25:02,371 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 01:25:18,796 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 01:25:21,648 - __main__ - INFO - Loaded gradients from node 0: 142 negative comparisons
2025-04-14 01:25:21,879 - __main__ - INFO - Loaded gradients from node 1: 123 negative comparisons
2025-04-14 01:25:22,113 - __main__ - INFO - Loaded gradients from node 2: 139 negative comparisons
2025-04-14 01:25:22,271 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 01:25:22,460 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 01:25:22,649 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119136898
2025-04-14 01:25:22,837 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107348925
2025-04-14 01:25:23,025 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84839829
2025-04-14 01:25:23,212 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47196246
2025-04-14 01:25:23,399 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22253816
2025-04-14 01:25:23,586 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803841
2025-04-14 01:25:23,773 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550599
2025-04-14 01:25:23,959 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554480
2025-04-14 01:25:24,300 - __main__ - INFO - Non-zero entries after thresholding: 47196246
2025-04-14 01:25:24,301 - __main__ - INFO - Testing stepsize: 0
2025-04-14 01:25:24,917 - __main__ - INFO -   chosen_logps: -279.89786, rejected_logps: -281.86896
2025-04-14 01:25:24,917 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 01:25:25,166 - __main__ - INFO -   chosen_logps: -279.37476, rejected_logps: -282.02954
2025-04-14 01:25:25,166 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 01:25:25,415 - __main__ - INFO -   chosen_logps: -277.68661, rejected_logps: -282.76660
2025-04-14 01:25:25,415 - __main__ - INFO - Testing stepsize: 1
2025-04-14 01:25:25,667 - __main__ - INFO -   chosen_logps: -276.07587, rejected_logps: -284.12250
2025-04-14 01:25:25,667 - __main__ - INFO - Testing stepsize: 2
2025-04-14 01:25:25,922 - __main__ - INFO -   chosen_logps: -271.71606, rejected_logps: -286.12073
2025-04-14 01:25:25,922 - __main__ - INFO - Testing stepsize: 5
2025-04-14 01:25:26,179 - __main__ - INFO -   chosen_logps: -260.95404, rejected_logps: -291.93805
2025-04-14 01:25:26,179 - __main__ - INFO - Testing stepsize: 10
2025-04-14 01:25:26,439 - __main__ - INFO -   chosen_logps: -244.23984, rejected_logps: -303.29889
2025-04-14 01:25:26,439 - __main__ - INFO - Testing stepsize: 15
2025-04-14 01:25:26,699 - __main__ - INFO -   chosen_logps: -229.72726, rejected_logps: -315.17313
2025-04-14 01:25:27,268 - __main__ - INFO - Update scale: 0.007855555555555557
2025-04-14 01:25:27,350 - __main__ - INFO - Model weights updated successfully
2025-04-14 01:25:28,228 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:25:28,265 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:25:28,623 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 01:25:28,633 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 01:25:28,633 - __main__ - INFO - Skipping full model checkpoint for iteration 2
2025-04-14 01:25:28,633 - __main__ - INFO - Iteration 2 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 2
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 2
Iteration 2 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 3
===========================================
Iteration 3 is in valid list - performing full update
Finding noisy pairs for iteration 3...
2025-04-14 01:25:49,657 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 01:25:50,206 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:25:50,350 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:25:50,364 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 01:25:50,365 - __main__ - INFO - Loading dataset
2025-04-14 01:25:51,197 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 01:26:13,111 - __main__ - INFO - Processing dataset
2025-04-14 01:26:13,376 - __main__ - INFO - Searching for noisy pairs starting from index 1031
2025-04-14 01:26:28,389 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 01:26:41,977 - __main__ - INFO - Noisy pair found at index 1163: chosen_logps=-147.0000, rejected_logps=-149.0000
2025-04-14 01:26:41,978 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 01:26:42,047 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_3
2025-04-14 01:26:42,048 - __main__ - INFO - New dataset offset: 1164
Updated dataset offset to: 1164 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 3
Worker node 2 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 01:26:57,526 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.22it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]2025-04-14 01:26:57,941 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
2025-04-14 01:26:57,984 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 01:26:58,075 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 01:26:58,220 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:26:58,235 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 01:26:58,235 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 01:26:58,493 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:26:58,540 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:26:58,645 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:26:58,660 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 01:26:58,661 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:26:58,682 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:26:58,695 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 01:26:58,695 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:27:14,743 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 01:27:15,265 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 01:27:15,274 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 01:27:24,891 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 01:27:25,456 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 01:27:25,550 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 01:27:28,754 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 01:27:29,279 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 01:27:29,467 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 01:27:31,756 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 01:27:32,445 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 01:27:32,632 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 01:27:32,648 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 01:27:33,399 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 01:27:33,797 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 01:27:36,184 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 01:27:36,522 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 01:27:36,825 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 01:27:36,950 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 01:27:37,213 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 01:27:37,633 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 01:27:39,806 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 01:27:40,681 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 01:27:40,911 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 01:27:41,222 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 01:27:41,331 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 01:27:41,534 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 01:27:43,920 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 01:27:44,664 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 01:27:45,041 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 01:27:45,210 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 01:27:45,312 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 01:27:45,527 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 01:27:48,232 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 01:27:48,637 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 01:27:48,937 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 01:27:49,519 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 01:27:49,539 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 01:27:50,190 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 01:27:52,480 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 01:27:52,669 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 01:27:53,304 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 01:27:53,807 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 01:27:54,214 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 01:27:54,366 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 01:27:56,797 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 01:27:57,523 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 01:27:57,796 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 01:27:57,871 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 01:27:57,881 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 01:27:58,788 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 01:27:58,793 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 01:27:58,852 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 01:27:58,860 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 01:28:01,348 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 01:28:01,528 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 01:28:02,437 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 01:28:05,587 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 01:28:05,857 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 01:28:06,102 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 01:28:06,402 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 01:28:07,233 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 01:28:07,623 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 01:29:28,600 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 01:29:29,556 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 01:29:31,842 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 01:29:33,197 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 01:29:34,330 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 01:29:36,586 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 01:29:37,740 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 01:29:37,892 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 01:29:39,014 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 01:29:40,819 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 01:29:41,993 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 01:29:44,579 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 01:29:45,866 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 01:29:49,319 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 01:29:49,861 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 01:29:51,052 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 01:29:51,235 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 01:29:52,465 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 01:29:54,290 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 01:29:55,083 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 01:29:57,053 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 01:29:58,210 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 01:29:58,645 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 01:30:02,141 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 01:30:02,630 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 01:30:02,846 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 01:30:03,704 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 01:30:06,526 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 01:30:06,954 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 01:30:12,292 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
[E414 01:32:03.520732741 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c496aa71446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c491fdcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c491fdcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7c491fdcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7c491fdd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c491fdd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c496abd85c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c496b694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c496b726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:32:03.523814994 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78d5c015c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78d5755cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78d5755cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x78d5755cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x78d5755d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78d5755d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78d5c02c35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78d5c0c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78d5c0d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:32:03.668971419 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c60c95ea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c607e9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c607e9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7c607e9cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7c607e9d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c607e9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c60c97515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c60ca294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c60ca326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:32:03.675305300 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7dd2ffe5c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7dd2b51cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7dd2b51cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7dd2b51cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7dd2b51d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7dd2b51d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7dd2fffc35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7dd300a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7dd300b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:32:03,865 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_1.npy
2025-04-14 01:32:03,866 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 159 negative comparisons
2025-04-14 01:32:04,021 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_2.npy
2025-04-14 01:32:04,022 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 151 negative comparisons
2025-04-14 01:32:06,615 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 01:32:07,076 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 3
Worker node 2 successfully completed gradient computation for iteration 3
[E414 01:32:14.385105782 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70d7e29dc446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70d797dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70d797dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x70d797dcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x70d797dd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70d797dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70d7e2b435c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70d7e3494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70d7e3526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:32:14.389345564 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75ba50ff1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75ba063cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75ba063cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x75ba063cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x75ba063d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75ba063d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x75ba511585c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x75ba51a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x75ba51b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:32:14,572 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_0.npy
2025-04-14 01:32:14,573 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 151 negative comparisons
2025-04-14 01:32:17,420 - __main__ - INFO - Node 0: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 01:32:34,834 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 01:32:35,381 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:32:35,521 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:32:35,534 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 01:32:51,986 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 01:32:54,971 - __main__ - INFO - Loaded gradients from node 0: 151 negative comparisons
2025-04-14 01:32:55,213 - __main__ - INFO - Loaded gradients from node 1: 159 negative comparisons
2025-04-14 01:32:55,456 - __main__ - INFO - Loaded gradients from node 2: 151 negative comparisons
2025-04-14 01:32:55,623 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 01:32:55,820 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 01:32:56,018 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119129718
2025-04-14 01:32:56,215 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107347625
2025-04-14 01:32:56,413 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84833289
2025-04-14 01:32:56,610 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47201546
2025-04-14 01:32:56,807 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22256685
2025-04-14 01:32:57,005 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806780
2025-04-14 01:32:57,202 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1548900
2025-04-14 01:32:57,400 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553834
2025-04-14 01:32:57,757 - __main__ - INFO - Non-zero entries after thresholding: 47201546
2025-04-14 01:32:57,757 - __main__ - INFO - Testing stepsize: 0
2025-04-14 01:32:58,513 - __main__ - INFO -   chosen_logps: -147.82297, rejected_logps: -147.74109
2025-04-14 01:32:58,513 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 01:32:58,900 - __main__ - INFO -   chosen_logps: -147.62329, rejected_logps: -147.87654
2025-04-14 01:32:58,900 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 01:32:59,288 - __main__ - INFO -   chosen_logps: -145.85782, rejected_logps: -148.42238
2025-04-14 01:32:59,288 - __main__ - INFO - Testing stepsize: 1
2025-04-14 01:32:59,679 - __main__ - INFO -   chosen_logps: -144.49490, rejected_logps: -148.74994
2025-04-14 01:32:59,679 - __main__ - INFO - Testing stepsize: 2
2025-04-14 01:33:00,075 - __main__ - INFO -   chosen_logps: -141.66508, rejected_logps: -149.76230
2025-04-14 01:33:00,075 - __main__ - INFO - Testing stepsize: 5
2025-04-14 01:33:00,472 - __main__ - INFO -   chosen_logps: -133.15421, rejected_logps: -153.35860
2025-04-14 01:33:00,472 - __main__ - INFO - Testing stepsize: 10
2025-04-14 01:33:00,872 - __main__ - INFO -   chosen_logps: -121.28802, rejected_logps: -158.82674
2025-04-14 01:33:00,872 - __main__ - INFO - Testing stepsize: 15
2025-04-14 01:33:01,272 - __main__ - INFO -   chosen_logps: -112.35298, rejected_logps: -164.54202
2025-04-14 01:33:02,052 - __main__ - INFO - Update scale: 0.00896388888888889
2025-04-14 01:33:02,135 - __main__ - INFO - Model weights updated successfully
2025-04-14 01:33:02,974 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:33:03,009 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:33:03,382 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 01:33:03,391 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 01:33:03,391 - __main__ - INFO - Skipping full model checkpoint for iteration 3
2025-04-14 01:33:03,391 - __main__ - INFO - Iteration 3 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 3
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 3
Iteration 3 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 4
===========================================
Iteration 4 is in valid list - performing full update
Finding noisy pairs for iteration 4...
2025-04-14 01:33:24,058 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 01:33:24,604 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:33:24,749 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:33:24,763 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 01:33:24,763 - __main__ - INFO - Loading dataset
2025-04-14 01:33:25,597 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 01:33:47,427 - __main__ - INFO - Processing dataset
2025-04-14 01:33:47,678 - __main__ - INFO - Searching for noisy pairs starting from index 1164
2025-04-14 01:33:54,589 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 01:34:15,938 - __main__ - INFO - Processed 136 samples without finding a noisy pair
2025-04-14 01:34:26,059 - __main__ - INFO - Noisy pair found at index 1344: chosen_logps=-268.0000, rejected_logps=-270.0000
2025-04-14 01:34:26,060 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 01:34:26,127 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_4
2025-04-14 01:34:26,127 - __main__ - INFO - New dataset offset: 1345
Updated dataset offset to: 1345 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 4
Worker node 2 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 01:34:41,643 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.49it/s]2025-04-14 01:34:41,980 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 01:34:41,992 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.32it/s]2025-04-14 01:34:42,202 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.35it/s]2025-04-14 01:34:42,345 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:34:42,361 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 01:34:42,361 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]
2025-04-14 01:34:42,537 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:34:42,545 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:34:42,680 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:34:42,693 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 01:34:42,693 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:34:42,699 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:34:42,714 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 01:34:42,715 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:34:58,863 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 01:34:59,273 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 01:34:59,316 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 01:35:08,923 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 01:35:09,438 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 01:35:09,948 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 01:35:12,739 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 01:35:13,291 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 01:35:13,846 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 01:35:15,603 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 01:35:16,285 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 01:35:16,658 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 01:35:16,938 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 01:35:17,237 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 01:35:17,798 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 01:35:20,172 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 01:35:20,374 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 01:35:20,456 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 01:35:20,971 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 01:35:21,127 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 01:35:21,693 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 01:35:23,928 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 01:35:24,277 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 01:35:24,557 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 01:35:24,764 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 01:35:25,348 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 01:35:25,571 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 01:35:27,700 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 01:35:28,264 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 01:35:28,646 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 01:35:28,709 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 01:35:29,111 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 01:35:29,574 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 01:35:31,432 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 01:35:31,829 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 01:35:33,066 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 01:35:33,083 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 01:35:33,081 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 01:35:33,898 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 01:35:36,130 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 01:35:36,234 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 01:35:37,198 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 01:35:37,463 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 01:35:37,529 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 01:35:38,008 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 01:35:40,252 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 01:35:40,907 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 01:35:41,368 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 01:35:41,388 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 01:35:41,398 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 01:35:41,601 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 01:35:41,610 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 01:35:42,171 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 01:35:42,172 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 01:35:44,531 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 01:35:44,922 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 01:35:45,539 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 01:35:48,456 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 01:35:48,586 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 01:35:49,066 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 01:35:49,473 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 01:35:50,364 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 01:35:50,392 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 01:37:07,078 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 01:37:08,384 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 01:37:10,266 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 01:37:11,533 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 01:37:12,071 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 01:37:12,374 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 01:37:15,489 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 01:37:17,011 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 01:37:18,708 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 01:37:19,265 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 01:37:21,663 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 01:37:23,650 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 01:37:24,748 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 01:37:25,091 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 01:37:27,177 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 01:37:29,290 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 01:37:30,299 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 01:37:30,327 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 01:37:30,499 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 01:37:31,556 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 01:37:36,018 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 01:37:36,859 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 01:37:37,207 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 01:37:39,469 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 01:37:39,982 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 01:37:39,990 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 01:37:41,432 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 01:37:42,346 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 01:37:45,266 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 01:37:49,875 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
[E414 01:39:28.351500418 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ae741b6c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ae6f73cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ae6f73cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7ae6f73cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7ae6f73d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ae6f73d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7ae74209f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7ae742a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7ae742b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:39:28,711 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_1.npy
2025-04-14 01:39:28,711 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 174 negative comparisons
2025-04-14 01:39:31,609 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 01:39:37.977702219 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x759d6a238446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x759d1f5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x759d1f5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x759d1f5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x759d1f5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x759d1f5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x759d6a39f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x759d6ae94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x759d6af26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:39:37,304 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_2.npy
2025-04-14 01:39:37,305 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 198 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 4
2025-04-14 01:39:40,208 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 4
2025-04-14 01:39:46,678 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_0.npy
2025-04-14 01:39:46,678 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 185 negative comparisons
[E414 01:39:46.618698712 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d8b8b298446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7d8b405cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7d8b405cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7d8b405cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7d8b405d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d8b405d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7d8b8b3ff5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7d8b8be94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7d8b8bf26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:39:46.633627485 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x772e8e6f1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x772e439cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x772e439cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x772e439cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x772e439d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x772e439d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x772e8e8585c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x772e8f294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x772e8f326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:39:49,874 - __main__ - INFO - Node 0: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 01:40:08,424 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 01:40:08,974 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:40:09,114 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:40:09,127 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 01:40:25,568 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 01:40:28,499 - __main__ - INFO - Loaded gradients from node 0: 185 negative comparisons
2025-04-14 01:40:28,735 - __main__ - INFO - Loaded gradients from node 1: 174 negative comparisons
2025-04-14 01:40:28,974 - __main__ - INFO - Loaded gradients from node 2: 198 negative comparisons
2025-04-14 01:40:29,136 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 01:40:29,330 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 01:40:29,524 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119132314
2025-04-14 01:40:29,719 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107360972
2025-04-14 01:40:29,911 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84843326
2025-04-14 01:40:30,105 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47194642
2025-04-14 01:40:30,298 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22256909
2025-04-14 01:40:30,490 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803202
2025-04-14 01:40:30,683 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551229
2025-04-14 01:40:30,877 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554532
2025-04-14 01:40:31,227 - __main__ - INFO - Non-zero entries after thresholding: 47194642
2025-04-14 01:40:31,228 - __main__ - INFO - Testing stepsize: 0
2025-04-14 01:40:31,883 - __main__ - INFO -   chosen_logps: -267.81482, rejected_logps: -269.53848
2025-04-14 01:40:31,883 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 01:40:32,170 - __main__ - INFO -   chosen_logps: -267.51114, rejected_logps: -269.83557
2025-04-14 01:40:32,170 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 01:40:32,458 - __main__ - INFO -   chosen_logps: -265.63666, rejected_logps: -271.73181
2025-04-14 01:40:32,458 - __main__ - INFO - Testing stepsize: 1
2025-04-14 01:40:32,749 - __main__ - INFO -   chosen_logps: -263.44568, rejected_logps: -273.19495
2025-04-14 01:40:32,749 - __main__ - INFO - Testing stepsize: 2
2025-04-14 01:40:33,045 - __main__ - INFO -   chosen_logps: -259.13446, rejected_logps: -276.36377
2025-04-14 01:40:33,045 - __main__ - INFO - Testing stepsize: 5
2025-04-14 01:40:33,341 - __main__ - INFO -   chosen_logps: -247.12874, rejected_logps: -286.81488
2025-04-14 01:40:33,342 - __main__ - INFO - Testing stepsize: 10
2025-04-14 01:40:33,641 - __main__ - INFO -   chosen_logps: -229.22643, rejected_logps: -307.47681
2025-04-14 01:40:33,641 - __main__ - INFO - Testing stepsize: 15
2025-04-14 01:40:33,940 - __main__ - INFO -   chosen_logps: -213.49066, rejected_logps: -329.26123
2025-04-14 01:40:34,572 - __main__ - INFO - Update scale: 0.010830555555555557
2025-04-14 01:40:34,655 - __main__ - INFO - Model weights updated successfully
2025-04-14 01:40:35,946 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:40:35,987 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:40:36,363 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 01:40:36,372 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 01:40:36,373 - __main__ - INFO - Skipping full model checkpoint for iteration 4
2025-04-14 01:40:36,373 - __main__ - INFO - Iteration 4 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 4
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 4
Iteration 4 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 5
===========================================
Iteration 5 is in valid list - performing full update
Finding noisy pairs for iteration 5...
2025-04-14 01:40:57,396 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 01:40:57,945 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:40:58,089 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:40:58,104 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 01:40:58,104 - __main__ - INFO - Loading dataset
2025-04-14 01:40:59,146 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 01:41:20,883 - __main__ - INFO - Processing dataset
2025-04-14 01:41:21,129 - __main__ - INFO - Searching for noisy pairs starting from index 1345
2025-04-14 01:41:32,343 - __main__ - INFO - Processed 55 samples without finding a noisy pair
2025-04-14 01:41:52,978 - __main__ - INFO - Processed 155 samples without finding a noisy pair
2025-04-14 01:41:58,418 - __main__ - INFO - Noisy pair found at index 1530: chosen_logps=-360.0000, rejected_logps=-362.0000
2025-04-14 01:41:58,419 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 01:41:58,485 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_5
2025-04-14 01:41:58,486 - __main__ - INFO - New dataset offset: 1531
Updated dataset offset to: 1531 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 5
Worker node 1 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 01:42:13,959 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.50it/s]2025-04-14 01:42:14,336 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 01:42:14,354 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
2025-04-14 01:42:14,517 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]2025-04-14 01:42:14,662 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.38it/s]/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:42:14,677 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 01:42:14,677 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]
2025-04-14 01:42:14,888 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:42:14,911 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:42:15,042 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:42:15,057 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 01:42:15,057 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:42:15,052 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:42:15,064 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 01:42:15,064 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:42:31,282 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 01:42:31,594 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 01:42:31,632 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 01:42:41,594 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 01:42:41,702 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 01:42:42,000 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 01:42:45,547 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 01:42:45,589 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 01:42:45,700 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 01:42:48,541 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 01:42:48,627 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 01:42:49,089 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 01:42:49,608 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 01:42:49,626 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 01:42:49,921 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 01:42:52,427 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 01:42:52,920 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 01:42:53,000 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 01:42:53,365 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 01:42:53,388 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 01:42:53,824 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 01:42:56,730 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 01:42:56,916 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 01:42:57,102 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 01:42:57,258 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 01:42:57,581 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 01:42:57,780 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 01:43:00,547 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 01:43:00,663 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 01:43:01,155 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 01:43:01,235 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 01:43:01,777 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 01:43:02,053 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 01:43:04,449 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 01:43:04,795 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 01:43:05,088 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 01:43:05,485 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 01:43:06,137 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 01:43:06,410 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 01:43:08,625 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 01:43:09,164 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 01:43:09,333 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 01:43:09,728 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 01:43:10,375 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 01:43:10,918 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 01:43:12,996 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 01:43:13,181 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 01:43:13,884 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 01:43:14,355 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 01:43:14,356 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 01:43:14,361 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 01:43:14,364 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 01:43:15,456 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 01:43:15,458 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 01:43:17,286 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 01:43:17,460 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 01:43:18,422 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 01:43:21,595 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 01:43:21,759 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 01:43:22,040 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 01:43:22,258 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 01:43:23,082 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 01:43:23,316 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 01:44:40,427 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 01:44:40,985 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 01:44:41,841 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 01:44:44,963 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 01:44:46,283 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 01:44:47,523 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 01:44:49,304 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 01:44:49,737 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 01:44:50,413 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 01:44:52,757 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 01:44:54,592 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 01:44:55,005 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 01:44:56,571 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 01:44:58,749 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 01:44:59,773 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 01:45:00,039 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 01:45:00,879 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 01:45:01,942 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 01:45:03,673 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 01:45:04,932 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 01:45:09,168 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 01:45:09,685 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 01:45:10,115 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 01:45:13,117 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 01:45:13,239 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 01:45:13,580 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 01:45:14,183 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 01:45:14,260 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 01:45:14,774 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 01:45:18,327 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 01:47:02.746477686 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x796240ade446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7961f5dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7961f5dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7961f5dcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7961f5dd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7961f5dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x796240c455c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x796241694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x796241726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:47:03,031 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_0.npy
2025-04-14 01:47:03,031 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 89 negative comparisons
2025-04-14 01:47:03,654 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_1.npy
2025-04-14 01:47:03,654 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 91 negative comparisons
[E414 01:47:03.523728232 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77a8ab1a5446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x77a8605cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x77a8605cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x77a8605cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x77a8605d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x77a8605d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x77a8ab30c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x77a8abc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x77a8abd26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:47:03.550108150 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72b6ae284446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72b6635cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72b6635cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x72b6635cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x72b6635d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72b6635d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72b6ae3eb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72b6aee94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72b6aef26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:47:05,856 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 01:47:06,729 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 01:47:10.047417447 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e8c2ff84446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e8be53cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e8be53cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7e8be53cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7e8be53d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e8be53d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e8c300eb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e8c30c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e8c30d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:47:10.064071932 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x736d216fd446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x736cd69cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x736cd69cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x736cd69cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x736cd69d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x736cd69d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x736d218645c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x736d22294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x736d22326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:47:10.066428871 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72d9045ea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72d8b99cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72d8b99cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x72d8b99cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x72d8b99d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72d8b99d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72d9047515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72d905294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72d905326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:47:10,345 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_2.npy
2025-04-14 01:47:10,346 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 83 negative comparisons
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 5
2025-04-14 01:47:13,053 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 5
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 01:47:30,204 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 01:47:30,753 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:47:30,899 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:47:30,913 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 01:47:47,531 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 01:47:50,443 - __main__ - INFO - Loaded gradients from node 0: 89 negative comparisons
2025-04-14 01:47:50,653 - __main__ - INFO - Loaded gradients from node 1: 91 negative comparisons
2025-04-14 01:47:50,891 - __main__ - INFO - Loaded gradients from node 2: 83 negative comparisons
2025-04-14 01:47:51,053 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 01:47:51,246 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 01:47:51,438 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119137676
2025-04-14 01:47:51,631 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107355286
2025-04-14 01:47:51,824 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84831846
2025-04-14 01:47:52,017 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47196334
2025-04-14 01:47:52,210 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22254820
2025-04-14 01:47:52,403 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803695
2025-04-14 01:47:52,595 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551619
2025-04-14 01:47:52,789 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555111
2025-04-14 01:47:53,139 - __main__ - INFO - Non-zero entries after thresholding: 47196334
2025-04-14 01:47:53,139 - __main__ - INFO - Using relaxed comparison (count_negative: 263)
2025-04-14 01:47:53,139 - __main__ - INFO - Testing stepsize: 0
2025-04-14 01:47:53,818 - __main__ - INFO -   chosen_logps: -360.31857, rejected_logps: -361.56305
2025-04-14 01:47:53,818 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 01:47:54,123 - __main__ - INFO -   chosen_logps: -360.01367, rejected_logps: -361.41467
2025-04-14 01:47:54,123 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 01:47:54,427 - __main__ - INFO -   chosen_logps: -358.31262, rejected_logps: -363.32123
2025-04-14 01:47:54,427 - __main__ - INFO - Testing stepsize: 1
2025-04-14 01:47:54,736 - __main__ - INFO -   chosen_logps: -355.39990, rejected_logps: -363.57861
2025-04-14 01:47:54,736 - __main__ - INFO - Testing stepsize: 2
2025-04-14 01:47:55,047 - __main__ - INFO -   chosen_logps: -350.44012, rejected_logps: -366.23248
2025-04-14 01:47:55,047 - __main__ - INFO - Testing stepsize: 5
2025-04-14 01:47:55,360 - __main__ - INFO -   chosen_logps: -336.02097, rejected_logps: -374.03400
2025-04-14 01:47:55,360 - __main__ - INFO - Testing stepsize: 10
2025-04-14 01:47:55,676 - __main__ - INFO -   chosen_logps: -317.33524, rejected_logps: -386.23767
2025-04-14 01:47:55,676 - __main__ - INFO - Testing stepsize: 15
2025-04-14 01:47:55,992 - __main__ - INFO -   chosen_logps: -301.32968, rejected_logps: -401.39529
2025-04-14 01:47:56,655 - __main__ - INFO - Update scale: 0.005113888888888889
2025-04-14 01:47:56,656 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 01:47:56,739 - __main__ - INFO - Model weights updated successfully
2025-04-14 01:47:57,707 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:47:57,740 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:47:58,119 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 01:47:58,128 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 01:47:58,128 - __main__ - INFO - Skipping full model checkpoint for iteration 5
2025-04-14 01:47:58,128 - __main__ - INFO - Iteration 5 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 5
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 5
Iteration 5 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 6
===========================================
Iteration 6 is in valid list - performing full update
Finding noisy pairs for iteration 6...
2025-04-14 01:48:18,958 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 01:48:19,502 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:48:19,648 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:48:19,662 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 01:48:19,662 - __main__ - INFO - Loading dataset
2025-04-14 01:48:20,769 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 01:48:42,382 - __main__ - INFO - Processing dataset
2025-04-14 01:48:42,628 - __main__ - INFO - Searching for noisy pairs starting from index 1531
2025-04-14 01:48:56,712 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 01:49:01,262 - __main__ - INFO - Noisy pair found at index 1624: chosen_logps=-127.5000, rejected_logps=-130.0000
2025-04-14 01:49:01,263 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 01:49:01,330 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_6
2025-04-14 01:49:01,331 - __main__ - INFO - New dataset offset: 1625
Updated dataset offset to: 1625 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 6
Worker node 2 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 01:49:16,734 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 01:49:17,046 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.50it/s]2025-04-14 01:49:17,093 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]2025-04-14 01:49:17,292 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.29it/s]2025-04-14 01:49:17,436 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:49:17,450 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 01:49:17,451 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 01:49:17,597 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:49:17,648 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:49:17,750 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:49:17,769 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 01:49:17,769 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:49:17,790 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:49:17,802 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 01:49:17,803 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:49:34,045 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 01:49:34,354 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 01:49:34,391 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 01:49:44,450 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 01:49:44,515 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 01:49:44,843 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 01:49:48,219 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 01:49:48,401 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 01:49:48,581 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 01:49:51,418 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 01:49:51,716 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 01:49:52,077 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 01:49:52,173 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 01:49:52,440 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 01:49:52,563 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 01:49:55,676 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 01:49:55,716 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 01:49:55,945 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 01:49:56,067 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 01:49:56,357 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 01:49:56,377 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 01:49:59,595 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 01:49:59,645 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 01:49:59,761 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 01:49:59,991 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 01:50:00,231 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 01:50:00,472 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 01:50:03,170 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 01:50:03,785 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 01:50:03,997 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 01:50:04,072 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 01:50:04,102 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 01:50:04,639 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 01:50:07,238 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 01:50:07,531 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 01:50:07,692 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 01:50:08,380 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 01:50:08,420 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 01:50:08,829 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 01:50:11,368 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 01:50:11,738 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 01:50:12,105 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 01:50:12,530 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 01:50:12,940 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 01:50:13,004 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 01:50:15,466 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 01:50:16,030 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 01:50:16,046 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 01:50:17,004 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 01:50:17,009 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 01:50:17,273 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 01:50:17,279 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 01:50:17,595 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 01:50:17,599 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 01:50:19,770 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 01:50:20,530 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 01:50:20,615 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 01:50:24,380 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 01:50:24,596 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 01:50:24,858 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 01:50:24,922 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 01:50:25,649 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 01:50:26,192 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 01:51:48,520 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 01:51:48,567 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 01:51:48,647 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 01:51:50,656 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 01:51:51,390 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 01:51:55,845 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 01:51:56,582 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 01:51:57,176 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 01:51:59,121 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 01:51:59,458 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 01:51:59,558 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 01:52:03,423 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 01:52:03,710 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 01:52:05,565 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 01:52:06,617 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 01:52:07,265 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 01:52:07,685 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 01:52:12,097 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 01:52:12,174 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 01:52:12,346 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 01:52:12,470 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 01:52:14,477 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 01:52:16,191 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 01:52:17,232 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 01:52:19,913 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 01:52:20,163 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 01:52:20,440 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 01:52:21,323 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 01:52:21,507 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 01:52:25,128 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
[E414 01:54:14.958946215 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77578e068446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7757433cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7757433cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7757433cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7757433d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7757433d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x77578e1cf5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x77578ec94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x77578ed26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:54:14,238 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_0.npy
2025-04-14 01:54:14,238 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 148 negative comparisons
[E414 01:54:15.012893097 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78f218afb446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78f1cddcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78f1cddcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x78f1cddcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x78f1cddd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78f1cddd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78f218c625c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78f219694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78f219726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:54:15,238 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_1.npy
2025-04-14 01:54:15,240 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 164 negative comparisons
2025-04-14 01:54:17,124 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 01:54:18,132 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 01:54:21.052736401 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x757ba9968446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x757b5edcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x757b5edcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x757b5edcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x757b5edd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x757b5edd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x757ba9acf5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x757baa694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x757baa726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 01:54:21.058413520 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7099192fd446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7098ce5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7098ce5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7098ce5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7098ce5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7098ce5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7099194645c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x709919e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x709919f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 01:54:21,349 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_2.npy
2025-04-14 01:54:21,349 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 143 negative comparisons
Waiting for worker nodes to complete gradient computation...
2025-04-14 01:54:24,069 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 6
Worker node 2 successfully completed gradient computation for iteration 6
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 01:54:41,795 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 01:54:42,344 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:54:42,485 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:54:42,497 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 01:54:58,952 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 01:55:01,827 - __main__ - INFO - Loaded gradients from node 0: 148 negative comparisons
2025-04-14 01:55:02,062 - __main__ - INFO - Loaded gradients from node 1: 164 negative comparisons
2025-04-14 01:55:02,296 - __main__ - INFO - Loaded gradients from node 2: 143 negative comparisons
2025-04-14 01:55:02,455 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 01:55:02,644 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 01:55:02,834 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119131433
2025-04-14 01:55:03,023 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107347820
2025-04-14 01:55:03,212 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84832185
2025-04-14 01:55:03,401 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47186569
2025-04-14 01:55:03,591 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255496
2025-04-14 01:55:03,780 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8805668
2025-04-14 01:55:03,969 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551791
2025-04-14 01:55:04,158 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554841
2025-04-14 01:55:04,503 - __main__ - INFO - Non-zero entries after thresholding: 47186569
2025-04-14 01:55:04,503 - __main__ - INFO - Testing stepsize: 0
2025-04-14 01:55:05,234 - __main__ - INFO -   chosen_logps: -127.93053, rejected_logps: -129.48669
2025-04-14 01:55:05,234 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 01:55:05,600 - __main__ - INFO -   chosen_logps: -127.46301, rejected_logps: -129.48621
2025-04-14 01:55:05,600 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 01:55:05,967 - __main__ - INFO -   chosen_logps: -126.00422, rejected_logps: -129.90884
2025-04-14 01:55:05,967 - __main__ - INFO - Testing stepsize: 1
2025-04-14 01:55:06,338 - __main__ - INFO -   chosen_logps: -124.35532, rejected_logps: -130.30981
2025-04-14 01:55:06,338 - __main__ - INFO - Testing stepsize: 2
2025-04-14 01:55:06,712 - __main__ - INFO -   chosen_logps: -120.62510, rejected_logps: -131.11682
2025-04-14 01:55:06,712 - __main__ - INFO - Testing stepsize: 5
2025-04-14 01:55:07,089 - __main__ - INFO -   chosen_logps: -111.70503, rejected_logps: -133.42343
2025-04-14 01:55:07,089 - __main__ - INFO - Testing stepsize: 10
2025-04-14 01:55:07,467 - __main__ - INFO -   chosen_logps: -99.34283, rejected_logps: -137.32140
2025-04-14 01:55:07,467 - __main__ - INFO - Testing stepsize: 15
2025-04-14 01:55:07,850 - __main__ - INFO -   chosen_logps: -89.33717, rejected_logps: -141.45590
2025-04-14 01:55:08,593 - __main__ - INFO - Update scale: 0.008847222222222223
2025-04-14 01:55:08,674 - __main__ - INFO - Model weights updated successfully
2025-04-14 01:55:09,519 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:55:09,555 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:55:09,930 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 01:55:09,940 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 01:55:09,940 - __main__ - INFO - Skipping full model checkpoint for iteration 6
2025-04-14 01:55:09,940 - __main__ - INFO - Iteration 6 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 6
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 6
Iteration 6 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 7
===========================================
Iteration 7 is in valid list - performing full update
Finding noisy pairs for iteration 7...
2025-04-14 01:55:31,102 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 01:55:31,651 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:55:31,796 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 01:55:31,810 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 01:55:31,810 - __main__ - INFO - Loading dataset
2025-04-14 01:55:32,776 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 01:55:54,547 - __main__ - INFO - Processing dataset
2025-04-14 01:55:54,793 - __main__ - INFO - Searching for noisy pairs starting from index 1625
2025-04-14 01:56:02,398 - __main__ - INFO - Noisy pair found at index 1663: chosen_logps=-49.7500, rejected_logps=-50.5000
2025-04-14 01:56:02,399 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 01:56:02,466 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_7
2025-04-14 01:56:02,467 - __main__ - INFO - New dataset offset: 1664
Updated dataset offset to: 1664 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 7
Worker node 1 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 01:56:16,768 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]2025-04-14 01:56:17,180 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 01:56:17,200 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 01:56:17,314 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.24it/s]2025-04-14 01:56:17,458 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:56:17,473 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 01:56:17,473 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.50it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 01:56:17,733 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:56:17,756 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 01:56:17,887 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:56:17,901 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 01:56:17,902 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:56:17,896 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 01:56:17,909 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 01:56:17,909 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 01:56:34,010 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 01:56:34,442 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 01:56:34,464 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 01:56:44,140 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 01:56:44,506 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 01:56:44,564 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 01:56:47,969 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 01:56:48,436 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 01:56:48,691 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 01:56:51,321 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 01:56:51,587 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 01:56:51,807 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 01:56:51,913 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 01:56:52,320 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 01:56:52,622 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 01:56:55,480 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 01:56:55,902 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 01:56:56,175 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 01:56:56,227 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 01:56:56,313 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 01:56:56,456 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 01:56:59,339 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 01:56:59,793 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 01:57:00,051 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 01:57:00,339 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 01:57:00,338 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 01:57:00,372 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 01:57:03,397 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 01:57:03,543 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 01:57:03,576 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 01:57:04,255 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 01:57:04,379 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 01:57:04,397 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 01:57:07,473 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 01:57:07,508 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 01:57:07,855 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 01:57:08,129 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 01:57:08,421 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 01:57:08,784 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 01:57:11,453 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 01:57:12,035 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 01:57:12,043 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 01:57:12,334 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 01:57:12,739 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 01:57:12,755 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 01:57:15,636 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 01:57:16,115 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 01:57:16,126 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 01:57:16,154 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 01:57:16,560 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 01:57:16,694 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 01:57:16,700 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 01:57:17,265 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 01:57:17,273 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 01:57:19,271 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 01:57:20,226 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 01:57:20,575 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 01:57:23,368 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 01:57:23,539 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 01:57:24,231 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 01:57:24,799 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 01:57:24,913 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 01:57:25,450 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 01:58:46,996 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 01:58:51,522 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 01:58:51,939 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 01:58:52,300 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 01:58:53,134 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 01:58:53,340 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 01:58:55,818 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 01:58:57,116 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 01:58:58,126 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 01:58:58,131 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 01:58:59,017 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 01:58:59,964 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 01:59:01,911 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 01:59:02,786 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 01:59:07,079 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 01:59:07,949 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 01:59:08,715 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 01:59:11,507 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 01:59:12,468 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 01:59:12,922 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 01:59:16,766 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 01:59:16,819 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 01:59:18,197 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 01:59:18,580 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 01:59:19,400 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 01:59:19,529 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 01:59:19,964 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 01:59:20,002 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 01:59:20,131 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 01:59:21,599 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 02:01:12,753 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_0.npy
2025-04-14 02:01:12,753 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 78 negative comparisons
[E414 02:01:12.697406914 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7cc4a9adc446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7cc45edcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7cc45edcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7cc45edcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7cc45edd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7cc45edd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7cc4a9c435c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7cc4aa694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7cc4aa726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:01:13.705878865 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x722b15b10446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x722acadcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x722acadcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x722acadcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x722acadd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x722acadd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x722b15c775c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x722b16694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x722b16726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:01:13.721479930 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7edebcd5c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ede721cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ede721cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7ede721cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7ede721d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ede721d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7edebcec35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7edebd894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7edebd926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 02:01:14,115 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_1.npy
2025-04-14 02:01:14,115 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 71 negative comparisons
2025-04-14 02:01:15,746 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 02:01:16,951 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 02:01:17.800831822 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f5b727e3446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f5b27bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f5b27bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f5b27bcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f5b27bd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f5b27bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f5b7294a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f5b73494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f5b73526850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank4]:[E414 02:01:17.801025941 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70430845c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7042bd7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7042bd7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7042bd7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7042bd7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7042bd7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7043085c35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x704309094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x704309126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:01:17.800542290 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71f45ca17446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71f411dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71f411dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x71f411dcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x71f411dd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71f411dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x71f45cb7e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x71f45d694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x71f45d726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:01:17.804038228 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x720eeb6be446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x720ea09cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x720ea09cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x720ea09cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x720ea09d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x720ea09d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x720eeb8255c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x720eec294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x720eec326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 02:01:18,030 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_2.npy
2025-04-14 02:01:18,030 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 77 negative comparisons
2025-04-14 02:01:20,844 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 7
Worker node 2 successfully completed gradient computation for iteration 7
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 02:01:37,544 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 02:01:38,089 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:01:38,231 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:01:38,243 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 02:01:54,749 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 02:01:57,694 - __main__ - INFO - Loaded gradients from node 0: 78 negative comparisons
2025-04-14 02:01:57,930 - __main__ - INFO - Loaded gradients from node 1: 71 negative comparisons
2025-04-14 02:01:58,167 - __main__ - INFO - Loaded gradients from node 2: 77 negative comparisons
2025-04-14 02:01:58,328 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 02:01:58,519 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 02:01:58,711 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119136060
2025-04-14 02:01:58,902 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107349808
2025-04-14 02:01:59,092 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84831109
2025-04-14 02:01:59,283 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47197346
2025-04-14 02:01:59,473 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22254227
2025-04-14 02:01:59,663 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802957
2025-04-14 02:01:59,853 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552786
2025-04-14 02:02:00,044 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555228
2025-04-14 02:02:00,391 - __main__ - INFO - Non-zero entries after thresholding: 47197346
2025-04-14 02:02:00,391 - __main__ - INFO - Using relaxed comparison (count_negative: 226)
2025-04-14 02:02:00,391 - __main__ - INFO - Testing stepsize: 0
2025-04-14 02:02:01,118 - __main__ - INFO -   chosen_logps: -49.78365, rejected_logps: -50.12743
2025-04-14 02:02:01,118 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 02:02:01,482 - __main__ - INFO -   chosen_logps: -49.80279, rejected_logps: -50.19093
2025-04-14 02:02:01,483 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 02:02:01,849 - __main__ - INFO -   chosen_logps: -49.46022, rejected_logps: -50.11903
2025-04-14 02:02:01,849 - __main__ - INFO - Testing stepsize: 1
2025-04-14 02:02:02,217 - __main__ - INFO -   chosen_logps: -49.00501, rejected_logps: -50.91869
2025-04-14 02:02:02,217 - __main__ - INFO - Testing stepsize: 2
2025-04-14 02:02:02,587 - __main__ - INFO -   chosen_logps: -48.06370, rejected_logps: -51.28708
2025-04-14 02:02:02,587 - __main__ - INFO - Testing stepsize: 5
2025-04-14 02:02:02,957 - __main__ - INFO -   chosen_logps: -45.67022, rejected_logps: -53.06368
2025-04-14 02:02:02,957 - __main__ - INFO - Testing stepsize: 10
2025-04-14 02:02:03,329 - __main__ - INFO -   chosen_logps: -41.88865, rejected_logps: -56.37111
2025-04-14 02:02:03,329 - __main__ - INFO - Testing stepsize: 15
2025-04-14 02:02:03,702 - __main__ - INFO -   chosen_logps: -39.09070, rejected_logps: -59.71098
2025-04-14 02:02:04,433 - __main__ - INFO - Update scale: 0.004394444444444445
2025-04-14 02:02:04,435 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 02:02:04,515 - __main__ - INFO - Model weights updated successfully
2025-04-14 02:02:05,599 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:02:05,640 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:02:06,163 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 02:02:06,177 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 02:02:06,178 - __main__ - INFO - Skipping full model checkpoint for iteration 7
2025-04-14 02:02:06,178 - __main__ - INFO - Iteration 7 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 7
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 7
Iteration 7 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 8
===========================================
Iteration 8 is in valid list - performing full update
Finding noisy pairs for iteration 8...
2025-04-14 02:02:27,474 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 02:02:28,023 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:02:28,168 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:02:28,182 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 02:02:28,182 - __main__ - INFO - Loading dataset
2025-04-14 02:02:29,209 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 02:02:50,832 - __main__ - INFO - Processing dataset
2025-04-14 02:02:51,077 - __main__ - INFO - Searching for noisy pairs starting from index 1664
2025-04-14 02:02:59,559 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 02:03:08,324 - __main__ - INFO - Noisy pair found at index 1746: chosen_logps=-23.8750, rejected_logps=-24.2500
2025-04-14 02:03:08,325 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 02:03:08,393 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_8
2025-04-14 02:03:08,394 - __main__ - INFO - New dataset offset: 1747
Updated dataset offset to: 1747 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 8
Worker node 2 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 02:03:23,679 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 02:03:23,960 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.24it/s]2025-04-14 02:03:24,190 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 02:03:24,236 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.48it/s]2025-04-14 02:03:24,381 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:03:24,395 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 02:03:24,396 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  5.09it/s]2025-04-14 02:03:24,516 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  5.54it/s]2025-04-14 02:03:24,661 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:03:24,679 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 02:03:24,679 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.99it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.80it/s]
2025-04-14 02:03:24,876 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:03:25,029 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:03:25,044 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 02:03:25,044 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:03:40,918 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 02:03:41,338 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 02:03:41,590 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 02:03:51,037 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 02:03:51,194 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 02:03:51,540 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 02:03:54,799 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 02:03:55,044 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 02:03:55,281 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 02:03:58,171 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 02:03:58,267 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 02:03:58,653 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 02:03:58,892 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 02:03:59,019 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 02:03:59,261 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 02:04:02,134 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 02:04:02,581 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 02:04:02,575 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 02:04:02,791 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 02:04:02,966 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 02:04:03,327 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 02:04:06,521 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 02:04:06,641 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 02:04:06,658 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 02:04:06,833 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 02:04:07,070 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 02:04:07,323 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 02:04:10,256 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 02:04:10,466 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 02:04:10,472 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 02:04:11,202 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 02:04:11,235 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 02:04:11,423 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 02:04:14,375 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 02:04:14,431 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 02:04:14,504 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 02:04:15,079 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 02:04:15,223 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 02:04:15,346 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 02:04:18,566 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 02:04:18,858 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 02:04:18,957 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 02:04:19,161 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 02:04:19,170 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 02:04:19,413 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 02:04:22,281 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 02:04:22,539 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 02:04:22,862 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 02:04:23,263 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 02:04:23,264 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 02:04:23,466 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 02:04:23,474 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 02:04:23,509 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 02:04:23,512 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 02:04:26,380 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 02:04:26,937 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 02:04:27,198 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 02:04:31,164 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 02:04:31,215 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 02:04:31,499 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 02:04:31,567 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 02:04:31,661 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 02:04:31,722 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 02:05:55,104 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 02:05:57,215 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 02:05:57,915 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 02:05:58,459 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 02:06:00,170 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 02:06:03,111 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 02:06:03,177 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 02:06:04,549 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 02:06:05,376 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 02:06:07,980 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 02:06:08,678 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 02:06:08,926 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 02:06:10,653 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 02:06:11,566 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 02:06:13,079 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 02:06:15,473 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 02:06:16,129 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 02:06:16,484 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 02:06:18,994 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 02:06:19,098 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 02:06:19,891 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 02:06:22,564 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 02:06:24,656 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 02:06:26,220 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 02:06:27,387 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 02:06:28,083 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 02:06:28,315 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 02:06:28,887 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 02:06:30,531 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 02:06:31,560 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 02:08:22,261 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_1.npy
2025-04-14 02:08:22,261 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 105 negative comparisons
[E414 02:08:22.143781377 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7321a7ede446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x73215d1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x73215d1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x73215d1cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x73215d1d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73215d1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7321a80455c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7321a8a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7321a8b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:08:22.154391883 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76e63bce3446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76e5f0fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76e5f0fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x76e5f0fcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x76e5f0fd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76e5f0fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x76e63be4a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x76e63c894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x76e63c926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:08:22.826888693 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x762c22fd7446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x762bd83cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x762bd83cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x762bd83cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x762bd83d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x762bd83d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x762c2313e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x762c23a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x762c23b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:08:22.851678147 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x782a9843a446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x782a4d7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x782a4d7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x782a4d7cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x782a4d7d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x782a4d7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x782a985a15c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x782a99094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x782a99126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 02:08:23,162 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_0.npy
2025-04-14 02:08:23,162 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 103 negative comparisons
2025-04-14 02:08:25,341 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 02:08:26,033 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 02:08:27,566 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_2.npy
2025-04-14 02:08:27,566 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 104 negative comparisons
[E414 02:08:27.516219533 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74c5b6cc9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74c56bfcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74c56bfcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x74c56bfcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x74c56bfd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74c56bfd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74c5b6e305c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74c5b7894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74c5b7926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:08:27.521808323 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x753df4aea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x753da9dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x753da9dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x753da9dcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x753da9dd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x753da9dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x753df4c515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x753df5694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x753df5726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:08:27.524299576 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x738c049a5446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x738bb9dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x738bb9dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x738bb9dcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x738bb9dd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x738bb9dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x738c04b0c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x738c05694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x738c05726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 02:08:30,617 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 8
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 8
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 02:08:48,469 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 02:08:49,013 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:08:49,154 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:08:49,166 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 02:09:05,658 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 02:09:08,637 - __main__ - INFO - Loaded gradients from node 0: 103 negative comparisons
2025-04-14 02:09:08,874 - __main__ - INFO - Loaded gradients from node 1: 105 negative comparisons
2025-04-14 02:09:09,112 - __main__ - INFO - Loaded gradients from node 2: 104 negative comparisons
2025-04-14 02:09:09,274 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 02:09:09,466 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 02:09:09,660 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133323
2025-04-14 02:09:09,853 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107350510
2025-04-14 02:09:10,045 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84826057
2025-04-14 02:09:10,240 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47186236
2025-04-14 02:09:10,434 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252684
2025-04-14 02:09:10,629 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8807048
2025-04-14 02:09:10,823 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552382
2025-04-14 02:09:11,018 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555260
2025-04-14 02:09:11,371 - __main__ - INFO - Non-zero entries after thresholding: 47186236
2025-04-14 02:09:11,371 - __main__ - INFO - Using relaxed comparison (count_negative: 312)
2025-04-14 02:09:11,371 - __main__ - INFO - Testing stepsize: 0
2025-04-14 02:09:12,122 - __main__ - INFO -   chosen_logps: -23.92811, rejected_logps: -23.81994
2025-04-14 02:09:12,122 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 02:09:12,507 - __main__ - INFO -   chosen_logps: -23.91564, rejected_logps: -23.80081
2025-04-14 02:09:12,507 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 02:09:12,892 - __main__ - INFO -   chosen_logps: -23.59365, rejected_logps: -24.25766
2025-04-14 02:09:12,892 - __main__ - INFO - Testing stepsize: 1
2025-04-14 02:09:13,281 - __main__ - INFO -   chosen_logps: -23.22491, rejected_logps: -24.33455
2025-04-14 02:09:13,281 - __main__ - INFO - Testing stepsize: 2
2025-04-14 02:09:13,673 - __main__ - INFO -   chosen_logps: -22.41902, rejected_logps: -25.17517
2025-04-14 02:09:13,673 - __main__ - INFO - Testing stepsize: 5
2025-04-14 02:09:14,067 - __main__ - INFO -   chosen_logps: -19.90719, rejected_logps: -27.34488
2025-04-14 02:09:14,067 - __main__ - INFO - Testing stepsize: 10
2025-04-14 02:09:14,462 - __main__ - INFO -   chosen_logps: -16.35690, rejected_logps: -31.07036
2025-04-14 02:09:14,462 - __main__ - INFO - Testing stepsize: 15
2025-04-14 02:09:14,858 - __main__ - INFO -   chosen_logps: -13.27791, rejected_logps: -34.72033
2025-04-14 02:09:15,626 - __main__ - INFO - Update scale: 0.006066666666666667
2025-04-14 02:09:15,628 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 02:09:15,707 - __main__ - INFO - Model weights updated successfully
2025-04-14 02:09:16,904 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:09:16,943 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:09:17,327 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 02:09:17,337 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 02:09:17,337 - __main__ - INFO - Skipping full model checkpoint for iteration 8
2025-04-14 02:09:17,337 - __main__ - INFO - Iteration 8 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 8
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 8
Iteration 8 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 9
===========================================
Iteration 9 is in valid list - performing full update
Finding noisy pairs for iteration 9...
2025-04-14 02:09:38,564 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 02:09:39,114 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:09:39,259 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:09:39,273 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 02:09:39,273 - __main__ - INFO - Loading dataset
2025-04-14 02:09:40,065 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 02:10:01,584 - __main__ - INFO - Processing dataset
2025-04-14 02:10:01,825 - __main__ - INFO - Searching for noisy pairs starting from index 1747
2025-04-14 02:10:12,098 - __main__ - INFO - Processed 53 samples without finding a noisy pair
2025-04-14 02:10:30,300 - __main__ - INFO - Noisy pair found at index 1892: chosen_logps=-58.0000, rejected_logps=-59.7500
2025-04-14 02:10:30,301 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 02:10:30,367 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_9
2025-04-14 02:10:30,368 - __main__ - INFO - New dataset offset: 1893
Updated dataset offset to: 1893 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 9
Worker node 2 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 02:10:45,537 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]2025-04-14 02:10:45,991 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 02:10:45,996 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 02:10:46,082 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]2025-04-14 02:10:46,226 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:10:46,241 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 02:10:46,241 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 02:10:46,544 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:10:46,550 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:10:46,690 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 02:10:46,696 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:10:46,703 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 02:10:46,703 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:10:46,711 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 02:10:46,711 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:11:02,782 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 02:11:03,350 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 02:11:03,370 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 02:11:12,761 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 02:11:13,384 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 02:11:13,483 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 02:11:16,521 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 02:11:17,113 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 02:11:17,408 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 02:11:19,544 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 02:11:20,559 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 02:11:20,800 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 02:11:20,903 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 02:11:20,914 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 02:11:21,463 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 02:11:23,260 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 02:11:24,375 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 02:11:24,665 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 02:11:24,818 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 02:11:25,169 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 02:11:25,464 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 02:11:28,084 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 02:11:28,356 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 02:11:28,580 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 02:11:28,596 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 02:11:29,059 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 02:11:29,321 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 02:11:32,145 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 02:11:32,529 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 02:11:33,022 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 02:11:33,079 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 02:11:33,100 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 02:11:33,661 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 02:11:35,727 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 02:11:36,335 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 02:11:36,662 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 02:11:36,810 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 02:11:37,333 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 02:11:37,659 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 02:11:39,930 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 02:11:40,699 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 02:11:40,813 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 02:11:41,090 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 02:11:41,300 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 02:11:41,738 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 02:11:43,976 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 02:11:44,787 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 02:11:45,020 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 02:11:45,021 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 02:11:45,480 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 02:11:45,491 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 02:11:45,780 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 02:11:45,806 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 02:11:45,806 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 02:11:48,124 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 02:11:48,845 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 02:11:50,007 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 02:11:52,362 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 02:11:52,408 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 02:11:53,174 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 02:11:53,480 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 02:11:53,506 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 02:11:53,717 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 02:13:15,621 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 02:13:16,961 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 02:13:19,108 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 02:13:21,281 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 02:13:22,510 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 02:13:24,338 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 02:13:26,615 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 02:13:26,678 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 02:13:30,142 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 02:13:30,693 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 02:13:31,387 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 02:13:33,046 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 02:13:33,166 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 02:13:34,008 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 02:13:36,483 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 02:13:37,306 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 02:13:37,527 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 02:13:38,000 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 02:13:41,461 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 02:13:42,374 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 02:13:44,497 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 02:13:44,926 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 02:13:45,802 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 02:13:48,844 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 02:13:49,704 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 02:13:50,238 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 02:13:50,398 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 02:13:50,461 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 02:13:51,141 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 02:13:54,617 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 02:15:45,013 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_0.npy
2025-04-14 02:15:45,013 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 102 negative comparisons
2025-04-14 02:15:45,613 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_1.npy
2025-04-14 02:15:45,613 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 113 negative comparisons
2025-04-14 02:15:48,104 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 02:15:48,672 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 02:15:51.303184819 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f898023f446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f89355cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f89355cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f89355cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f89355d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f89355d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f89803a65c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f8980e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f8980f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:15:51.312577373 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x718c0fb6c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x718bc53cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x718bc53cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x718bc53cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x718bc53d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x718bc53d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x718c1009f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x718c10a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x718c10b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:15:51.316764742 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fb2f79b2446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7fb2acdcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7fb2acdcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7fb2acdcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7fb2acdd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fb2acdd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7fb2f7b195c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7fb2f8694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7fb2f8726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 02:15:51,623 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_2.npy
2025-04-14 02:15:51,624 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 106 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 9
2025-04-14 02:15:54,487 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 9
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 02:16:11,028 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 02:16:11,573 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:16:11,719 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:16:11,731 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 02:16:28,275 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 02:16:31,227 - __main__ - INFO - Loaded gradients from node 0: 102 negative comparisons
2025-04-14 02:16:31,466 - __main__ - INFO - Loaded gradients from node 1: 113 negative comparisons
2025-04-14 02:16:31,705 - __main__ - INFO - Loaded gradients from node 2: 106 negative comparisons
2025-04-14 02:16:31,865 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 02:16:32,055 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 02:16:32,246 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119135769
2025-04-14 02:16:32,437 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107347322
2025-04-14 02:16:32,627 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84834087
2025-04-14 02:16:32,818 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47191514
2025-04-14 02:16:33,009 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22256164
2025-04-14 02:16:33,199 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804298
2025-04-14 02:16:33,390 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551344
2025-04-14 02:16:33,580 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555574
2025-04-14 02:16:33,928 - __main__ - INFO - Non-zero entries after thresholding: 47191514
2025-04-14 02:16:33,928 - __main__ - INFO - Using relaxed comparison (count_negative: 321)
2025-04-14 02:16:33,929 - __main__ - INFO - Testing stepsize: 0
2025-04-14 02:16:34,679 - __main__ - INFO -   chosen_logps: -57.56350, rejected_logps: -59.94020
2025-04-14 02:16:34,679 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 02:16:35,059 - __main__ - INFO -   chosen_logps: -57.27088, rejected_logps: -60.03182
2025-04-14 02:16:35,059 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 02:16:35,439 - __main__ - INFO -   chosen_logps: -56.48803, rejected_logps: -60.43664
2025-04-14 02:16:35,439 - __main__ - INFO - Testing stepsize: 1
2025-04-14 02:16:35,824 - __main__ - INFO -   chosen_logps: -55.61756, rejected_logps: -60.83820
2025-04-14 02:16:35,824 - __main__ - INFO - Testing stepsize: 2
2025-04-14 02:16:36,210 - __main__ - INFO -   chosen_logps: -53.62093, rejected_logps: -61.61544
2025-04-14 02:16:36,210 - __main__ - INFO - Testing stepsize: 5
2025-04-14 02:16:36,598 - __main__ - INFO -   chosen_logps: -48.79432, rejected_logps: -64.25227
2025-04-14 02:16:36,598 - __main__ - INFO - Testing stepsize: 10
2025-04-14 02:16:36,987 - __main__ - INFO -   chosen_logps: -42.20008, rejected_logps: -68.61572
2025-04-14 02:16:36,987 - __main__ - INFO - Testing stepsize: 15
2025-04-14 02:16:37,378 - __main__ - INFO -   chosen_logps: -37.73555, rejected_logps: -72.93164
2025-04-14 02:16:38,141 - __main__ - INFO - Update scale: 0.006241666666666668
2025-04-14 02:16:38,142 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 02:16:38,221 - __main__ - INFO - Model weights updated successfully
2025-04-14 02:16:39,065 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:16:39,102 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:16:39,476 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 02:16:39,486 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 02:16:39,486 - __main__ - INFO - Skipping full model checkpoint for iteration 9
2025-04-14 02:16:39,486 - __main__ - INFO - Iteration 9 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 9
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 9
Iteration 9 completed successfully
Cleaning up iteration files...
Multi-node training completed successfully!
Copying scripts to worker nodes...
Starting with dataset offset: 0
===========================================
Processing iteration 0
===========================================
Iteration 0 is in valid list - performing full update
Finding noisy pairs for iteration 0...
2025-04-14 02:17:03,513 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.75it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.84it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.78it/s]
2025-04-14 02:17:04,050 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:17:04,193 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:17:04,208 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 02:17:04,208 - __main__ - INFO - Loading dataset
2025-04-14 02:17:05,641 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 02:17:27,233 - __main__ - INFO - Processing dataset
2025-04-14 02:17:27,480 - __main__ - INFO - Searching for noisy pairs starting from index 0
2025-04-14 02:17:28,121 - __main__ - INFO - Processed 0 samples without finding a noisy pair
2025-04-14 02:17:48,515 - __main__ - INFO - Processed 100 samples without finding a noisy pair
2025-04-14 02:18:06,650 - __main__ - INFO - Processed 200 samples without finding a noisy pair
2025-04-14 02:18:25,677 - __main__ - INFO - Processed 300 samples without finding a noisy pair
2025-04-14 02:18:32,398 - __main__ - INFO - Noisy pair found at index 337: chosen_logps=-90.0000, rejected_logps=-92.0000
2025-04-14 02:18:32,399 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 02:18:32,465 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_0
2025-04-14 02:18:32,466 - __main__ - INFO - New dataset offset: 338
Updated dataset offset to: 338 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 0
Worker node 1 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 02:18:48,000 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.23it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]2025-04-14 02:18:48,384 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
2025-04-14 02:18:48,491 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 02:18:48,547 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]2025-04-14 02:18:48,692 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:18:48,707 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 02:18:48,707 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 02:18:48,940 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s]
2025-04-14 02:18:49,042 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:18:49,083 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:18:49,096 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 02:18:49,096 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:18:49,195 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:18:49,210 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 02:18:49,210 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:19:05,299 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 02:19:05,655 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 02:19:05,796 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 02:19:15,332 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 02:19:15,698 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 02:19:16,153 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 02:19:19,075 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 02:19:19,558 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 02:19:20,400 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 02:19:22,446 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 02:19:22,634 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 02:19:22,701 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 02:19:22,987 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 02:19:23,720 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 02:19:24,331 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 02:19:26,350 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 02:19:26,431 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 02:19:26,823 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 02:19:27,598 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 02:19:27,925 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 02:19:28,711 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 02:19:30,168 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 02:19:30,576 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 02:19:30,664 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 02:19:31,810 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 02:19:32,254 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 02:19:32,722 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 02:19:34,011 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 02:19:34,529 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 02:19:35,211 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 02:19:35,599 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 02:19:36,588 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 02:19:36,757 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 02:19:37,921 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 02:19:38,628 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 02:19:39,452 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 02:19:40,142 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 02:19:40,683 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 02:19:41,006 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 02:19:41,861 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 02:19:43,015 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 02:19:43,753 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 02:19:43,853 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 02:19:44,714 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 02:19:45,526 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 02:19:45,834 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 02:19:47,619 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 02:19:47,719 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 02:19:47,720 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 02:19:48,261 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 02:19:48,856 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 02:19:48,863 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 02:19:49,677 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 02:19:49,682 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 02:19:50,219 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 02:19:51,854 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 02:19:53,440 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 02:19:54,971 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 02:19:55,587 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 02:19:56,156 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 02:19:56,200 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 02:19:57,625 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 02:19:57,668 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 02:21:12,972 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 02:21:13,153 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 02:21:13,326 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 02:21:17,104 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 02:21:18,073 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 02:21:19,363 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 02:21:21,574 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 02:21:22,004 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 02:21:23,646 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 02:21:24,669 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 02:21:24,935 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 02:21:26,389 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 02:21:28,728 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 02:21:31,590 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 02:21:31,851 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 02:21:33,386 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 02:21:34,441 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 02:21:34,637 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 02:21:37,685 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 02:21:39,848 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 02:21:40,804 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 02:21:41,780 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 02:21:41,982 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 02:21:44,174 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 02:21:44,853 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 02:21:46,460 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 02:21:46,538 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 02:21:46,888 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 02:21:50,366 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 02:21:52,050 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 02:23:33,755 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_0.npy
2025-04-14 02:23:33,755 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 124 negative comparisons
2025-04-14 02:23:34,212 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_1.npy
2025-04-14 02:23:34,213 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 150 negative comparisons
2025-04-14 02:23:36,551 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 02:23:37,000 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 02:23:42,759 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_2.npy
2025-04-14 02:23:42,759 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 136 negative comparisons
[E414 02:23:42.715442611 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x797ea62be446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x797e5b5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x797e5b5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x797e5b5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x797e5b5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x797e5b5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x797ea64255c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x797ea6e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x797ea6f26850 in /lib/x86_64-linux-gnu/libc.so.6)

Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 0
2025-04-14 02:23:45,811 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 0
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 02:24:03,439 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.37it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 02:24:03,987 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:24:04,127 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:24:04,139 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 02:24:20,611 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 02:24:23,635 - __main__ - INFO - Loaded gradients from node 0: 124 negative comparisons
2025-04-14 02:24:23,878 - __main__ - INFO - Loaded gradients from node 1: 150 negative comparisons
2025-04-14 02:24:24,123 - __main__ - INFO - Loaded gradients from node 2: 136 negative comparisons
2025-04-14 02:24:24,289 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 02:24:24,487 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 02:24:24,684 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119130405
2025-04-14 02:24:24,882 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352667
2025-04-14 02:24:25,080 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84833164
2025-04-14 02:24:25,277 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47197960
2025-04-14 02:24:25,474 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255688
2025-04-14 02:24:25,671 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8801472
2025-04-14 02:24:25,870 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551012
2025-04-14 02:24:26,067 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554005
2025-04-14 02:24:26,425 - __main__ - INFO - Non-zero entries after thresholding: 47197960
2025-04-14 02:24:26,425 - __main__ - INFO - Testing stepsize: 0
2025-04-14 02:24:27,068 - __main__ - INFO -   chosen_logps: -90.05239, rejected_logps: -91.92720
2025-04-14 02:24:27,068 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 02:24:27,340 - __main__ - INFO -   chosen_logps: -89.56968, rejected_logps: -92.05219
2025-04-14 02:24:27,340 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 02:24:27,613 - __main__ - INFO -   chosen_logps: -88.63340, rejected_logps: -92.82870
2025-04-14 02:24:27,613 - __main__ - INFO - Testing stepsize: 1
2025-04-14 02:24:27,889 - __main__ - INFO -   chosen_logps: -87.88398, rejected_logps: -93.14758
2025-04-14 02:24:27,889 - __main__ - INFO - Testing stepsize: 2
2025-04-14 02:24:28,168 - __main__ - INFO -   chosen_logps: -85.49944, rejected_logps: -95.12883
2025-04-14 02:24:28,168 - __main__ - INFO - Testing stepsize: 5
2025-04-14 02:24:28,448 - __main__ - INFO -   chosen_logps: -79.01916, rejected_logps: -99.15439
2025-04-14 02:24:28,448 - __main__ - INFO - Testing stepsize: 10
2025-04-14 02:24:28,730 - __main__ - INFO -   chosen_logps: -69.30867, rejected_logps: -106.65807
2025-04-14 02:24:28,730 - __main__ - INFO - Testing stepsize: 15
2025-04-14 02:24:29,014 - __main__ - INFO -   chosen_logps: -60.92112, rejected_logps: -113.98157
2025-04-14 02:24:29,628 - __main__ - INFO - Update scale: 0.007972222222222223
2025-04-14 02:24:29,711 - __main__ - INFO - Model weights updated successfully
2025-04-14 02:24:30,548 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:24:30,581 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:24:31,484 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 02:24:31,518 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 02:24:31,518 - __main__ - INFO - Skipping full model checkpoint for iteration 0
2025-04-14 02:24:31,518 - __main__ - INFO - Iteration 0 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 0
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 0
Iteration 0 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 1
===========================================
Iteration 1 is in valid list - performing full update
Finding noisy pairs for iteration 1...
2025-04-14 02:24:52,564 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 02:24:53,114 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:24:53,259 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:24:53,273 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 02:24:53,273 - __main__ - INFO - Loading dataset
2025-04-14 02:24:54,112 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 02:25:15,822 - __main__ - INFO - Processing dataset
2025-04-14 02:25:16,065 - __main__ - INFO - Searching for noisy pairs starting from index 338
2025-04-14 02:25:22,800 - __main__ - INFO - Noisy pair found at index 369: chosen_logps=-153.0000, rejected_logps=-154.0000
2025-04-14 02:25:22,805 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 02:25:22,872 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_1
2025-04-14 02:25:22,872 - __main__ - INFO - New dataset offset: 370
Updated dataset offset to: 370 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 1
Worker node 1 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 02:25:37,488 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 02:25:37,672 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 02:25:37,703 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.50it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]2025-04-14 02:25:38,045 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]
2025-04-14 02:25:38,190 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:25:38,205 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 02:25:38,205 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:25:38,229 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:25:38,255 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:25:38,370 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:25:38,382 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 02:25:38,382 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:25:38,409 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:25:38,424 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 02:25:38,424 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:25:54,752 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 02:25:54,927 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 02:25:55,068 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 02:26:05,028 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 02:26:05,097 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 02:26:05,248 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 02:26:08,791 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 02:26:08,895 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 02:26:09,049 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 02:26:11,773 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 02:26:12,033 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 02:26:12,597 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 02:26:12,697 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 02:26:12,897 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 02:26:13,555 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 02:26:15,688 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 02:26:15,843 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 02:26:15,907 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 02:26:16,572 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 02:26:16,823 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 02:26:17,852 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 02:26:19,758 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 02:26:19,954 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 02:26:20,423 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 02:26:20,924 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 02:26:20,990 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 02:26:21,726 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 02:26:23,800 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 02:26:23,964 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 02:26:24,435 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 02:26:24,927 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 02:26:25,004 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 02:26:25,856 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 02:26:27,808 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 02:26:28,093 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 02:26:28,525 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 02:26:28,737 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 02:26:29,302 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 02:26:30,242 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 02:26:31,792 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 02:26:31,961 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 02:26:32,696 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 02:26:33,208 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 02:26:33,589 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 02:26:34,810 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 02:26:36,213 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 02:26:36,246 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 02:26:36,894 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 02:26:36,901 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 02:26:37,563 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 02:26:37,715 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 02:26:37,724 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 02:26:38,811 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 02:26:38,812 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 02:26:40,489 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 02:26:40,676 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 02:26:41,911 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 02:26:44,823 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 02:26:44,836 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 02:26:45,191 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 02:26:45,215 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 02:26:45,890 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 02:26:46,889 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 02:28:00,655 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 02:28:02,227 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 02:28:05,750 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 02:28:05,960 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 02:28:06,747 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 02:28:07,089 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 02:28:09,771 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 02:28:10,981 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 02:28:12,706 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 02:28:14,717 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 02:28:15,217 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 02:28:15,589 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 02:28:18,625 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 02:28:18,812 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 02:28:21,435 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 02:28:22,616 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 02:28:22,640 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 02:28:25,837 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 02:28:26,823 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 02:28:28,490 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 02:28:28,730 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 02:28:30,539 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 02:28:31,450 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 02:28:33,336 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 02:28:34,054 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 02:28:34,813 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 02:28:35,203 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 02:28:35,851 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 02:28:36,230 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 02:28:38,421 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 02:30:21.048617928 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7382f5464446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7382aa7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7382aa7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7382aa7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7382aa7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7382aa7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7382f55cb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7382f6094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7382f6126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:30:21.075885872 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7234195f1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7233ce9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7233ce9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7233ce9cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7233ce9d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7233ce9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7234197585c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72341a094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72341a126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 02:30:21,426 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_1.npy
2025-04-14 02:30:21,426 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 117 negative comparisons
2025-04-14 02:30:23,027 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_0.npy
2025-04-14 02:30:23,028 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 112 negative comparisons
2025-04-14 02:30:24,138 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 02:30:25,833 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 02:30:29.914847509 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e8556c31446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e850bfcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e850bfcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7e850bfcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7e850bfd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e850bfd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e8556d985c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e8557894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e8557926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 02:30:29,273 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_2.npy
2025-04-14 02:30:29,273 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 115 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 1
2025-04-14 02:30:32,067 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 1
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 02:30:49,241 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 02:30:49,785 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:30:49,927 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:30:49,939 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 02:31:06,455 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 02:31:09,401 - __main__ - INFO - Loaded gradients from node 0: 112 negative comparisons
2025-04-14 02:31:09,636 - __main__ - INFO - Loaded gradients from node 1: 117 negative comparisons
2025-04-14 02:31:09,871 - __main__ - INFO - Loaded gradients from node 2: 115 negative comparisons
2025-04-14 02:31:10,031 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 02:31:10,221 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 02:31:10,411 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119135238
2025-04-14 02:31:10,601 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107353008
2025-04-14 02:31:10,791 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84837171
2025-04-14 02:31:10,981 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47187068
2025-04-14 02:31:11,171 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22254709
2025-04-14 02:31:11,362 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804378
2025-04-14 02:31:11,552 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552574
2025-04-14 02:31:11,742 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 556505
2025-04-14 02:31:12,090 - __main__ - INFO - Non-zero entries after thresholding: 47187068
2025-04-14 02:31:12,090 - __main__ - INFO - Using relaxed comparison (count_negative: 344)
2025-04-14 02:31:12,090 - __main__ - INFO - Testing stepsize: 0
2025-04-14 02:31:12,720 - __main__ - INFO -   chosen_logps: -152.85391, rejected_logps: -153.87029
2025-04-14 02:31:12,720 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 02:31:12,981 - __main__ - INFO -   chosen_logps: -152.78912, rejected_logps: -153.89819
2025-04-14 02:31:12,981 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 02:31:13,242 - __main__ - INFO -   chosen_logps: -152.35689, rejected_logps: -154.35594
2025-04-14 02:31:13,242 - __main__ - INFO - Testing stepsize: 1
2025-04-14 02:31:13,506 - __main__ - INFO -   chosen_logps: -151.77130, rejected_logps: -154.49605
2025-04-14 02:31:13,506 - __main__ - INFO - Testing stepsize: 2
2025-04-14 02:31:13,774 - __main__ - INFO -   chosen_logps: -150.75911, rejected_logps: -155.59492
2025-04-14 02:31:13,774 - __main__ - INFO - Testing stepsize: 5
2025-04-14 02:31:14,046 - __main__ - INFO -   chosen_logps: -148.57098, rejected_logps: -157.71408
2025-04-14 02:31:14,046 - __main__ - INFO - Testing stepsize: 10
2025-04-14 02:31:14,316 - __main__ - INFO -   chosen_logps: -144.86227, rejected_logps: -161.94496
2025-04-14 02:31:14,316 - __main__ - INFO - Testing stepsize: 15
2025-04-14 02:31:14,588 - __main__ - INFO -   chosen_logps: -141.62529, rejected_logps: -165.63132
2025-04-14 02:31:15,181 - __main__ - INFO - Update scale: 0.00668888888888889
2025-04-14 02:31:15,182 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 02:31:15,262 - __main__ - INFO - Model weights updated successfully
2025-04-14 02:31:16,538 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:31:16,574 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:31:17,473 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 02:31:17,508 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 02:31:17,508 - __main__ - INFO - Skipping full model checkpoint for iteration 1
2025-04-14 02:31:17,508 - __main__ - INFO - Iteration 1 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 1
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 1
Iteration 1 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 2
===========================================
Iteration 2 is in valid list - performing full update
Finding noisy pairs for iteration 2...
2025-04-14 02:31:38,699 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.38it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.75it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
2025-04-14 02:31:39,240 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:31:39,384 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:31:39,398 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 02:31:39,398 - __main__ - INFO - Loading dataset
2025-04-14 02:31:41,171 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 02:32:02,789 - __main__ - INFO - Processing dataset
2025-04-14 02:32:03,035 - __main__ - INFO - Searching for noisy pairs starting from index 370
2025-04-14 02:32:09,204 - __main__ - INFO - Processed 30 samples without finding a noisy pair
2025-04-14 02:32:29,743 - __main__ - INFO - Processed 130 samples without finding a noisy pair
2025-04-14 02:32:51,244 - __main__ - INFO - Processed 230 samples without finding a noisy pair
2025-04-14 02:33:10,579 - __main__ - INFO - Processed 330 samples without finding a noisy pair
2025-04-14 02:33:30,847 - __main__ - INFO - Processed 430 samples without finding a noisy pair
2025-04-14 02:33:50,499 - __main__ - INFO - Processed 530 samples without finding a noisy pair
2025-04-14 02:34:09,154 - __main__ - INFO - Processed 630 samples without finding a noisy pair
2025-04-14 02:34:14,633 - __main__ - INFO - Noisy pair found at index 1030: chosen_logps=-280.0000, rejected_logps=-282.0000
2025-04-14 02:34:14,634 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 02:34:14,700 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_2
2025-04-14 02:34:14,701 - __main__ - INFO - New dataset offset: 1031
Updated dataset offset to: 1031 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 2
Worker node 1 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 02:34:30,072 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.21it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]2025-04-14 02:34:30,427 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
2025-04-14 02:34:30,593 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 02:34:30,620 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]2025-04-14 02:34:30,765 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:34:30,780 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 02:34:30,780 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.24it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.25it/s]2025-04-14 02:34:30,979 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
2025-04-14 02:34:31,132 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:34:31,147 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 02:34:31,147 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:34:31,149 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:34:31,290 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:34:31,303 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 02:34:31,303 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:34:47,335 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 02:34:47,699 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 02:34:47,863 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 02:34:57,411 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 02:34:57,703 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 02:34:57,949 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 02:35:01,148 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 02:35:01,674 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 02:35:01,813 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 02:35:04,378 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 02:35:04,743 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 02:35:04,778 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 02:35:05,076 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 02:35:05,672 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 02:35:05,801 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 02:35:08,278 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 02:35:08,700 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 02:35:08,876 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 02:35:09,052 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 02:35:09,401 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 02:35:10,071 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 02:35:12,306 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 02:35:12,652 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 02:35:12,720 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 02:35:12,989 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 02:35:13,397 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 02:35:13,911 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 02:35:16,015 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 02:35:16,369 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 02:35:16,884 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 02:35:17,200 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 02:35:17,862 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 02:35:18,074 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 02:35:19,964 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 02:35:20,327 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 02:35:20,859 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 02:35:21,753 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 02:35:21,902 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 02:35:22,044 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 02:35:24,092 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 02:35:24,878 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 02:35:25,169 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 02:35:25,233 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 02:35:26,151 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 02:35:26,297 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 02:35:27,855 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 02:35:29,165 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 02:35:29,169 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 02:35:29,189 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 02:35:29,295 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 02:35:30,202 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 02:35:30,212 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 02:35:30,270 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 02:35:30,271 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 02:35:32,638 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 02:35:33,356 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 02:35:33,369 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 02:35:36,509 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 02:35:36,518 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 02:35:37,439 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 02:35:37,532 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 02:35:37,666 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 02:35:38,028 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 02:36:55,203 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 02:36:55,224 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 02:36:56,399 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 02:36:57,164 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 02:36:58,243 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 02:37:00,453 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 02:37:01,079 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 02:37:02,588 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 02:37:05,193 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 02:37:05,241 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 02:37:05,870 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 02:37:06,021 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 02:37:10,087 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 02:37:11,024 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 02:37:13,208 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 02:37:13,603 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 02:37:13,827 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 02:37:13,876 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 02:37:16,387 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 02:37:18,141 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 02:37:21,724 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 02:37:22,009 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 02:37:22,644 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 02:37:24,355 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 02:37:25,969 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 02:37:26,007 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 02:37:26,223 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 02:37:26,621 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 02:37:27,857 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 02:37:29,797 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 02:39:14.949140151 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f8e57e88446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f8e0d1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f8e0d1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f8e0d1cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f8e0d1d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f8e0d1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f8e57fef5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f8e58a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f8e58b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:39:14.979645902 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75a1e6f93446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75a19c3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75a19c3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x75a19c3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x75a19c3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75a19c3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x75a1e70ee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x75a1e7a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x75a1e7b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:39:14.987648112 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7365f60b9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7365ab3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7365ab3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7365ab3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7365ab3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7365ab3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7365f62205c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7365f6c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7365f6d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 02:39:14,290 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_0.npy
2025-04-14 02:39:14,290 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 126 negative comparisons
2025-04-14 02:39:15,121 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_1.npy
2025-04-14 02:39:15,121 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 130 negative comparisons
2025-04-14 02:39:17,080 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 02:39:17,887 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 02:39:18.664931807 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x725ca6417446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x725c5b7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x725c5b7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x725c5b7cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x725c5b7d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x725c5b7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x725ca657e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x725ca7094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x725ca7126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 02:39:18.676341822 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7cedcc743446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ced81bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ced81bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7ced81bcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7ced81bd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ced81bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7cedcc8aa5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7cedcd494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7cedcd526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 02:39:18,969 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_2.npy
2025-04-14 02:39:18,969 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 125 negative comparisons
2025-04-14 02:39:21,964 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 2
Worker node 2 successfully completed gradient computation for iteration 2
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 02:39:38,016 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 02:39:38,560 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:39:38,702 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:39:38,714 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 02:39:55,172 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 02:39:58,214 - __main__ - INFO - Loaded gradients from node 0: 126 negative comparisons
2025-04-14 02:39:58,457 - __main__ - INFO - Loaded gradients from node 1: 130 negative comparisons
2025-04-14 02:39:58,700 - __main__ - INFO - Loaded gradients from node 2: 125 negative comparisons
2025-04-14 02:39:58,867 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 02:39:59,066 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 02:39:59,265 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119135308
2025-04-14 02:39:59,463 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352330
2025-04-14 02:39:59,661 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84838886
2025-04-14 02:39:59,858 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47197306
2025-04-14 02:40:00,056 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255066
2025-04-14 02:40:00,253 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806887
2025-04-14 02:40:00,451 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550807
2025-04-14 02:40:00,648 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554198
2025-04-14 02:40:01,006 - __main__ - INFO - Non-zero entries after thresholding: 47197306
2025-04-14 02:40:01,006 - __main__ - INFO - Testing stepsize: 0
2025-04-14 02:40:01,637 - __main__ - INFO -   chosen_logps: -279.87091, rejected_logps: -281.80594
2025-04-14 02:40:01,637 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 02:40:01,894 - __main__ - INFO -   chosen_logps: -279.23956, rejected_logps: -281.96552
2025-04-14 02:40:01,894 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 02:40:02,151 - __main__ - INFO -   chosen_logps: -277.72128, rejected_logps: -283.21729
2025-04-14 02:40:02,151 - __main__ - INFO - Testing stepsize: 1
2025-04-14 02:40:02,412 - __main__ - INFO -   chosen_logps: -275.32858, rejected_logps: -284.20587
2025-04-14 02:40:02,412 - __main__ - INFO - Testing stepsize: 2
2025-04-14 02:40:02,677 - __main__ - INFO -   chosen_logps: -271.47031, rejected_logps: -286.67389
2025-04-14 02:40:02,677 - __main__ - INFO - Testing stepsize: 5
2025-04-14 02:40:02,942 - __main__ - INFO -   chosen_logps: -260.36420, rejected_logps: -293.41809
2025-04-14 02:40:02,942 - __main__ - INFO - Testing stepsize: 10
2025-04-14 02:40:03,209 - __main__ - INFO -   chosen_logps: -242.64952, rejected_logps: -305.40424
2025-04-14 02:40:03,209 - __main__ - INFO - Testing stepsize: 15
2025-04-14 02:40:03,476 - __main__ - INFO -   chosen_logps: -227.33614, rejected_logps: -317.37927
2025-04-14 02:40:04,070 - __main__ - INFO - Update scale: 0.0074083333333333345
2025-04-14 02:40:04,152 - __main__ - INFO - Model weights updated successfully
2025-04-14 02:40:05,422 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:40:05,458 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:40:06,478 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 02:40:06,517 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 02:40:06,517 - __main__ - INFO - Skipping full model checkpoint for iteration 2
2025-04-14 02:40:06,517 - __main__ - INFO - Iteration 2 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 2
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 2
Iteration 2 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 3
===========================================
Iteration 3 is in valid list - performing full update
Finding noisy pairs for iteration 3...
2025-04-14 02:40:27,660 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 02:40:28,209 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:40:28,353 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:40:28,368 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 02:40:28,368 - __main__ - INFO - Loading dataset
2025-04-14 02:40:30,436 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 02:40:52,243 - __main__ - INFO - Processing dataset
2025-04-14 02:40:52,493 - __main__ - INFO - Searching for noisy pairs starting from index 1031
2025-04-14 02:41:07,452 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 02:41:21,063 - __main__ - INFO - Noisy pair found at index 1163: chosen_logps=-147.0000, rejected_logps=-149.0000
2025-04-14 02:41:21,064 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 02:41:21,132 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_3
2025-04-14 02:41:21,133 - __main__ - INFO - New dataset offset: 1164
Updated dataset offset to: 1164 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 3
Worker node 1 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 02:41:36,941 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]2025-04-14 02:41:37,227 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 02:41:37,343 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 02:41:37,490 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]2025-04-14 02:41:37,635 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:41:37,650 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 02:41:37,650 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 02:41:37,784 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]
2025-04-14 02:41:37,895 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:41:37,924 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:41:37,936 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 02:41:37,936 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:41:38,050 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:41:38,066 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 02:41:38,066 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:41:54,240 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 02:41:54,492 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 02:41:54,606 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 02:42:04,325 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 02:42:04,574 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 02:42:04,875 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 02:42:08,165 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 02:42:08,320 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 02:42:09,112 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 02:42:11,664 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 02:42:11,886 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 02:42:12,009 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 02:42:12,310 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 02:42:12,390 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 02:42:13,154 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 02:42:15,543 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 02:42:15,816 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 02:42:16,203 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 02:42:16,247 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 02:42:16,514 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 02:42:16,964 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 02:42:19,661 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 02:42:19,940 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 02:42:20,101 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 02:42:20,115 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 02:42:20,405 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 02:42:20,765 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 02:42:23,610 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 02:42:23,912 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 02:42:24,032 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 02:42:24,178 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 02:42:24,579 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 02:42:24,874 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 02:42:27,548 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 02:42:27,889 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 02:42:28,150 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 02:42:28,163 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 02:42:28,580 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 02:42:29,109 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 02:42:31,586 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 02:42:32,005 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 02:42:32,137 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 02:42:32,595 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 02:42:33,013 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 02:42:33,050 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 02:42:35,896 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 02:42:36,391 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 02:42:36,416 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 02:42:36,880 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 02:42:36,881 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 02:42:37,161 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 02:42:37,170 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 02:42:37,386 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 02:42:37,390 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 02:42:39,770 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 02:42:40,284 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 02:42:41,044 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 02:42:44,070 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 02:42:44,236 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 02:42:44,473 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 02:42:44,511 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 02:42:45,238 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 02:42:45,399 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 02:44:08,856 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 02:44:11,794 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 02:44:13,207 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 02:44:13,462 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 02:44:14,171 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 02:44:15,914 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 02:44:17,590 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 02:44:17,632 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 02:44:18,767 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 02:44:20,758 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 02:44:23,086 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 02:44:23,246 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 02:44:24,824 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 02:44:27,152 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 02:44:27,653 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 02:44:29,972 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 02:44:31,070 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 02:44:31,909 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 02:44:33,895 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 02:44:34,244 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 02:44:36,149 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 02:44:36,854 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 02:44:37,289 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 02:44:40,663 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 02:44:41,222 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 02:44:41,458 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 02:44:41,929 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 02:44:42,024 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 02:44:42,109 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 02:44:43,133 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
[rank7]:[E414 02:46:35.395237029 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x718567d87446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71851d1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71851d1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x71851d1cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x71851d1d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71851d1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x718567eee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x718568894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x718568926850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank5]:[E414 02:46:35.407678930 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e624c83a446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e6201bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e6201bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7e6201bcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7e6201bd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e6201bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e624c9a15c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e624d494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e624d526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 02:46:35,595 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_0.npy
2025-04-14 02:46:35,596 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 150 negative comparisons
[E414 02:46:38.144436738 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e3c32676446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e3be79cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e3be79cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7e3be79cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7e3be79d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e3be79d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e3c327dd5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e3c33294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e3c33326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 02:46:38,459 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_1.npy
2025-04-14 02:46:38,459 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 144 negative comparisons
2025-04-14 02:46:38,597 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 02:46:39,004 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_2.npy
2025-04-14 02:46:39,004 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 159 negative comparisons
2025-04-14 02:46:41,167 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 02:46:41,936 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 3
Worker node 2 successfully completed gradient computation for iteration 3
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 02:46:59,918 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 02:47:00,466 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:47:00,606 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:47:00,619 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 02:47:17,107 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 02:47:19,997 - __main__ - INFO - Loaded gradients from node 0: 150 negative comparisons
2025-04-14 02:47:20,231 - __main__ - INFO - Loaded gradients from node 1: 144 negative comparisons
2025-04-14 02:47:20,465 - __main__ - INFO - Loaded gradients from node 2: 159 negative comparisons
2025-04-14 02:47:20,625 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 02:47:20,815 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 02:47:21,005 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133646
2025-04-14 02:47:21,196 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107355251
2025-04-14 02:47:21,386 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84839152
2025-04-14 02:47:21,577 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47198644
2025-04-14 02:47:21,767 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252527
2025-04-14 02:47:21,957 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8801893
2025-04-14 02:47:22,147 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551967
2025-04-14 02:47:22,338 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554665
2025-04-14 02:47:22,685 - __main__ - INFO - Non-zero entries after thresholding: 47198644
2025-04-14 02:47:22,685 - __main__ - INFO - Testing stepsize: 0
2025-04-14 02:47:23,428 - __main__ - INFO -   chosen_logps: -147.81166, rejected_logps: -147.74109
2025-04-14 02:47:23,428 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 02:47:23,808 - __main__ - INFO -   chosen_logps: -147.46060, rejected_logps: -147.88171
2025-04-14 02:47:23,808 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 02:47:24,191 - __main__ - INFO -   chosen_logps: -145.85770, rejected_logps: -148.33437
2025-04-14 02:47:24,191 - __main__ - INFO - Testing stepsize: 1
2025-04-14 02:47:24,580 - __main__ - INFO -   chosen_logps: -144.23250, rejected_logps: -149.03366
2025-04-14 02:47:24,580 - __main__ - INFO - Testing stepsize: 2
2025-04-14 02:47:24,969 - __main__ - INFO -   chosen_logps: -140.91844, rejected_logps: -150.13283
2025-04-14 02:47:24,969 - __main__ - INFO - Testing stepsize: 5
2025-04-14 02:47:25,359 - __main__ - INFO -   chosen_logps: -132.03618, rejected_logps: -153.45880
2025-04-14 02:47:25,359 - __main__ - INFO - Testing stepsize: 10
2025-04-14 02:47:25,751 - __main__ - INFO -   chosen_logps: -119.98137, rejected_logps: -159.43472
2025-04-14 02:47:25,751 - __main__ - INFO - Testing stepsize: 15
2025-04-14 02:47:26,144 - __main__ - INFO -   chosen_logps: -111.67274, rejected_logps: -165.33276
2025-04-14 02:47:26,903 - __main__ - INFO - Update scale: 0.008808333333333333
2025-04-14 02:47:26,985 - __main__ - INFO - Model weights updated successfully
2025-04-14 02:47:27,826 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:47:27,860 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:47:28,761 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 02:47:28,794 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 02:47:28,795 - __main__ - INFO - Skipping full model checkpoint for iteration 3
2025-04-14 02:47:28,795 - __main__ - INFO - Iteration 3 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 3
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 3
Iteration 3 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 4
===========================================
Iteration 4 is in valid list - performing full update
Finding noisy pairs for iteration 4...
2025-04-14 02:47:49,467 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 02:47:50,013 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:47:50,159 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:47:50,173 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 02:47:50,173 - __main__ - INFO - Loading dataset
2025-04-14 02:47:51,012 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 02:48:12,533 - __main__ - INFO - Processing dataset
2025-04-14 02:48:12,779 - __main__ - INFO - Searching for noisy pairs starting from index 1164
2025-04-14 02:48:19,711 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 02:48:41,039 - __main__ - INFO - Processed 136 samples without finding a noisy pair
2025-04-14 02:48:51,140 - __main__ - INFO - Noisy pair found at index 1344: chosen_logps=-268.0000, rejected_logps=-270.0000
2025-04-14 02:48:51,141 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 02:48:51,208 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_4
2025-04-14 02:48:51,208 - __main__ - INFO - New dataset offset: 1345
Updated dataset offset to: 1345 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 4
Worker node 1 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 02:49:06,546 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]2025-04-14 02:49:06,794 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 02:49:06,827 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]2025-04-14 02:49:07,104 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]2025-04-14 02:49:07,248 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:49:07,262 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 02:49:07,262 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s]
2025-04-14 02:49:07,350 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:49:07,380 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:49:07,490 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:49:07,503 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 02:49:07,503 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:49:07,534 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:49:07,549 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 02:49:07,549 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:49:23,803 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 02:49:24,058 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 02:49:24,089 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 02:49:34,023 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 02:49:34,384 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 02:49:34,501 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 02:49:38,138 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 02:49:38,197 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 02:49:38,234 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 02:49:40,666 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 02:49:41,452 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 02:49:41,493 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 02:49:42,056 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 02:49:42,122 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 02:49:42,430 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 02:49:45,261 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 02:49:45,559 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 02:49:45,757 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 02:49:45,975 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 02:49:46,259 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 02:49:46,409 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 02:49:49,139 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 02:49:49,271 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 02:49:49,798 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 02:49:49,923 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 02:49:50,298 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 02:49:50,307 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 02:49:53,186 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 02:49:53,458 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 02:49:53,708 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 02:49:53,854 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 02:49:54,241 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 02:49:54,711 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 02:49:57,174 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 02:49:57,288 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 02:49:57,760 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 02:49:57,940 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 02:49:58,109 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 02:49:58,768 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 02:50:01,252 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 02:50:01,495 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 02:50:01,690 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 02:50:01,985 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 02:50:02,225 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 02:50:03,121 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 02:50:05,243 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 02:50:05,546 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 02:50:05,717 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 02:50:06,300 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 02:50:06,305 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 02:50:06,400 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 02:50:06,410 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 02:50:07,271 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 02:50:07,276 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 02:50:09,081 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 02:50:09,566 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 02:50:10,449 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 02:50:13,319 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 02:50:13,367 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 02:50:13,989 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 02:50:14,598 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 02:50:14,769 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 02:50:14,799 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 02:51:31,009 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 02:51:32,847 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 02:51:35,788 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 02:51:37,300 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 02:51:39,345 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 02:51:39,953 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 02:51:41,742 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 02:51:42,104 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 02:51:43,402 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 02:51:43,845 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 02:51:46,356 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 02:51:49,146 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 02:51:49,493 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 02:51:49,908 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 02:51:53,748 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 02:51:53,834 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 02:51:53,885 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 02:51:54,247 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 02:51:58,091 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 02:51:58,378 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 02:52:00,574 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 02:52:01,569 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 02:52:02,374 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 02:52:04,676 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 02:52:04,809 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 02:52:05,333 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 02:52:05,457 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 02:52:06,207 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 02:52:06,480 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 02:52:08,645 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 02:53:54.281466898 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x724e23898446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x724dd8bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x724dd8bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x724dd8bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x724dd8bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x724dd8bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x724e239ff5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x724e24494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x724e24526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 02:53:54,599 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_0.npy
2025-04-14 02:53:54,599 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 191 negative comparisons
[E414 02:53:55.933405327 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x710d171ef446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x710ccc5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x710ccc5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x710ccc5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x710ccc5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x710ccc5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x710d173565c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x710d17c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x710d17d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 02:53:55,297 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_1.npy
2025-04-14 02:53:55,298 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 220 negative comparisons
2025-04-14 02:53:57,495 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 02:53:58,165 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 02:54:00,233 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_2.npy
2025-04-14 02:54:00,233 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 186 negative comparisons
2025-04-14 02:54:03,510 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 4
Worker node 2 successfully completed gradient computation for iteration 4
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 02:54:21,138 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.54it/s]
2025-04-14 02:54:21,686 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:54:21,828 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:54:21,846 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 02:54:38,375 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 02:54:41,249 - __main__ - INFO - Loaded gradients from node 0: 191 negative comparisons
2025-04-14 02:54:41,480 - __main__ - INFO - Loaded gradients from node 1: 220 negative comparisons
2025-04-14 02:54:41,711 - __main__ - INFO - Loaded gradients from node 2: 186 negative comparisons
2025-04-14 02:54:41,870 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 02:54:42,058 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 02:54:42,246 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119131679
2025-04-14 02:54:42,434 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107347109
2025-04-14 02:54:42,622 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84829125
2025-04-14 02:54:42,809 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47195647
2025-04-14 02:54:42,995 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22256375
2025-04-14 02:54:43,182 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804838
2025-04-14 02:54:43,369 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550412
2025-04-14 02:54:43,556 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555395
2025-04-14 02:54:43,895 - __main__ - INFO - Non-zero entries after thresholding: 47195647
2025-04-14 02:54:43,895 - __main__ - INFO - Testing stepsize: 0
2025-04-14 02:54:44,545 - __main__ - INFO -   chosen_logps: -267.81494, rejected_logps: -269.53839
2025-04-14 02:54:44,545 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 02:54:44,829 - __main__ - INFO -   chosen_logps: -267.54688, rejected_logps: -269.67996
2025-04-14 02:54:44,829 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 02:54:45,113 - __main__ - INFO -   chosen_logps: -265.41351, rejected_logps: -271.61789
2025-04-14 02:54:45,113 - __main__ - INFO - Testing stepsize: 1
2025-04-14 02:54:45,400 - __main__ - INFO -   chosen_logps: -263.27350, rejected_logps: -273.20001
2025-04-14 02:54:45,400 - __main__ - INFO - Testing stepsize: 2
2025-04-14 02:54:45,690 - __main__ - INFO -   chosen_logps: -259.13989, rejected_logps: -276.46283
2025-04-14 02:54:45,690 - __main__ - INFO - Testing stepsize: 5
2025-04-14 02:54:45,982 - __main__ - INFO -   chosen_logps: -247.23019, rejected_logps: -288.28772
2025-04-14 02:54:45,982 - __main__ - INFO - Testing stepsize: 10
2025-04-14 02:54:46,276 - __main__ - INFO -   chosen_logps: -229.07520, rejected_logps: -310.79755
2025-04-14 02:54:46,276 - __main__ - INFO - Testing stepsize: 15
2025-04-14 02:54:46,570 - __main__ - INFO -   chosen_logps: -213.28500, rejected_logps: -336.07269
2025-04-14 02:54:47,191 - __main__ - INFO - Update scale: 0.011608333333333335
2025-04-14 02:54:47,271 - __main__ - INFO - Model weights updated successfully
2025-04-14 02:54:48,500 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:54:48,537 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:54:49,603 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 02:54:49,639 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 02:54:49,639 - __main__ - INFO - Skipping full model checkpoint for iteration 4
2025-04-14 02:54:49,639 - __main__ - INFO - Iteration 4 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 4
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 4
Iteration 4 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 5
===========================================
Iteration 5 is in valid list - performing full update
Finding noisy pairs for iteration 5...
2025-04-14 02:55:10,494 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.72it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
2025-04-14 02:55:11,040 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:55:11,185 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 02:55:11,200 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 02:55:11,200 - __main__ - INFO - Loading dataset
2025-04-14 02:55:12,160 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 02:55:33,863 - __main__ - INFO - Processing dataset
2025-04-14 02:55:34,111 - __main__ - INFO - Searching for noisy pairs starting from index 1345
2025-04-14 02:55:45,339 - __main__ - INFO - Processed 55 samples without finding a noisy pair
2025-04-14 02:56:05,985 - __main__ - INFO - Processed 155 samples without finding a noisy pair
2025-04-14 02:56:11,435 - __main__ - INFO - Noisy pair found at index 1530: chosen_logps=-360.0000, rejected_logps=-362.0000
2025-04-14 02:56:11,436 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 02:56:11,504 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_5
2025-04-14 02:56:11,505 - __main__ - INFO - New dataset offset: 1531
Updated dataset offset to: 1531 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 5
Worker node 2 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 02:56:26,744 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.20it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]2025-04-14 02:56:27,148 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 02:56:27,239 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 02:56:27,293 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]2025-04-14 02:56:27,439 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:56:27,454 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 02:56:27,454 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]
2025-04-14 02:56:27,700 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
2025-04-14 02:56:27,792 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 02:56:27,855 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:56:27,870 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 02:56:27,870 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:56:27,933 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 02:56:27,945 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 02:56:27,945 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 02:56:43,986 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 02:56:44,414 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 02:56:44,562 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 02:56:54,015 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 02:56:54,340 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 02:56:54,630 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 02:56:57,789 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 02:56:58,110 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 02:56:58,543 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 02:57:01,238 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 02:57:01,328 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 02:57:01,608 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 02:57:01,726 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 02:57:02,071 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 02:57:02,563 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 02:57:05,430 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 02:57:05,494 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 02:57:05,504 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 02:57:06,055 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 02:57:06,263 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 02:57:06,513 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 02:57:09,024 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 02:57:09,182 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 02:57:09,436 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 02:57:09,752 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 02:57:10,200 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 02:57:10,601 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 02:57:13,336 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 02:57:13,336 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 02:57:13,620 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 02:57:14,233 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 02:57:14,405 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 02:57:14,554 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 02:57:16,619 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 02:57:17,254 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 02:57:17,561 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 02:57:18,223 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 02:57:18,349 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 02:57:18,494 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 02:57:21,406 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 02:57:21,620 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 02:57:21,801 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 02:57:21,815 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 02:57:22,425 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 02:57:22,795 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 02:57:25,326 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 02:57:25,586 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 02:57:25,931 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 02:57:26,049 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 02:57:26,059 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 02:57:26,938 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 02:57:26,945 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 02:57:27,045 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 02:57:27,055 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 02:57:29,194 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 02:57:29,637 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 02:57:30,183 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 02:57:33,463 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 02:57:33,483 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 02:57:34,131 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 02:57:34,442 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 02:57:35,136 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 02:57:35,210 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 02:58:53,855 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 02:58:54,050 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 02:58:57,761 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 02:58:57,862 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 02:58:58,226 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 02:58:58,361 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 02:59:00,589 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 02:59:01,239 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 02:59:01,924 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 02:59:05,962 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 02:59:06,187 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 02:59:08,354 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 02:59:09,892 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 02:59:10,163 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 02:59:12,330 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 02:59:13,587 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 02:59:13,852 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 02:59:17,983 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 02:59:18,048 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 02:59:18,361 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 02:59:18,760 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 02:59:22,264 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 02:59:22,684 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 02:59:22,723 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 02:59:24,598 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 02:59:25,889 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 02:59:26,476 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 02:59:29,559 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 02:59:29,650 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 02:59:29,883 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 03:01:15.973054984 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70c9a6afb446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70c95bdcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70c95bdcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x70c95bdcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x70c95bdd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70c95bdd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70c9a6c625c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70c9a7694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70c9a7726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:01:15,221 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_0.npy
2025-04-14 03:01:15,221 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 95 negative comparisons
2025-04-14 03:01:18,034 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 03:01:18.738550208 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70cb7a0d5446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70cb2f3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70cb2f3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x70cb2f3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x70cb2f3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70cb2f3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70cb7a23c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70cb7ac94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70cb7ad26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:01:19,170 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_1.npy
2025-04-14 03:01:19,170 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 93 negative comparisons
2025-04-14 03:01:21,764 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_2.npy
2025-04-14 03:01:21,764 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 85 negative comparisons
2025-04-14 03:01:22,117 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 03:01:24,524 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 5
Worker node 2 successfully completed gradient computation for iteration 5
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 03:01:41,849 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 03:01:42,397 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:01:42,537 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:01:42,550 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 03:01:59,014 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 03:02:01,916 - __main__ - INFO - Loaded gradients from node 0: 95 negative comparisons
2025-04-14 03:02:02,155 - __main__ - INFO - Loaded gradients from node 1: 93 negative comparisons
2025-04-14 03:02:02,392 - __main__ - INFO - Loaded gradients from node 2: 85 negative comparisons
2025-04-14 03:02:02,554 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 03:02:02,748 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 03:02:02,942 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119131371
2025-04-14 03:02:03,134 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107351291
2025-04-14 03:02:03,327 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84832208
2025-04-14 03:02:03,519 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47192564
2025-04-14 03:02:03,711 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255122
2025-04-14 03:02:03,904 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803904
2025-04-14 03:02:04,096 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552900
2025-04-14 03:02:04,289 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 556210
2025-04-14 03:02:04,639 - __main__ - INFO - Non-zero entries after thresholding: 47192564
2025-04-14 03:02:04,639 - __main__ - INFO - Using relaxed comparison (count_negative: 273)
2025-04-14 03:02:04,639 - __main__ - INFO - Testing stepsize: 0
2025-04-14 03:02:05,316 - __main__ - INFO -   chosen_logps: -360.32120, rejected_logps: -361.51398
2025-04-14 03:02:05,316 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 03:02:05,619 - __main__ - INFO -   chosen_logps: -359.84064, rejected_logps: -361.58685
2025-04-14 03:02:05,619 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 03:02:05,922 - __main__ - INFO -   chosen_logps: -357.62158, rejected_logps: -362.22809
2025-04-14 03:02:05,922 - __main__ - INFO - Testing stepsize: 1
2025-04-14 03:02:06,228 - __main__ - INFO -   chosen_logps: -355.81436, rejected_logps: -363.41068
2025-04-14 03:02:06,228 - __main__ - INFO - Testing stepsize: 2
2025-04-14 03:02:06,539 - __main__ - INFO -   chosen_logps: -350.58475, rejected_logps: -366.97699
2025-04-14 03:02:06,539 - __main__ - INFO - Testing stepsize: 5
2025-04-14 03:02:06,850 - __main__ - INFO -   chosen_logps: -336.15872, rejected_logps: -372.62622
2025-04-14 03:02:06,850 - __main__ - INFO - Testing stepsize: 10
2025-04-14 03:02:07,163 - __main__ - INFO -   chosen_logps: -315.95428, rejected_logps: -386.24097
2025-04-14 03:02:07,163 - __main__ - INFO - Testing stepsize: 15
2025-04-14 03:02:07,478 - __main__ - INFO -   chosen_logps: -298.81772, rejected_logps: -400.94043
2025-04-14 03:02:08,137 - __main__ - INFO - Update scale: 0.005308333333333334
2025-04-14 03:02:08,138 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 03:02:08,220 - __main__ - INFO - Model weights updated successfully
2025-04-14 03:02:09,538 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:02:09,577 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:02:10,465 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 03:02:10,500 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 03:02:10,500 - __main__ - INFO - Skipping full model checkpoint for iteration 5
2025-04-14 03:02:10,501 - __main__ - INFO - Iteration 5 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 5
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 5
Iteration 5 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 6
===========================================
Iteration 6 is in valid list - performing full update
Finding noisy pairs for iteration 6...
2025-04-14 03:02:31,569 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 03:02:32,115 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:02:32,261 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:02:32,275 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 03:02:32,275 - __main__ - INFO - Loading dataset
2025-04-14 03:02:33,151 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 03:02:54,497 - __main__ - INFO - Processing dataset
2025-04-14 03:02:54,744 - __main__ - INFO - Searching for noisy pairs starting from index 1531
2025-04-14 03:03:08,819 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 03:03:13,374 - __main__ - INFO - Noisy pair found at index 1624: chosen_logps=-127.5000, rejected_logps=-130.0000
2025-04-14 03:03:13,375 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 03:03:13,442 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_6
2025-04-14 03:03:13,442 - __main__ - INFO - New dataset offset: 1625
Updated dataset offset to: 1625 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 6
Worker node 1 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 03:03:28,627 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 03:03:28,909 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 03:03:28,998 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]2025-04-14 03:03:29,184 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]2025-04-14 03:03:29,329 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.30it/s]/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:03:29,344 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 03:03:29,344 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]
2025-04-14 03:03:29,462 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 03:03:29,553 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:03:29,618 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:03:29,633 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 03:03:29,633 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:03:29,693 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:03:29,706 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 03:03:29,706 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:03:45,928 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 03:03:46,181 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 03:03:46,278 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 03:03:56,061 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 03:03:56,315 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 03:03:56,492 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 03:04:00,011 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 03:04:00,104 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 03:04:00,354 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 03:04:03,261 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 03:04:03,379 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 03:04:03,485 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 03:04:03,962 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 03:04:04,050 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 03:04:04,736 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 03:04:07,130 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 03:04:07,320 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 03:04:07,588 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 03:04:07,867 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 03:04:07,952 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 03:04:08,685 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 03:04:11,196 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 03:04:11,310 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 03:04:11,799 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 03:04:12,201 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 03:04:12,291 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 03:04:12,774 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 03:04:15,312 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 03:04:15,466 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 03:04:15,763 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 03:04:16,212 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 03:04:16,271 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 03:04:16,886 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 03:04:18,961 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 03:04:19,421 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 03:04:19,772 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 03:04:20,443 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 03:04:20,569 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 03:04:20,973 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 03:04:23,124 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 03:04:23,341 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 03:04:23,909 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 03:04:24,374 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 03:04:24,497 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 03:04:25,009 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 03:04:26,848 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 03:04:28,082 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 03:04:28,139 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 03:04:28,139 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 03:04:28,393 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 03:04:28,789 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 03:04:28,798 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 03:04:29,414 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 03:04:29,425 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 03:04:30,971 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 03:04:32,091 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 03:04:32,631 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 03:04:35,639 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 03:04:35,747 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 03:04:36,089 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 03:04:36,205 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 03:04:37,550 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 03:04:37,614 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 03:05:59,466 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 03:06:00,099 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 03:06:02,316 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 03:06:04,049 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 03:06:04,160 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 03:06:06,542 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 03:06:06,964 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 03:06:07,565 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 03:06:08,312 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 03:06:11,905 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 03:06:13,050 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 03:06:13,320 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 03:06:14,434 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 03:06:14,863 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 03:06:17,703 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 03:06:19,010 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 03:06:19,018 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 03:06:23,038 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 03:06:23,247 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 03:06:23,615 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 03:06:24,312 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 03:06:26,264 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 03:06:28,844 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 03:06:28,954 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 03:06:31,667 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 03:06:31,682 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 03:06:32,099 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 03:06:32,233 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 03:06:35,908 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 03:06:36,232 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 03:08:24,488 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_0.npy
2025-04-14 03:08:24,488 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 148 negative comparisons
[E414 03:08:24.449944971 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e2967880446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e291cbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e291cbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7e291cbcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7e291cbd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e291cbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e29679e75c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e2968494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e2968526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 03:08:25.747599947 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f9e79bef446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f9e2efcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f9e2efcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7f9e2efcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f9e2efd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f9e2efd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f9e79d565c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f9e7a694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f9e7a726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 03:08:25.762236045 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b1dce171446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b1d835cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b1d835cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7b1d835cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7b1d835d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b1d835d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b1dce2d85c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b1dcec94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b1dced26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:08:26,098 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_1.npy
2025-04-14 03:08:26,098 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 148 negative comparisons
2025-04-14 03:08:27,612 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 03:08:28,936 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 03:08:31.170721292 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79e396c31446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79e34bfcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79e34bfcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x79e34bfcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x79e34bfd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79e34bfd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79e396d985c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x79e397894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x79e397926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 03:08:31.173603929 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71e23e523446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71e1f37cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71e1f37cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x71e1f37cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x71e1f37d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71e1f37d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x71e23e68a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x71e23f094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x71e23f126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:08:31,435 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_2.npy
2025-04-14 03:08:31,435 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 145 negative comparisons
2025-04-14 03:08:34,156 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 6
Worker node 2 successfully completed gradient computation for iteration 6
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 03:08:52,014 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 03:08:52,562 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:08:52,704 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:08:52,716 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 03:09:09,239 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 03:09:12,142 - __main__ - INFO - Loaded gradients from node 0: 148 negative comparisons
2025-04-14 03:09:12,385 - __main__ - INFO - Loaded gradients from node 1: 148 negative comparisons
2025-04-14 03:09:12,623 - __main__ - INFO - Loaded gradients from node 2: 145 negative comparisons
2025-04-14 03:09:12,785 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 03:09:12,978 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 03:09:13,171 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119131224
2025-04-14 03:09:13,363 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352692
2025-04-14 03:09:13,556 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84837234
2025-04-14 03:09:13,749 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47192051
2025-04-14 03:09:13,941 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22257780
2025-04-14 03:09:14,134 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802310
2025-04-14 03:09:14,326 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551785
2025-04-14 03:09:14,520 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555159
2025-04-14 03:09:14,870 - __main__ - INFO - Non-zero entries after thresholding: 47192051
2025-04-14 03:09:14,870 - __main__ - INFO - Testing stepsize: 0
2025-04-14 03:09:15,610 - __main__ - INFO -   chosen_logps: -127.93081, rejected_logps: -129.48668
2025-04-14 03:09:15,610 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 03:09:15,979 - __main__ - INFO -   chosen_logps: -127.45518, rejected_logps: -129.48703
2025-04-14 03:09:15,979 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 03:09:16,348 - __main__ - INFO -   chosen_logps: -125.81206, rejected_logps: -129.88161
2025-04-14 03:09:16,348 - __main__ - INFO - Testing stepsize: 1
2025-04-14 03:09:16,720 - __main__ - INFO -   chosen_logps: -124.32034, rejected_logps: -130.27202
2025-04-14 03:09:16,721 - __main__ - INFO - Testing stepsize: 2
2025-04-14 03:09:17,096 - __main__ - INFO -   chosen_logps: -121.14751, rejected_logps: -131.32758
2025-04-14 03:09:17,096 - __main__ - INFO - Testing stepsize: 5
2025-04-14 03:09:17,472 - __main__ - INFO -   chosen_logps: -112.79732, rejected_logps: -134.04909
2025-04-14 03:09:17,473 - __main__ - INFO - Testing stepsize: 10
2025-04-14 03:09:17,852 - __main__ - INFO -   chosen_logps: -100.30905, rejected_logps: -138.94658
2025-04-14 03:09:17,852 - __main__ - INFO - Testing stepsize: 15
2025-04-14 03:09:18,235 - __main__ - INFO -   chosen_logps: -89.99481, rejected_logps: -143.95261
2025-04-14 03:09:18,989 - __main__ - INFO - Update scale: 0.008575000000000001
2025-04-14 03:09:19,071 - __main__ - INFO - Model weights updated successfully
2025-04-14 03:09:19,915 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:09:19,952 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:09:20,845 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 03:09:20,879 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 03:09:20,879 - __main__ - INFO - Skipping full model checkpoint for iteration 6
2025-04-14 03:09:20,880 - __main__ - INFO - Iteration 6 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 6
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 6
Iteration 6 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 7
===========================================
Iteration 7 is in valid list - performing full update
Finding noisy pairs for iteration 7...
2025-04-14 03:09:42,064 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 03:09:42,613 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:09:42,758 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:09:42,772 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 03:09:42,772 - __main__ - INFO - Loading dataset
2025-04-14 03:09:44,207 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 03:10:05,700 - __main__ - INFO - Processing dataset
2025-04-14 03:10:05,944 - __main__ - INFO - Searching for noisy pairs starting from index 1625
2025-04-14 03:10:13,558 - __main__ - INFO - Noisy pair found at index 1663: chosen_logps=-49.7500, rejected_logps=-50.5000
2025-04-14 03:10:13,559 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 03:10:13,623 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_7
2025-04-14 03:10:13,624 - __main__ - INFO - New dataset offset: 1664
Updated dataset offset to: 1664 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 7
Worker node 2 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 03:10:28,010 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.21it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]2025-04-14 03:10:28,445 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
2025-04-14 03:10:28,474 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 03:10:28,558 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 03:10:28,703 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:10:28,718 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 03:10:28,718 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
2025-04-14 03:10:28,998 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:10:29,031 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:10:29,151 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:10:29,166 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 03:10:29,166 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:10:29,171 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:10:29,183 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 03:10:29,184 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:10:45,287 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 03:10:45,732 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 03:10:45,779 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 03:10:55,464 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 03:10:55,954 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 03:10:56,047 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 03:10:59,678 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 03:10:59,778 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 03:10:59,801 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 03:11:02,637 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 03:11:02,994 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 03:11:03,332 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 03:11:03,673 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 03:11:03,699 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 03:11:03,762 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 03:11:06,936 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 03:11:07,037 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 03:11:07,265 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 03:11:07,619 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 03:11:07,667 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 03:11:07,793 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 03:11:10,935 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 03:11:11,014 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 03:11:11,145 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 03:11:11,483 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 03:11:11,587 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 03:11:11,683 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 03:11:14,863 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 03:11:15,010 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 03:11:15,289 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 03:11:15,497 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 03:11:15,612 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 03:11:15,671 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 03:11:18,669 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 03:11:18,816 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 03:11:19,222 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 03:11:19,572 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 03:11:19,661 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 03:11:19,851 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 03:11:22,716 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 03:11:22,909 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 03:11:23,166 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 03:11:23,794 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 03:11:23,865 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 03:11:24,295 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 03:11:26,850 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 03:11:27,056 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 03:11:27,390 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 03:11:28,118 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 03:11:28,128 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 03:11:28,242 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 03:11:28,252 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 03:11:28,592 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 03:11:28,596 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 03:11:31,235 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 03:11:31,505 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 03:11:32,003 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 03:11:35,476 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 03:11:35,671 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 03:11:36,009 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 03:11:36,247 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 03:11:36,494 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 03:11:36,728 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 03:12:57,629 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 03:12:59,340 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 03:13:02,697 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 03:13:03,032 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 03:13:03,181 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 03:13:04,142 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 03:13:07,591 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 03:13:07,602 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 03:13:08,210 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 03:13:10,620 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 03:13:10,943 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 03:13:11,289 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 03:13:13,996 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 03:13:14,801 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 03:13:17,200 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 03:13:18,418 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 03:13:18,471 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 03:13:21,509 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 03:13:21,851 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 03:13:21,976 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 03:13:22,370 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 03:13:27,869 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 03:13:30,458 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 03:13:30,800 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 03:13:30,855 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 03:13:31,171 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 03:13:31,423 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 03:13:34,325 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 03:13:35,538 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 03:13:35,934 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
[E414 03:15:23.267864231 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74a3c0bde446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74a375fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74a375fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x74a375fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x74a375fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74a375fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74a3c0d455c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74a3c1694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74a3c1726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:15:23,543 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_0.npy
2025-04-14 03:15:23,543 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 78 negative comparisons
2025-04-14 03:15:26,316 - __main__ - INFO - Node 0: Gradient computation completed successfully
[rank2]:[E414 03:15:31.663728326 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78b034c47446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78afe9fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78afe9fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x78afe9fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x78afe9fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78afe9fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78b034dae5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78b035894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78b035926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 03:15:31.674160641 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74e8cc4c7446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74e8817cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74e8817cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x74e8817cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x74e8817d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74e8817d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74e8cc62e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74e8cd094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74e8cd126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:15:31,887 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_1.npy
2025-04-14 03:15:31,887 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 79 negative comparisons
[E414 03:15:32.196739163 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74aeffd78446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74aeb51cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74aeb51cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x74aeb51cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x74aeb51d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74aeb51d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74aeffedf5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74af00a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74af00b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank4]:[E414 03:15:32.200500348 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78b25cba1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78b211fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78b211fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x78b211fcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x78b211fd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78b211fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78b25cd085c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78b25d894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78b25d926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:15:32,490 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_2.npy
2025-04-14 03:15:32,490 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 90 negative comparisons
Waiting for worker nodes to complete gradient computation...
2025-04-14 03:15:34,775 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 03:15:35,264 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 7
Worker node 2 successfully completed gradient computation for iteration 7
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 03:15:53,244 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 03:15:53,792 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:15:53,933 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:15:53,945 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 03:16:10,404 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 03:16:13,390 - __main__ - INFO - Loaded gradients from node 0: 78 negative comparisons
2025-04-14 03:16:13,629 - __main__ - INFO - Loaded gradients from node 1: 79 negative comparisons
2025-04-14 03:16:13,868 - __main__ - INFO - Loaded gradients from node 2: 90 negative comparisons
2025-04-14 03:16:14,030 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 03:16:14,226 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 03:16:14,421 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133414
2025-04-14 03:16:14,616 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107345532
2025-04-14 03:16:14,812 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84830411
2025-04-14 03:16:15,007 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47188381
2025-04-14 03:16:15,203 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22253999
2025-04-14 03:16:15,399 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804636
2025-04-14 03:16:15,595 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1554290
2025-04-14 03:16:15,792 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555397
2025-04-14 03:16:16,148 - __main__ - INFO - Non-zero entries after thresholding: 47188381
2025-04-14 03:16:16,149 - __main__ - INFO - Using relaxed comparison (count_negative: 247)
2025-04-14 03:16:16,149 - __main__ - INFO - Testing stepsize: 0
2025-04-14 03:16:16,884 - __main__ - INFO -   chosen_logps: -49.78995, rejected_logps: -50.12743
2025-04-14 03:16:16,884 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 03:16:17,249 - __main__ - INFO -   chosen_logps: -49.80130, rejected_logps: -50.06491
2025-04-14 03:16:17,249 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 03:16:17,615 - __main__ - INFO -   chosen_logps: -49.60558, rejected_logps: -50.42733
2025-04-14 03:16:17,616 - __main__ - INFO - Testing stepsize: 1
2025-04-14 03:16:17,985 - __main__ - INFO -   chosen_logps: -48.87926, rejected_logps: -50.48119
2025-04-14 03:16:17,985 - __main__ - INFO - Testing stepsize: 2
2025-04-14 03:16:18,358 - __main__ - INFO -   chosen_logps: -47.89553, rejected_logps: -51.12636
2025-04-14 03:16:18,358 - __main__ - INFO - Testing stepsize: 5
2025-04-14 03:16:18,731 - __main__ - INFO -   chosen_logps: -45.20505, rejected_logps: -52.40919
2025-04-14 03:16:18,731 - __main__ - INFO - Testing stepsize: 10
2025-04-14 03:16:19,107 - __main__ - INFO -   chosen_logps: -41.03054, rejected_logps: -55.05109
2025-04-14 03:16:19,107 - __main__ - INFO - Testing stepsize: 15
2025-04-14 03:16:19,485 - __main__ - INFO -   chosen_logps: -37.75849, rejected_logps: -58.14622
2025-04-14 03:16:20,226 - __main__ - INFO - Update scale: 0.004802777777777778
2025-04-14 03:16:20,227 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 03:16:20,308 - __main__ - INFO - Model weights updated successfully
2025-04-14 03:16:21,157 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:16:21,194 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:16:22,076 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 03:16:22,110 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 03:16:22,110 - __main__ - INFO - Skipping full model checkpoint for iteration 7
2025-04-14 03:16:22,110 - __main__ - INFO - Iteration 7 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 7
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 7
Iteration 7 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 8
===========================================
Iteration 8 is in valid list - performing full update
Finding noisy pairs for iteration 8...
2025-04-14 03:16:43,267 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 03:16:43,816 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:16:43,961 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:16:43,975 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 03:16:43,975 - __main__ - INFO - Loading dataset
2025-04-14 03:16:45,087 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 03:17:06,350 - __main__ - INFO - Processing dataset
2025-04-14 03:17:06,592 - __main__ - INFO - Searching for noisy pairs starting from index 1664
2025-04-14 03:17:15,056 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 03:17:23,835 - __main__ - INFO - Noisy pair found at index 1746: chosen_logps=-23.8750, rejected_logps=-24.2500
2025-04-14 03:17:23,836 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 03:17:23,902 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_8
2025-04-14 03:17:23,903 - __main__ - INFO - New dataset offset: 1747
Updated dataset offset to: 1747 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 8
Worker node 1 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 03:17:39,042 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.20it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]2025-04-14 03:17:39,434 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 03:17:39,486 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 03:17:39,591 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 03:17:39,735 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:17:39,750 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 03:17:39,750 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 03:17:39,987 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:17:40,042 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:17:40,143 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:17:40,164 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 03:17:40,164 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:17:40,184 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:17:40,197 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 03:17:40,197 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:17:56,312 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 03:17:56,750 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 03:17:56,757 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 03:18:06,338 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 03:18:06,796 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 03:18:06,919 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 03:18:10,155 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 03:18:10,975 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 03:18:11,101 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 03:18:13,628 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 03:18:13,907 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 03:18:13,924 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 03:18:14,070 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 03:18:14,992 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 03:18:15,274 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 03:18:17,661 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 03:18:17,856 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 03:18:18,156 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 03:18:18,675 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 03:18:19,010 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 03:18:19,034 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 03:18:21,411 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 03:18:21,823 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 03:18:22,409 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 03:18:22,487 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 03:18:22,866 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 03:18:23,175 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 03:18:25,373 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 03:18:25,914 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 03:18:26,248 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 03:18:26,786 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 03:18:27,001 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 03:18:27,110 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 03:18:29,433 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 03:18:29,847 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 03:18:30,209 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 03:18:30,748 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 03:18:30,927 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 03:18:31,427 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 03:18:33,404 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 03:18:33,862 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 03:18:34,050 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 03:18:34,708 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 03:18:34,941 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 03:18:35,488 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 03:18:37,351 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 03:18:37,896 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 03:18:37,904 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 03:18:37,977 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 03:18:38,867 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 03:18:39,075 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 03:18:39,079 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 03:18:39,595 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 03:18:39,602 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 03:18:41,760 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 03:18:42,288 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 03:18:43,287 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 03:18:45,522 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 03:18:45,784 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 03:18:46,515 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 03:18:46,659 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 03:18:47,760 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 03:18:47,844 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 03:20:10,269 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 03:20:11,414 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 03:20:12,546 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 03:20:15,319 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 03:20:15,356 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 03:20:18,682 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 03:20:19,689 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 03:20:19,811 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 03:20:22,196 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 03:20:22,684 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 03:20:23,738 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 03:20:25,256 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 03:20:26,141 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 03:20:27,837 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 03:20:30,275 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 03:20:31,257 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 03:20:31,275 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 03:20:34,122 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 03:20:36,325 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 03:20:36,595 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 03:20:36,616 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 03:20:39,310 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 03:20:39,546 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 03:20:42,123 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 03:20:43,007 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 03:20:43,033 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 03:20:43,771 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 03:20:45,212 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 03:20:47,760 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 03:20:48,107 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 03:22:37.152017425 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x764014593446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x763fc99cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x763fc99cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x763fc99cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x763fc99d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x763fc99d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7640146ee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x764015094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x764015126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 03:22:37.158876905 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71e2fa010446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71e2af3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71e2af3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x71e2af3cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x71e2af3d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71e2af3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x71e2fa1775c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x71e2faa94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x71e2fab26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:22:37,434 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_0.npy
2025-04-14 03:22:37,434 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 116 negative comparisons
2025-04-14 03:22:38,406 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_1.npy
2025-04-14 03:22:38,406 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 127 negative comparisons
[E414 03:22:38.273115237 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7987e7d23446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79879cfcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79879cfcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x79879cfcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x79879cfd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79879cfd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7987e7e8a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7987e8894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7987e8926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 03:22:38.287268390 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x717359e17446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71730f1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71730f1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x71730f1cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x71730f1d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71730f1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x717359f7e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x71735a894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x71735a926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 03:22:38.295367005 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7aff643de446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7aff197cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7aff197cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7aff197cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7aff197d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7aff197d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7aff645455c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7aff64e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7aff64f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:22:40,307 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 03:22:41,422 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 03:22:44.699118305 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x750f12431446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x750ec77cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x750ec77cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x750ec77cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x750ec77d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x750ec77d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x750f125985c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x750f13094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x750f13126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:22:45,047 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_2.npy
2025-04-14 03:22:45,048 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 100 negative comparisons
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 8
2025-04-14 03:22:48,008 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 8
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 03:23:04,964 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 03:23:05,509 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:23:05,651 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:23:05,663 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 03:23:22,133 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 03:23:25,047 - __main__ - INFO - Loaded gradients from node 0: 116 negative comparisons
2025-04-14 03:23:25,272 - __main__ - INFO - Loaded gradients from node 1: 127 negative comparisons
2025-04-14 03:23:25,502 - __main__ - INFO - Loaded gradients from node 2: 100 negative comparisons
2025-04-14 03:23:25,660 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 03:23:25,848 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 03:23:26,036 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119132073
2025-04-14 03:23:26,224 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107343801
2025-04-14 03:23:26,410 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84823210
2025-04-14 03:23:26,597 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47185684
2025-04-14 03:23:26,784 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22250842
2025-04-14 03:23:26,972 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8808618
2025-04-14 03:23:27,159 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1553158
2025-04-14 03:23:27,346 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 556562
2025-04-14 03:23:27,687 - __main__ - INFO - Non-zero entries after thresholding: 47185684
2025-04-14 03:23:27,687 - __main__ - INFO - Using relaxed comparison (count_negative: 343)
2025-04-14 03:23:27,687 - __main__ - INFO - Testing stepsize: 0
2025-04-14 03:23:28,431 - __main__ - INFO -   chosen_logps: -23.92810, rejected_logps: -23.81994
2025-04-14 03:23:28,432 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 03:23:28,812 - __main__ - INFO -   chosen_logps: -23.88463, rejected_logps: -23.81363
2025-04-14 03:23:28,812 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 03:23:29,192 - __main__ - INFO -   chosen_logps: -23.61394, rejected_logps: -24.05290
2025-04-14 03:23:29,192 - __main__ - INFO - Testing stepsize: 1
2025-04-14 03:23:29,577 - __main__ - INFO -   chosen_logps: -23.07700, rejected_logps: -24.48695
2025-04-14 03:23:29,577 - __main__ - INFO - Testing stepsize: 2
2025-04-14 03:23:29,964 - __main__ - INFO -   chosen_logps: -22.19772, rejected_logps: -25.04139
2025-04-14 03:23:29,965 - __main__ - INFO - Testing stepsize: 5
2025-04-14 03:23:30,354 - __main__ - INFO -   chosen_logps: -19.26711, rejected_logps: -27.18193
2025-04-14 03:23:30,354 - __main__ - INFO - Testing stepsize: 10
2025-04-14 03:23:30,745 - __main__ - INFO -   chosen_logps: -15.17443, rejected_logps: -30.44736
2025-04-14 03:23:30,745 - __main__ - INFO - Testing stepsize: 15
2025-04-14 03:23:31,137 - __main__ - INFO -   chosen_logps: -11.60954, rejected_logps: -34.01426
2025-04-14 03:23:31,899 - __main__ - INFO - Update scale: 0.006669444444444445
2025-04-14 03:23:31,900 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 03:23:31,980 - __main__ - INFO - Model weights updated successfully
2025-04-14 03:23:32,820 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:23:32,854 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:23:33,748 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 03:23:33,781 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 03:23:33,781 - __main__ - INFO - Skipping full model checkpoint for iteration 8
2025-04-14 03:23:33,781 - __main__ - INFO - Iteration 8 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 8
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 8
Iteration 8 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 9
===========================================
Iteration 9 is in valid list - performing full update
Finding noisy pairs for iteration 9...
2025-04-14 03:23:54,916 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
2025-04-14 03:23:55,465 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:23:55,610 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:23:55,625 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 03:23:55,625 - __main__ - INFO - Loading dataset
2025-04-14 03:23:56,420 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 03:24:17,851 - __main__ - INFO - Processing dataset
2025-04-14 03:24:18,095 - __main__ - INFO - Searching for noisy pairs starting from index 1747
2025-04-14 03:24:28,359 - __main__ - INFO - Processed 53 samples without finding a noisy pair
2025-04-14 03:24:46,541 - __main__ - INFO - Noisy pair found at index 1892: chosen_logps=-58.0000, rejected_logps=-59.7500
2025-04-14 03:24:46,542 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 03:24:46,609 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_9
2025-04-14 03:24:46,610 - __main__ - INFO - New dataset offset: 1893
Updated dataset offset to: 1893 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 9
Worker node 1 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 03:25:02,039 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.21it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.53it/s]2025-04-14 03:25:02,389 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 03:25:02,489 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.43it/s]2025-04-14 03:25:02,587 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.67it/s]2025-04-14 03:25:02,732 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:25:02,747 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 03:25:02,747 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 03:25:02,934 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 03:25:03,045 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:25:03,087 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:25:03,102 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 03:25:03,102 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:25:03,189 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:25:03,201 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 03:25:03,201 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:25:19,283 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 03:25:19,664 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 03:25:19,756 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 03:25:29,685 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 03:25:29,812 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 03:25:29,861 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 03:25:33,595 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 03:25:33,651 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 03:25:34,103 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 03:25:36,671 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 03:25:36,735 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 03:25:36,801 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 03:25:37,783 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 03:25:38,010 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 03:25:38,187 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 03:25:40,966 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 03:25:40,969 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 03:25:41,079 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 03:25:41,809 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 03:25:41,881 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 03:25:41,957 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 03:25:45,135 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 03:25:45,379 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 03:25:45,443 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 03:25:45,769 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 03:25:45,927 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 03:25:46,023 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 03:25:49,356 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 03:25:49,361 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 03:25:49,702 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 03:25:49,788 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 03:25:49,930 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 03:25:50,146 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 03:25:53,230 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 03:25:53,447 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 03:25:53,531 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 03:25:53,646 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 03:25:54,133 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 03:25:54,332 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 03:25:57,174 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 03:25:57,187 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 03:25:57,611 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 03:25:57,922 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 03:25:58,304 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 03:25:58,534 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 03:26:01,309 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 03:26:01,735 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 03:26:02,298 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 03:26:02,307 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 03:26:02,409 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 03:26:02,774 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 03:26:02,782 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 03:26:03,155 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 03:26:03,164 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 03:26:05,226 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 03:26:06,074 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 03:26:06,179 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 03:26:09,559 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 03:26:09,651 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 03:26:10,326 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 03:26:10,580 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 03:26:10,972 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 03:26:11,110 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 03:27:35,437 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 03:27:36,807 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 03:27:37,704 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 03:27:38,326 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 03:27:39,601 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 03:27:41,973 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 03:27:42,195 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 03:27:43,222 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 03:27:46,238 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 03:27:46,661 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 03:27:46,922 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 03:27:47,548 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 03:27:49,737 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 03:27:50,407 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 03:27:52,941 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 03:27:53,261 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 03:27:53,363 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 03:27:58,430 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 03:27:58,961 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 03:27:59,685 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 03:28:02,169 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 03:28:02,226 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 03:28:02,382 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 03:28:02,533 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 03:28:06,386 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 03:28:06,883 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 03:28:06,911 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 03:28:06,944 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 03:28:08,323 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 03:28:10,941 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 03:30:00.570734374 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78a9f5904446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78a9aabcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78a9aabcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x78a9aabcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x78a9aabd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78a9aabd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78a9f5a6b5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78a9f6494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78a9f6526850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank6]:[E414 03:30:00.587291891 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7727d4aef446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x772789dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x772789dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x772789dcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x772789dd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x772789dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7727d4c565c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7727d5694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7727d5726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:30:00,930 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_1.npy
2025-04-14 03:30:00,930 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 137 negative comparisons
2025-04-14 03:30:01,305 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_0.npy
2025-04-14 03:30:01,305 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 116 negative comparisons
2025-04-14 03:30:03,759 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 03:30:04,402 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 03:30:07,210 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_2.npy
2025-04-14 03:30:07,210 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 120 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 9
2025-04-14 03:30:10,341 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 9
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 03:30:27,733 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 03:30:28,284 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:30:28,430 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:30:28,444 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 03:30:44,964 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 03:30:47,815 - __main__ - INFO - Loaded gradients from node 0: 116 negative comparisons
2025-04-14 03:30:48,018 - __main__ - INFO - Loaded gradients from node 1: 137 negative comparisons
2025-04-14 03:30:48,248 - __main__ - INFO - Loaded gradients from node 2: 120 negative comparisons
2025-04-14 03:30:48,406 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 03:30:48,594 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 03:30:48,782 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119137743
2025-04-14 03:30:48,970 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107357631
2025-04-14 03:30:49,159 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84835658
2025-04-14 03:30:49,347 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47186345
2025-04-14 03:30:49,534 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252094
2025-04-14 03:30:49,721 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804215
2025-04-14 03:30:49,908 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552295
2025-04-14 03:30:50,095 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 556418
2025-04-14 03:30:50,437 - __main__ - INFO - Non-zero entries after thresholding: 47186345
2025-04-14 03:30:50,437 - __main__ - INFO - Testing stepsize: 0
2025-04-14 03:30:51,181 - __main__ - INFO -   chosen_logps: -57.56343, rejected_logps: -59.94021
2025-04-14 03:30:51,181 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 03:30:51,558 - __main__ - INFO -   chosen_logps: -57.19429, rejected_logps: -60.03182
2025-04-14 03:30:51,558 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 03:30:51,935 - __main__ - INFO -   chosen_logps: -56.60306, rejected_logps: -60.34485
2025-04-14 03:30:51,935 - __main__ - INFO - Testing stepsize: 1
2025-04-14 03:30:52,315 - __main__ - INFO -   chosen_logps: -55.60491, rejected_logps: -60.83835
2025-04-14 03:30:52,315 - __main__ - INFO - Testing stepsize: 2
2025-04-14 03:30:52,700 - __main__ - INFO -   chosen_logps: -53.45502, rejected_logps: -61.64191
2025-04-14 03:30:52,700 - __main__ - INFO - Testing stepsize: 5
2025-04-14 03:30:53,088 - __main__ - INFO -   chosen_logps: -48.23521, rejected_logps: -64.00832
2025-04-14 03:30:53,088 - __main__ - INFO - Testing stepsize: 10
2025-04-14 03:30:53,475 - __main__ - INFO -   chosen_logps: -40.62661, rejected_logps: -68.22179
2025-04-14 03:30:53,475 - __main__ - INFO - Testing stepsize: 15
2025-04-14 03:30:53,865 - __main__ - INFO -   chosen_logps: -35.51234, rejected_logps: -72.43906
2025-04-14 03:30:54,622 - __main__ - INFO - Update scale: 0.007252777777777778
2025-04-14 03:30:54,704 - __main__ - INFO - Model weights updated successfully
2025-04-14 03:30:55,996 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:30:56,034 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:30:56,927 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 03:30:56,962 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 03:30:56,963 - __main__ - INFO - Skipping full model checkpoint for iteration 9
2025-04-14 03:30:56,963 - __main__ - INFO - Iteration 9 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 9
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 9
Iteration 9 completed successfully
Cleaning up iteration files...
Multi-node training completed successfully!
Copying scripts to worker nodes...
Starting with dataset offset: 0
===========================================
Processing iteration 0
===========================================
Iteration 0 is in valid list - performing full update
Finding noisy pairs for iteration 0...
2025-04-14 03:31:21,037 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 03:31:21,586 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:31:21,732 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:31:21,747 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 03:31:21,747 - __main__ - INFO - Loading dataset
2025-04-14 03:31:22,603 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 03:31:44,103 - __main__ - INFO - Processing dataset
2025-04-14 03:31:44,345 - __main__ - INFO - Searching for noisy pairs starting from index 0
2025-04-14 03:31:44,990 - __main__ - INFO - Processed 0 samples without finding a noisy pair
2025-04-14 03:32:05,356 - __main__ - INFO - Processed 100 samples without finding a noisy pair
2025-04-14 03:32:23,450 - __main__ - INFO - Processed 200 samples without finding a noisy pair
2025-04-14 03:32:42,466 - __main__ - INFO - Processed 300 samples without finding a noisy pair
2025-04-14 03:32:49,201 - __main__ - INFO - Noisy pair found at index 337: chosen_logps=-90.0000, rejected_logps=-92.0000
2025-04-14 03:32:49,202 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 03:32:49,269 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_0
2025-04-14 03:32:49,270 - __main__ - INFO - New dataset offset: 338
Updated dataset offset to: 338 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 0
Worker node 2 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 03:33:04,809 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 03:33:05,011 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 03:33:05,046 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]2025-04-14 03:33:05,368 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.50it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 03:33:05,512 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:33:05,527 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 03:33:05,527 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:33:05,556 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:33:05,602 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:33:05,709 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:33:05,724 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 03:33:05,724 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:33:05,743 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:33:05,755 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 03:33:05,755 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:33:22,061 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 03:33:22,269 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 03:33:22,373 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 03:33:32,065 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 03:33:32,495 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 03:33:32,510 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 03:33:35,920 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 03:33:36,255 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 03:33:36,752 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 03:33:39,162 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 03:33:39,501 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 03:33:39,713 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 03:33:39,843 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 03:33:40,194 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 03:33:40,862 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 03:33:43,111 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 03:33:43,427 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 03:33:43,912 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 03:33:44,126 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 03:33:44,187 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 03:33:45,125 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 03:33:47,189 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 03:33:47,508 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 03:33:47,827 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 03:33:48,057 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 03:33:48,095 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 03:33:49,020 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 03:33:51,407 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 03:33:51,596 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 03:33:52,096 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 03:33:52,121 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 03:33:52,144 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 03:33:53,080 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 03:33:55,295 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 03:33:55,372 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 03:33:56,083 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 03:33:56,141 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 03:33:56,627 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 03:33:57,081 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 03:33:59,515 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 03:33:59,860 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 03:33:59,961 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 03:34:00,447 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 03:34:00,979 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 03:34:01,289 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 03:34:03,525 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 03:34:03,907 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 03:34:04,060 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 03:34:04,566 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 03:34:04,569 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 03:34:05,250 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 03:34:05,254 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 03:34:05,290 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 03:34:05,299 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 03:34:07,805 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 03:34:08,242 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 03:34:08,513 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 03:34:12,167 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 03:34:12,358 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 03:34:12,463 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 03:34:12,496 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 03:34:12,885 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 03:34:12,892 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 03:35:30,347 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 03:35:31,394 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 03:35:32,635 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 03:35:33,079 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 03:35:35,685 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 03:35:37,128 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 03:35:38,624 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 03:35:39,601 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 03:35:40,164 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 03:35:41,749 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 03:35:42,283 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 03:35:45,784 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 03:35:47,713 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 03:35:48,319 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 03:35:49,831 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 03:35:49,844 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 03:35:51,922 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 03:35:53,785 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 03:35:54,149 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 03:35:54,370 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 03:35:55,110 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 03:35:57,333 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 03:35:57,744 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 03:35:58,031 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 03:36:01,163 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 03:36:02,551 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 03:36:03,331 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 03:36:03,427 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 03:36:03,817 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 03:36:06,616 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 03:37:50.808730151 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ccf16d17446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ccecbfcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ccecbfcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7ccecbfcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7ccecbfd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ccecbfd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7ccf16e7e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7ccf17894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7ccf17926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:37:51,167 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_0.npy
2025-04-14 03:37:51,168 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 137 negative comparisons
[E414 03:37:51.510193415 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b8e86231446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b8e3b5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b8e3b5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7b8e3b5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7b8e3b5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b8e3b5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b8e863985c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b8e86c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b8e86d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 03:37:51.526929065 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a14ab255446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7a14605cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7a14605cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7a14605cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7a14605d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a14605d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7a14ab3bc5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7a14abe94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7a14abf26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:37:51,918 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_1.npy
2025-04-14 03:37:51,918 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 137 negative comparisons
2025-04-14 03:37:54,080 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 03:37:54,794 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 03:37:56,349 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_2.npy
2025-04-14 03:37:56,349 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 140 negative comparisons
2025-04-14 03:37:59,374 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 0
Worker node 2 successfully completed gradient computation for iteration 0
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 03:38:17,007 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 03:38:17,556 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:38:17,696 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:38:17,709 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 03:38:34,163 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 03:38:37,051 - __main__ - INFO - Loaded gradients from node 0: 137 negative comparisons
2025-04-14 03:38:37,284 - __main__ - INFO - Loaded gradients from node 1: 137 negative comparisons
2025-04-14 03:38:37,518 - __main__ - INFO - Loaded gradients from node 2: 140 negative comparisons
2025-04-14 03:38:37,679 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 03:38:37,869 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 03:38:38,061 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133500
2025-04-14 03:38:38,253 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352574
2025-04-14 03:38:38,445 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84829198
2025-04-14 03:38:38,637 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47186750
2025-04-14 03:38:38,830 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255984
2025-04-14 03:38:39,020 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803280
2025-04-14 03:38:39,211 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551604
2025-04-14 03:38:39,402 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 556327
2025-04-14 03:38:39,748 - __main__ - INFO - Non-zero entries after thresholding: 47186750
2025-04-14 03:38:39,748 - __main__ - INFO - Testing stepsize: 0
2025-04-14 03:38:40,382 - __main__ - INFO -   chosen_logps: -90.05240, rejected_logps: -91.92722
2025-04-14 03:38:40,382 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 03:38:40,648 - __main__ - INFO -   chosen_logps: -89.57686, rejected_logps: -92.05206
2025-04-14 03:38:40,649 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 03:38:40,915 - __main__ - INFO -   chosen_logps: -88.68678, rejected_logps: -92.63270
2025-04-14 03:38:40,915 - __main__ - INFO - Testing stepsize: 1
2025-04-14 03:38:41,184 - __main__ - INFO -   chosen_logps: -87.63000, rejected_logps: -93.09103
2025-04-14 03:38:41,184 - __main__ - INFO - Testing stepsize: 2
2025-04-14 03:38:41,457 - __main__ - INFO -   chosen_logps: -85.32936, rejected_logps: -94.36095
2025-04-14 03:38:41,457 - __main__ - INFO - Testing stepsize: 5
2025-04-14 03:38:41,732 - __main__ - INFO -   chosen_logps: -78.60084, rejected_logps: -98.02586
2025-04-14 03:38:41,732 - __main__ - INFO - Testing stepsize: 10
2025-04-14 03:38:42,009 - __main__ - INFO -   chosen_logps: -68.20638, rejected_logps: -103.73901
2025-04-14 03:38:42,009 - __main__ - INFO - Testing stepsize: 15
2025-04-14 03:38:42,286 - __main__ - INFO -   chosen_logps: -59.86756, rejected_logps: -110.25253
2025-04-14 03:38:42,884 - __main__ - INFO - Update scale: 0.008050000000000002
2025-04-14 03:38:42,965 - __main__ - INFO - Model weights updated successfully
2025-04-14 03:38:43,800 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:38:43,834 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:38:45,067 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 03:38:45,101 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 03:38:45,102 - __main__ - INFO - Skipping full model checkpoint for iteration 0
2025-04-14 03:38:45,102 - __main__ - INFO - Iteration 0 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 0
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 0
Iteration 0 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 1
===========================================
Iteration 1 is in valid list - performing full update
Finding noisy pairs for iteration 1...
2025-04-14 03:39:06,085 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.31it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.72it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
2025-04-14 03:39:06,630 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:39:06,774 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:39:06,788 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 03:39:06,788 - __main__ - INFO - Loading dataset
2025-04-14 03:39:07,684 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 03:39:29,109 - __main__ - INFO - Processing dataset
2025-04-14 03:39:29,357 - __main__ - INFO - Searching for noisy pairs starting from index 338
2025-04-14 03:39:36,072 - __main__ - INFO - Noisy pair found at index 369: chosen_logps=-153.0000, rejected_logps=-154.0000
2025-04-14 03:39:36,073 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 03:39:36,141 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_1
2025-04-14 03:39:36,142 - __main__ - INFO - New dataset offset: 370
Updated dataset offset to: 370 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 1
Worker node 2 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 03:39:50,612 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 03:39:50,740 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.19it/s]2025-04-14 03:39:50,943 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  6.54it/s]2025-04-14 03:39:51,161 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 03:39:51,295 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  6.68it/s]2025-04-14 03:39:51,304 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:39:51,319 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 03:39:51,319 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.13it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  6.98it/s]
2025-04-14 03:39:51,437 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:39:51,450 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 03:39:51,450 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:39:51,525 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:39:51,659 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:39:51,671 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 03:39:51,671 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:40:07,818 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 03:40:08,014 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 03:40:08,241 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 03:40:18,091 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 03:40:18,191 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 03:40:18,944 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 03:40:21,937 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 03:40:22,095 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 03:40:22,956 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 03:40:24,932 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 03:40:25,206 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 03:40:25,538 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 03:40:26,116 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 03:40:26,365 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 03:40:27,108 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 03:40:28,787 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 03:40:28,842 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 03:40:29,751 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 03:40:29,969 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 03:40:30,657 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 03:40:30,954 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 03:40:33,638 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 03:40:33,844 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 03:40:33,942 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 03:40:34,262 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 03:40:34,690 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 03:40:34,752 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 03:40:37,373 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 03:40:37,661 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 03:40:38,211 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 03:40:38,388 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 03:40:39,035 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 03:40:39,164 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 03:40:41,515 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 03:40:41,703 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 03:40:41,788 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 03:40:42,651 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 03:40:43,027 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 03:40:43,096 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 03:40:45,687 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 03:40:45,737 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 03:40:46,695 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 03:40:46,793 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 03:40:46,976 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 03:40:47,379 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 03:40:49,821 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 03:40:49,836 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 03:40:50,405 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 03:40:50,744 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 03:40:50,750 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 03:40:50,957 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 03:40:50,959 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 03:40:51,420 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 03:40:51,423 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 03:40:53,889 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 03:40:54,086 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 03:40:54,801 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 03:40:57,881 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 03:40:57,994 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 03:40:58,629 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 03:40:58,659 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 03:40:58,992 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 03:40:59,274 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 03:42:15,104 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 03:42:15,114 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 03:42:19,107 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 03:42:20,213 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 03:42:21,005 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 03:42:22,757 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 03:42:22,811 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 03:42:22,816 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 03:42:23,095 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 03:42:26,987 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 03:42:27,468 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 03:42:29,003 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 03:42:31,569 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 03:42:33,125 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 03:42:35,596 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 03:42:35,773 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 03:42:36,091 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 03:42:37,178 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 03:42:38,392 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 03:42:40,028 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 03:42:43,365 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 03:42:43,653 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 03:42:44,535 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 03:42:46,325 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 03:42:47,740 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 03:42:47,965 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 03:42:47,980 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 03:42:49,265 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 03:42:52,598 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 03:42:53,000 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 03:44:35,353 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_0.npy
2025-04-14 03:44:35,353 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 121 negative comparisons
2025-04-14 03:44:35,389 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_1.npy
2025-04-14 03:44:35,389 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 121 negative comparisons
[E414 03:44:35.302870243 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x711b979fd446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x711b4cdcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x711b4cdcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x711b4cdcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x711b4cdd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x711b4cdd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x711b97b645c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x711b98494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x711b98526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 03:44:35.239704745 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d7b08efb446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7d7abe1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7d7abe1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7d7abe1cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7d7abe1d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d7abe1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7d7b090625c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7d7b09a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7d7b09b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 03:44:35.307289536 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c7ae39b2446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c7a98dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c7a98dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7c7a98dcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7c7a98dd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c7a98dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c7ae3b195c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c7ae4494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c7ae4526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 03:44:35.256693726 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7878a9009446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78785e3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78785e3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x78785e3cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x78785e3d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78785e3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7878a91705c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7878a9a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7878a9b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:44:38,381 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 03:44:38,529 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 03:44:43.834471791 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x770df4df1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x770daa1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x770daa1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x770daa1cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x770daa1d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x770daa1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x770df4f585c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x770df5a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x770df5b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 03:44:43.843859947 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d9b90e1e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7d9b461cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7d9b461cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7d9b461cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7d9b461d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d9b461d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7d9b90f855c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7d9b91a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7d9b91b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:44:44,128 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_2.npy
2025-04-14 03:44:44,128 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 143 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 1
Waiting for worker nodes to complete gradient computation...
2025-04-14 03:44:47,159 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 1
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 03:45:04,172 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 03:45:04,718 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:45:04,858 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:45:04,870 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 03:45:21,371 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 03:45:24,275 - __main__ - INFO - Loaded gradients from node 0: 121 negative comparisons
2025-04-14 03:45:24,506 - __main__ - INFO - Loaded gradients from node 1: 121 negative comparisons
2025-04-14 03:45:24,736 - __main__ - INFO - Loaded gradients from node 2: 143 negative comparisons
2025-04-14 03:45:24,893 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 03:45:25,080 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 03:45:25,267 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119130941
2025-04-14 03:45:25,454 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107347270
2025-04-14 03:45:25,641 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84829991
2025-04-14 03:45:25,827 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47188552
2025-04-14 03:45:26,014 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252063
2025-04-14 03:45:26,201 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804556
2025-04-14 03:45:26,388 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1553072
2025-04-14 03:45:26,575 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554834
2025-04-14 03:45:26,916 - __main__ - INFO - Non-zero entries after thresholding: 47188552
2025-04-14 03:45:26,917 - __main__ - INFO - Testing stepsize: 0
2025-04-14 03:45:27,537 - __main__ - INFO -   chosen_logps: -152.85394, rejected_logps: -153.87029
2025-04-14 03:45:27,538 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 03:45:27,796 - __main__ - INFO -   chosen_logps: -152.66510, rejected_logps: -153.89236
2025-04-14 03:45:27,796 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 03:45:28,056 - __main__ - INFO -   chosen_logps: -152.16411, rejected_logps: -154.33276
2025-04-14 03:45:28,056 - __main__ - INFO - Testing stepsize: 1
2025-04-14 03:45:28,318 - __main__ - INFO -   chosen_logps: -151.35703, rejected_logps: -154.63358
2025-04-14 03:45:28,318 - __main__ - INFO - Testing stepsize: 2
2025-04-14 03:45:28,584 - __main__ - INFO -   chosen_logps: -149.91510, rejected_logps: -155.58044
2025-04-14 03:45:28,584 - __main__ - INFO - Testing stepsize: 5
2025-04-14 03:45:28,853 - __main__ - INFO -   chosen_logps: -146.08894, rejected_logps: -157.61382
2025-04-14 03:45:28,853 - __main__ - INFO - Testing stepsize: 10
2025-04-14 03:45:29,122 - __main__ - INFO -   chosen_logps: -140.25458, rejected_logps: -161.54254
2025-04-14 03:45:29,122 - __main__ - INFO - Testing stepsize: 15
2025-04-14 03:45:29,392 - __main__ - INFO -   chosen_logps: -135.56778, rejected_logps: -165.63437
2025-04-14 03:45:29,974 - __main__ - INFO - Update scale: 0.007486111111111112
2025-04-14 03:45:30,056 - __main__ - INFO - Model weights updated successfully
2025-04-14 03:45:30,897 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:45:30,933 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:45:31,821 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 03:45:31,854 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 03:45:31,854 - __main__ - INFO - Skipping full model checkpoint for iteration 1
2025-04-14 03:45:31,855 - __main__ - INFO - Iteration 1 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 1
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 1
Iteration 1 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 2
===========================================
Iteration 2 is in valid list - performing full update
Finding noisy pairs for iteration 2...
2025-04-14 03:45:52,845 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 03:45:53,396 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:45:53,541 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:45:53,556 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 03:45:53,556 - __main__ - INFO - Loading dataset
2025-04-14 03:45:54,929 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 03:46:16,467 - __main__ - INFO - Processing dataset
2025-04-14 03:46:16,716 - __main__ - INFO - Searching for noisy pairs starting from index 370
2025-04-14 03:46:22,878 - __main__ - INFO - Processed 30 samples without finding a noisy pair
2025-04-14 03:46:43,385 - __main__ - INFO - Processed 130 samples without finding a noisy pair
2025-04-14 03:47:04,849 - __main__ - INFO - Processed 230 samples without finding a noisy pair
2025-04-14 03:47:24,152 - __main__ - INFO - Processed 330 samples without finding a noisy pair
2025-04-14 03:47:44,405 - __main__ - INFO - Processed 430 samples without finding a noisy pair
2025-04-14 03:48:04,032 - __main__ - INFO - Processed 530 samples without finding a noisy pair
2025-04-14 03:48:22,671 - __main__ - INFO - Processed 630 samples without finding a noisy pair
2025-04-14 03:48:28,142 - __main__ - INFO - Noisy pair found at index 1030: chosen_logps=-280.0000, rejected_logps=-282.0000
2025-04-14 03:48:28,143 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 03:48:28,210 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_2
2025-04-14 03:48:28,210 - __main__ - INFO - New dataset offset: 1031
Updated dataset offset to: 1031 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 2
Worker node 1 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 03:48:43,527 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]2025-04-14 03:48:43,861 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 03:48:44,049 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]2025-04-14 03:48:44,074 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]2025-04-14 03:48:44,219 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:48:44,234 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 03:48:44,234 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.25it/s]2025-04-14 03:48:44,406 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
2025-04-14 03:48:44,563 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:48:44,578 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 03:48:44,578 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:48:44,604 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:48:44,747 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:48:44,760 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 03:48:44,760 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:49:00,719 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 03:49:01,131 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 03:49:01,286 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 03:49:10,919 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 03:49:11,272 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 03:49:11,399 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 03:49:14,770 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 03:49:15,068 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 03:49:15,276 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 03:49:17,971 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 03:49:18,144 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 03:49:18,259 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 03:49:18,780 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 03:49:18,965 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 03:49:19,312 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 03:49:22,138 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 03:49:22,405 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 03:49:22,516 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 03:49:22,698 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 03:49:22,980 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 03:49:23,341 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 03:49:25,889 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 03:49:25,900 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 03:49:26,620 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 03:49:26,866 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 03:49:27,293 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 03:49:27,298 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 03:49:29,991 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 03:49:30,286 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 03:49:30,390 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 03:49:30,529 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 03:49:30,860 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 03:49:31,285 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 03:49:33,769 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 03:49:33,960 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 03:49:34,668 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 03:49:34,676 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 03:49:34,940 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 03:49:35,896 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 03:49:37,642 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 03:49:38,145 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 03:49:38,684 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 03:49:38,698 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 03:49:39,474 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 03:49:40,406 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 03:49:41,651 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 03:49:41,861 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 03:49:42,961 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 03:49:42,968 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 03:49:43,114 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 03:49:43,569 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 03:49:43,577 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 03:49:44,462 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 03:49:44,470 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 03:49:45,805 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 03:49:46,817 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 03:49:47,643 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 03:49:50,129 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 03:49:50,275 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 03:49:50,502 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 03:49:50,781 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 03:49:51,951 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 03:49:52,338 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 03:51:06,880 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 03:51:07,813 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 03:51:10,599 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 03:51:10,736 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 03:51:12,293 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 03:51:13,208 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 03:51:15,642 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 03:51:15,702 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 03:51:18,221 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 03:51:19,946 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 03:51:20,316 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 03:51:21,778 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 03:51:22,956 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 03:51:23,008 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 03:51:26,519 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 03:51:27,746 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 03:51:29,313 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 03:51:29,371 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 03:51:30,845 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 03:51:31,126 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 03:51:32,644 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 03:51:33,080 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 03:51:35,226 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 03:51:36,927 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 03:51:38,606 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 03:51:39,405 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 03:51:40,240 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 03:51:40,588 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 03:51:45,129 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 03:51:49,800 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 03:53:26.977925132 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e6fcec68446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e6f83fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e6f83fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7e6f83fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7e6f83fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e6f83fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e6fcedcf5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e6fcf894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e6fcf926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:53:26,328 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_0.npy
2025-04-14 03:53:26,328 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 133 negative comparisons
[E414 03:53:29.870979982 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7127a26a9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7127579cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7127579cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7127579cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7127579d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7127579d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7127a28105c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7127a3294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7127a3326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:53:29,135 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 03:53:29,223 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_1.npy
2025-04-14 03:53:29,223 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 112 negative comparisons
2025-04-14 03:53:32,266 - __main__ - INFO - Node 1: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 2
2025-04-14 03:53:42,205 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_2.npy
2025-04-14 03:53:42,205 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 120 negative comparisons
[E414 03:53:42.176187184 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7aca7b723446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7aca309cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7aca309cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7aca309cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7aca309d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7aca309d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7aca7b88a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7aca7c294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7aca7c326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 03:53:45,327 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 2
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 03:54:02,042 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 03:54:02,589 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:54:02,730 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:54:02,743 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 03:54:19,262 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 03:54:22,202 - __main__ - INFO - Loaded gradients from node 0: 133 negative comparisons
2025-04-14 03:54:22,436 - __main__ - INFO - Loaded gradients from node 1: 112 negative comparisons
2025-04-14 03:54:22,669 - __main__ - INFO - Loaded gradients from node 2: 120 negative comparisons
2025-04-14 03:54:22,828 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 03:54:23,019 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 03:54:23,209 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119132746
2025-04-14 03:54:23,399 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352680
2025-04-14 03:54:23,588 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84837406
2025-04-14 03:54:23,780 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47194770
2025-04-14 03:54:23,971 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22257406
2025-04-14 03:54:24,162 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802816
2025-04-14 03:54:24,354 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1549698
2025-04-14 03:54:24,545 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555038
2025-04-14 03:54:24,892 - __main__ - INFO - Non-zero entries after thresholding: 47194770
2025-04-14 03:54:24,892 - __main__ - INFO - Using relaxed comparison (count_negative: 365)
2025-04-14 03:54:24,892 - __main__ - INFO - Testing stepsize: 0
2025-04-14 03:54:25,512 - __main__ - INFO -   chosen_logps: -279.80750, rejected_logps: -281.87646
2025-04-14 03:54:25,512 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 03:54:25,762 - __main__ - INFO -   chosen_logps: -279.34195, rejected_logps: -281.96091
2025-04-14 03:54:25,762 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 03:54:26,012 - __main__ - INFO -   chosen_logps: -277.84827, rejected_logps: -282.59552
2025-04-14 03:54:26,012 - __main__ - INFO - Testing stepsize: 1
2025-04-14 03:54:26,266 - __main__ - INFO -   chosen_logps: -275.50439, rejected_logps: -284.13010
2025-04-14 03:54:26,266 - __main__ - INFO - Testing stepsize: 2
2025-04-14 03:54:26,523 - __main__ - INFO -   chosen_logps: -272.13132, rejected_logps: -286.17963
2025-04-14 03:54:26,523 - __main__ - INFO - Testing stepsize: 5
2025-04-14 03:54:26,782 - __main__ - INFO -   chosen_logps: -260.97638, rejected_logps: -292.19733
2025-04-14 03:54:26,782 - __main__ - INFO - Testing stepsize: 10
2025-04-14 03:54:27,042 - __main__ - INFO -   chosen_logps: -244.15744, rejected_logps: -303.12030
2025-04-14 03:54:27,042 - __main__ - INFO - Testing stepsize: 15
2025-04-14 03:54:27,301 - __main__ - INFO -   chosen_logps: -229.09207, rejected_logps: -313.71704
2025-04-14 03:54:27,874 - __main__ - INFO - Update scale: 0.007097222222222223
2025-04-14 03:54:27,875 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 03:54:27,955 - __main__ - INFO - Model weights updated successfully
2025-04-14 03:54:28,801 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:54:28,837 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:54:29,784 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 03:54:29,820 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 03:54:29,820 - __main__ - INFO - Skipping full model checkpoint for iteration 2
2025-04-14 03:54:29,820 - __main__ - INFO - Iteration 2 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 2
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 2
Iteration 2 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 3
===========================================
Iteration 3 is in valid list - performing full update
Finding noisy pairs for iteration 3...
2025-04-14 03:54:50,883 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 03:54:51,432 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:54:51,578 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 03:54:51,592 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 03:54:51,592 - __main__ - INFO - Loading dataset
2025-04-14 03:54:52,452 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 03:55:14,110 - __main__ - INFO - Processing dataset
2025-04-14 03:55:14,358 - __main__ - INFO - Searching for noisy pairs starting from index 1031
2025-04-14 03:55:29,288 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 03:55:42,874 - __main__ - INFO - Noisy pair found at index 1163: chosen_logps=-147.0000, rejected_logps=-149.0000
2025-04-14 03:55:42,875 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 03:55:42,942 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_3
2025-04-14 03:55:42,942 - __main__ - INFO - New dataset offset: 1164
Updated dataset offset to: 1164 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 3
Worker node 1 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 03:55:58,524 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 03:55:58,721 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 03:55:58,774 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.24it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.67it/s]2025-04-14 03:55:59,081 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.50it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 03:55:59,224 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:55:59,240 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 03:55:59,240 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 03:55:59,265 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:55:59,330 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 03:55:59,420 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:55:59,435 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 03:55:59,435 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:55:59,472 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 03:55:59,484 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 03:55:59,485 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 03:56:15,825 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 03:56:15,998 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 03:56:16,027 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 03:56:25,999 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 03:56:26,148 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 03:56:26,492 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 03:56:29,772 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 03:56:29,944 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 03:56:30,373 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 03:56:33,200 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 03:56:33,232 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 03:56:33,469 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 03:56:33,673 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 03:56:33,877 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 03:56:34,338 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 03:56:37,281 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 03:56:37,286 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 03:56:37,751 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 03:56:37,845 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 03:56:37,856 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 03:56:38,121 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 03:56:41,096 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 03:56:41,171 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 03:56:41,459 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 03:56:41,790 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 03:56:42,032 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 03:56:42,153 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 03:56:45,317 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 03:56:45,385 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 03:56:45,399 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 03:56:45,734 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 03:56:45,913 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 03:56:46,196 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 03:56:49,211 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 03:56:49,430 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 03:56:49,668 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 03:56:50,127 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 03:56:50,205 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 03:56:50,487 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 03:56:53,497 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 03:56:53,935 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 03:56:53,971 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 03:56:54,290 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 03:56:54,347 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 03:56:54,865 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 03:56:57,518 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 03:56:57,653 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 03:56:58,356 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 03:56:58,362 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 03:56:58,433 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 03:56:58,439 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 03:56:58,587 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 03:56:59,117 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 03:56:59,117 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 03:57:01,545 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 03:57:01,961 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 03:57:02,676 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 03:57:05,811 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 03:57:05,846 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 03:57:05,910 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 03:57:05,923 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 03:57:07,349 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 03:57:07,371 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 03:58:29,420 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 03:58:29,437 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 03:58:33,785 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 03:58:33,891 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 03:58:34,334 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 03:58:34,704 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 03:58:37,541 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 03:58:37,625 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 03:58:40,525 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 03:58:41,521 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 03:58:41,834 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 03:58:43,495 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 03:58:45,952 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 03:58:46,541 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 03:58:49,682 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 03:58:50,905 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 03:58:51,618 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 03:58:54,234 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 03:58:55,067 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 03:58:55,145 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 03:58:58,464 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 03:58:59,436 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 03:59:01,629 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 03:59:02,226 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 03:59:02,836 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 03:59:03,450 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 03:59:04,170 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 03:59:04,467 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 03:59:06,371 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 03:59:07,950 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[rank7]:[E414 04:00:55.754180826 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a1598f0e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7a154e1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7a154e1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7a154e1cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7a154e1d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a154e1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7a15990755c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7a1599a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7a1599b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:00:55.780490676 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7214cb09d446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7214803cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7214803cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7214803cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7214803d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7214803d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7214cb2045c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7214cbc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7214cbd26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:00:56,008 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_0.npy
2025-04-14 04:00:56,008 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 132 negative comparisons
2025-04-14 04:00:58,876 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 04:01:04,175 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_2.npy
2025-04-14 04:01:04,176 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 126 negative comparisons
Waiting for worker nodes to complete gradient computation...
2025-04-14 04:01:07,064 - __main__ - INFO - Node 2: Gradient computation completed successfully
[E414 04:01:07.108092058 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7dfad2623446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7dfa879cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7dfa879cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7dfa879cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7dfa879d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7dfa879d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7dfad278a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7dfad3094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7dfad3126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:01:07.111400133 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77d6a63be446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x77d65b7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x77d65b7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x77d65b7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x77d65b7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x77d65b7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x77d6a65255c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x77d6a6e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x77d6a6f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:01:07,476 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_1.npy
2025-04-14 04:01:07,476 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 145 negative comparisons
2025-04-14 04:01:10,259 - __main__ - INFO - Node 1: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 3
Worker node 1 successfully completed gradient computation for iteration 3
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 04:01:27,636 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 04:01:28,184 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:01:28,324 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:01:28,336 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 04:01:44,806 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 04:01:47,682 - __main__ - INFO - Loaded gradients from node 0: 132 negative comparisons
2025-04-14 04:01:47,916 - __main__ - INFO - Loaded gradients from node 1: 145 negative comparisons
2025-04-14 04:01:48,151 - __main__ - INFO - Loaded gradients from node 2: 126 negative comparisons
2025-04-14 04:01:48,312 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 04:01:48,504 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 04:01:48,696 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119126211
2025-04-14 04:01:48,888 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107344167
2025-04-14 04:01:49,080 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84832852
2025-04-14 04:01:49,271 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47190931
2025-04-14 04:01:49,462 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22251189
2025-04-14 04:01:49,653 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802306
2025-04-14 04:01:49,843 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552107
2025-04-14 04:01:50,034 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555157
2025-04-14 04:01:50,383 - __main__ - INFO - Non-zero entries after thresholding: 47190931
2025-04-14 04:01:50,383 - __main__ - INFO - Testing stepsize: 0
2025-04-14 04:01:51,127 - __main__ - INFO -   chosen_logps: -147.82260, rejected_logps: -147.74109
2025-04-14 04:01:51,127 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 04:01:51,508 - __main__ - INFO -   chosen_logps: -147.48018, rejected_logps: -147.90276
2025-04-14 04:01:51,508 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 04:01:51,889 - __main__ - INFO -   chosen_logps: -145.91547, rejected_logps: -148.33925
2025-04-14 04:01:51,889 - __main__ - INFO - Testing stepsize: 1
2025-04-14 04:01:52,273 - __main__ - INFO -   chosen_logps: -144.65880, rejected_logps: -149.05840
2025-04-14 04:01:52,273 - __main__ - INFO - Testing stepsize: 2
2025-04-14 04:01:52,660 - __main__ - INFO -   chosen_logps: -141.97620, rejected_logps: -150.08868
2025-04-14 04:01:52,660 - __main__ - INFO - Testing stepsize: 5
2025-04-14 04:01:53,049 - __main__ - INFO -   chosen_logps: -134.10715, rejected_logps: -153.77791
2025-04-14 04:01:53,049 - __main__ - INFO - Testing stepsize: 10
2025-04-14 04:01:53,440 - __main__ - INFO -   chosen_logps: -122.67955, rejected_logps: -159.70999
2025-04-14 04:01:53,440 - __main__ - INFO - Testing stepsize: 15
2025-04-14 04:01:53,832 - __main__ - INFO -   chosen_logps: -113.56252, rejected_logps: -166.17683
2025-04-14 04:01:54,592 - __main__ - INFO - Update scale: 0.007836111111111111
2025-04-14 04:01:54,674 - __main__ - INFO - Model weights updated successfully
2025-04-14 04:01:55,610 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:01:55,649 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:01:56,538 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 04:01:56,571 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 04:01:56,571 - __main__ - INFO - Skipping full model checkpoint for iteration 3
2025-04-14 04:01:56,571 - __main__ - INFO - Iteration 3 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 3
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 3
Iteration 3 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 4
===========================================
Iteration 4 is in valid list - performing full update
Finding noisy pairs for iteration 4...
2025-04-14 04:02:17,544 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.31it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 04:02:18,090 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:02:18,235 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:02:18,249 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 04:02:18,249 - __main__ - INFO - Loading dataset
2025-04-14 04:02:19,166 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 04:02:40,690 - __main__ - INFO - Processing dataset
2025-04-14 04:02:40,935 - __main__ - INFO - Searching for noisy pairs starting from index 1164
2025-04-14 04:02:47,862 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 04:03:09,222 - __main__ - INFO - Processed 136 samples without finding a noisy pair
2025-04-14 04:03:19,357 - __main__ - INFO - Noisy pair found at index 1344: chosen_logps=-268.0000, rejected_logps=-270.0000
2025-04-14 04:03:19,358 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 04:03:19,423 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_4
2025-04-14 04:03:19,424 - __main__ - INFO - New dataset offset: 1345
Updated dataset offset to: 1345 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 4
Worker node 1 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 04:03:35,107 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 04:03:35,172 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 04:03:35,222 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 04:03:35,662 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
2025-04-14 04:03:35,716 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:03:35,777 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:03:35,806 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:03:35,821 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 04:03:35,821 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:03:35,870 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:03:35,885 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 04:03:35,885 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:03:35,920 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:03:35,933 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 04:03:35,933 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:03:52,340 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 04:03:52,465 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 04:03:52,493 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 04:04:02,500 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 04:04:02,569 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 04:04:02,817 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 04:04:06,360 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 04:04:06,383 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 04:04:06,883 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 04:04:09,244 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 04:04:09,599 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 04:04:09,834 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 04:04:10,351 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 04:04:10,432 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 04:04:10,791 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 04:04:13,032 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 04:04:13,130 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 04:04:13,918 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 04:04:14,242 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 04:04:14,647 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 04:04:14,687 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 04:04:17,572 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 04:04:17,600 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 04:04:17,884 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 04:04:18,195 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 04:04:18,549 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 04:04:18,567 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 04:04:21,210 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 04:04:21,755 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 04:04:22,052 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 04:04:22,382 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 04:04:22,678 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 04:04:22,945 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 04:04:25,359 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 04:04:25,558 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 04:04:26,101 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 04:04:26,430 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 04:04:26,668 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 04:04:27,353 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 04:04:29,663 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 04:04:30,178 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 04:04:30,296 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 04:04:30,690 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 04:04:30,949 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 04:04:31,936 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 04:04:33,745 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 04:04:33,828 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 04:04:34,787 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 04:04:35,014 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 04:04:35,022 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 04:04:35,204 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 04:04:35,211 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 04:04:36,021 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 04:04:36,026 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 04:04:37,695 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 04:04:38,271 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 04:04:39,362 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 04:04:42,205 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 04:04:42,380 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 04:04:42,994 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 04:04:43,064 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 04:04:43,547 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 04:04:43,925 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 04:06:01,913 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 04:06:03,109 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 04:06:04,088 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 04:06:04,990 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 04:06:06,293 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 04:06:08,130 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 04:06:08,450 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 04:06:08,694 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 04:06:08,934 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 04:06:12,258 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 04:06:14,078 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 04:06:16,302 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 04:06:16,885 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 04:06:18,212 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 04:06:18,252 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 04:06:20,307 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 04:06:21,233 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 04:06:22,691 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 04:06:24,409 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 04:06:26,116 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 04:06:28,261 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 04:06:28,497 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 04:06:29,571 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 04:06:32,564 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 04:06:32,650 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 04:06:34,298 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 04:06:34,514 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 04:06:35,335 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 04:06:37,224 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 04:06:39,689 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 04:08:22.047048935 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70be458a1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70bdfabcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70bdfabcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x70bdfabcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x70bdfabd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70bdfabd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70be45a085c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70be46494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70be46526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:08:22.071314352 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c316a447446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c311f7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c311f7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7c311f7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7c311f7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c311f7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c316a5ae5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c316b094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c316b126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:08:22,440 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_1.npy
2025-04-14 04:08:22,441 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 191 negative comparisons
[E414 04:08:23.765304624 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e490e5ce446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e48c39cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e48c39cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7e48c39cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7e48c39d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e48c39d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e490e7355c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e490f094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e490f126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:08:24,074 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_0.npy
2025-04-14 04:08:24,075 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 209 negative comparisons
2025-04-14 04:08:25,174 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 04:08:26,895 - __main__ - INFO - Node 0: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 4
[E414 04:08:32.229069703 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72f5a0ffd446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72f5563cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72f5563cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x72f5563cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x72f5563d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72f5563d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72f5a11645c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72f5a1c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72f5a1d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:08:32,466 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_2.npy
2025-04-14 04:08:32,466 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 197 negative comparisons
Waiting for worker nodes to complete gradient computation...
2025-04-14 04:08:35,365 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 4
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 04:08:52,921 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 04:08:53,471 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:08:53,612 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:08:53,624 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 04:09:10,047 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 04:09:12,978 - __main__ - INFO - Loaded gradients from node 0: 209 negative comparisons
2025-04-14 04:09:13,216 - __main__ - INFO - Loaded gradients from node 1: 191 negative comparisons
2025-04-14 04:09:13,454 - __main__ - INFO - Loaded gradients from node 2: 197 negative comparisons
2025-04-14 04:09:13,615 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 04:09:13,809 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 04:09:14,003 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119132849
2025-04-14 04:09:14,197 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107351947
2025-04-14 04:09:14,392 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84832393
2025-04-14 04:09:14,587 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47192849
2025-04-14 04:09:14,782 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252100
2025-04-14 04:09:14,978 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8805373
2025-04-14 04:09:15,173 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550861
2025-04-14 04:09:15,369 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554642
2025-04-14 04:09:15,721 - __main__ - INFO - Non-zero entries after thresholding: 47192849
2025-04-14 04:09:15,722 - __main__ - INFO - Testing stepsize: 0
2025-04-14 04:09:16,379 - __main__ - INFO -   chosen_logps: -267.81516, rejected_logps: -269.54068
2025-04-14 04:09:16,379 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 04:09:16,666 - __main__ - INFO -   chosen_logps: -267.51520, rejected_logps: -269.99164
2025-04-14 04:09:16,666 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 04:09:16,954 - __main__ - INFO -   chosen_logps: -265.33746, rejected_logps: -271.55463
2025-04-14 04:09:16,954 - __main__ - INFO - Testing stepsize: 1
2025-04-14 04:09:17,244 - __main__ - INFO -   chosen_logps: -263.46817, rejected_logps: -273.87146
2025-04-14 04:09:17,244 - __main__ - INFO - Testing stepsize: 2
2025-04-14 04:09:17,539 - __main__ - INFO -   chosen_logps: -259.85379, rejected_logps: -277.72113
2025-04-14 04:09:17,539 - __main__ - INFO - Testing stepsize: 5
2025-04-14 04:09:17,835 - __main__ - INFO -   chosen_logps: -248.21011, rejected_logps: -289.89050
2025-04-14 04:09:17,835 - __main__ - INFO - Testing stepsize: 10
2025-04-14 04:09:18,133 - __main__ - INFO -   chosen_logps: -231.23389, rejected_logps: -311.91434
2025-04-14 04:09:18,133 - __main__ - INFO - Testing stepsize: 15
2025-04-14 04:09:18,431 - __main__ - INFO -   chosen_logps: -216.56079, rejected_logps: -337.46207
2025-04-14 04:09:19,066 - __main__ - INFO - Update scale: 0.011608333333333335
2025-04-14 04:09:19,147 - __main__ - INFO - Model weights updated successfully
2025-04-14 04:09:19,992 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:09:20,027 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:09:21,220 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 04:09:21,256 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 04:09:21,256 - __main__ - INFO - Skipping full model checkpoint for iteration 4
2025-04-14 04:09:21,256 - __main__ - INFO - Iteration 4 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 4
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 4
Iteration 4 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 5
===========================================
Iteration 5 is in valid list - performing full update
Finding noisy pairs for iteration 5...
2025-04-14 04:09:42,600 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.20it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 04:09:43,148 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:09:43,293 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:09:43,308 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 04:09:43,308 - __main__ - INFO - Loading dataset
2025-04-14 04:09:44,334 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 04:10:06,115 - __main__ - INFO - Processing dataset
2025-04-14 04:10:06,369 - __main__ - INFO - Searching for noisy pairs starting from index 1345
2025-04-14 04:10:17,600 - __main__ - INFO - Processed 55 samples without finding a noisy pair
2025-04-14 04:10:38,242 - __main__ - INFO - Processed 155 samples without finding a noisy pair
2025-04-14 04:10:43,691 - __main__ - INFO - Noisy pair found at index 1530: chosen_logps=-360.0000, rejected_logps=-362.0000
2025-04-14 04:10:43,692 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 04:10:43,760 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_5
2025-04-14 04:10:43,761 - __main__ - INFO - New dataset offset: 1531
Updated dataset offset to: 1531 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 5
Worker node 1 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 04:10:59,471 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 04:10:59,642 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 04:10:59,762 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.40it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 04:11:00,028 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.30it/s]2025-04-14 04:11:00,172 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:11:00,186 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:11:00,187 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 04:11:00,187 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 04:11:00,317 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:11:00,341 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:11:00,356 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 04:11:00,356 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:11:00,458 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:11:00,471 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 04:11:00,471 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:11:16,756 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 04:11:16,895 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 04:11:17,002 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 04:11:26,953 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 04:11:27,159 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 04:11:27,240 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 04:11:31,027 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 04:11:31,102 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 04:11:31,511 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 04:11:33,913 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 04:11:33,983 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 04:11:34,346 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 04:11:34,961 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 04:11:35,086 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 04:11:35,487 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 04:11:38,233 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 04:11:38,645 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 04:11:38,840 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 04:11:39,115 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 04:11:39,191 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 04:11:39,323 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 04:11:42,280 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 04:11:42,312 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 04:11:42,798 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 04:11:42,869 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 04:11:43,100 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 04:11:43,113 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 04:11:46,283 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 04:11:46,627 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 04:11:46,707 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 04:11:46,791 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 04:11:47,155 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 04:11:47,355 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 04:11:49,928 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 04:11:50,396 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 04:11:50,432 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 04:11:50,659 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 04:11:51,272 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 04:11:51,707 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 04:11:53,855 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 04:11:54,657 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 04:11:54,831 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 04:11:54,944 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 04:11:55,809 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 04:11:56,066 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 04:11:58,507 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 04:11:58,531 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 04:11:59,192 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 04:11:59,216 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 04:11:59,218 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 04:12:00,164 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 04:12:00,167 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 04:12:00,310 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 04:12:00,314 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 04:12:02,354 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 04:12:03,312 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 04:12:03,408 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 04:12:06,311 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 04:12:06,328 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 04:12:07,435 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 04:12:07,480 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 04:12:08,039 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 04:12:08,107 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 04:13:26,240 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 04:13:26,695 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 04:13:28,917 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 04:13:30,298 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 04:13:32,098 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 04:13:34,172 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 04:13:34,468 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 04:13:36,188 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 04:13:39,387 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 04:13:40,632 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 04:13:42,375 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 04:13:42,481 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 04:13:42,753 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 04:13:43,136 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 04:13:44,243 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 04:13:46,173 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 04:13:46,879 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 04:13:49,711 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 04:13:50,624 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 04:13:51,658 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 04:13:55,589 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 04:13:55,614 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 04:13:56,444 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 04:13:57,932 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 04:13:59,781 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 04:14:00,165 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 04:14:00,517 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 04:14:00,703 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 04:14:01,490 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 04:14:02,587 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 04:15:50,426 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_0.npy
2025-04-14 04:15:50,426 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 105 negative comparisons
[E414 04:15:52.125749749 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f5b343b9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f5ae97cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f5ae97cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f5ae97cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f5ae97d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f5ae97d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f5b345205c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f5b34e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f5b34f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:15:52.128220439 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70daf0fbe446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70daa63cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70daa63cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x70daa63cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x70daa63d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70daa63d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70daf11255c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70daf1a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70daf1b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:15:52,512 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_1.npy
2025-04-14 04:15:52,513 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 89 negative comparisons
2025-04-14 04:15:53,343 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 04:15:55.930459871 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7000add04446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x700062fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x700062fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x700062fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x700062fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x700062fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7000ade6b5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7000ae894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7000ae926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:15:55,167 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_2.npy
2025-04-14 04:15:55,167 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 98 negative comparisons
2025-04-14 04:15:55,412 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 04:15:58,105 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 5
Worker node 2 successfully completed gradient computation for iteration 5
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 04:16:14,317 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 04:16:14,864 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:16:15,004 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:16:15,017 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 04:16:31,474 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 04:16:34,302 - __main__ - INFO - Loaded gradients from node 0: 105 negative comparisons
2025-04-14 04:16:34,531 - __main__ - INFO - Loaded gradients from node 1: 89 negative comparisons
2025-04-14 04:16:34,761 - __main__ - INFO - Loaded gradients from node 2: 98 negative comparisons
2025-04-14 04:16:34,918 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 04:16:35,105 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 04:16:35,291 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119129828
2025-04-14 04:16:35,478 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107349787
2025-04-14 04:16:35,664 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84836543
2025-04-14 04:16:35,851 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47198739
2025-04-14 04:16:36,038 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22253209
2025-04-14 04:16:36,224 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8805611
2025-04-14 04:16:36,411 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551048
2025-04-14 04:16:36,597 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554800
2025-04-14 04:16:36,938 - __main__ - INFO - Non-zero entries after thresholding: 47198739
2025-04-14 04:16:36,938 - __main__ - INFO - Using relaxed comparison (count_negative: 292)
2025-04-14 04:16:36,938 - __main__ - INFO - Testing stepsize: 0
2025-04-14 04:16:37,604 - __main__ - INFO -   chosen_logps: -360.32141, rejected_logps: -361.38934
2025-04-14 04:16:37,605 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 04:16:37,904 - __main__ - INFO -   chosen_logps: -359.96167, rejected_logps: -361.38025
2025-04-14 04:16:37,904 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 04:16:38,204 - __main__ - INFO -   chosen_logps: -357.81287, rejected_logps: -362.68906
2025-04-14 04:16:38,205 - __main__ - INFO - Testing stepsize: 1
2025-04-14 04:16:38,507 - __main__ - INFO -   chosen_logps: -354.84348, rejected_logps: -364.59229
2025-04-14 04:16:38,507 - __main__ - INFO - Testing stepsize: 2
2025-04-14 04:16:38,814 - __main__ - INFO -   chosen_logps: -349.77460, rejected_logps: -366.87994
2025-04-14 04:16:38,814 - __main__ - INFO - Testing stepsize: 5
2025-04-14 04:16:39,123 - __main__ - INFO -   chosen_logps: -334.88614, rejected_logps: -374.64954
2025-04-14 04:16:39,123 - __main__ - INFO - Testing stepsize: 10
2025-04-14 04:16:39,434 - __main__ - INFO -   chosen_logps: -312.01862, rejected_logps: -389.00195
2025-04-14 04:16:39,434 - __main__ - INFO - Testing stepsize: 15
2025-04-14 04:16:39,745 - __main__ - INFO -   chosen_logps: -293.79370, rejected_logps: -406.46286
2025-04-14 04:16:40,391 - __main__ - INFO - Update scale: 0.005677777777777778
2025-04-14 04:16:40,393 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 04:16:40,472 - __main__ - INFO - Model weights updated successfully
2025-04-14 04:16:41,312 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:16:41,349 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:16:42,234 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 04:16:42,275 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 04:16:42,275 - __main__ - INFO - Skipping full model checkpoint for iteration 5
2025-04-14 04:16:42,275 - __main__ - INFO - Iteration 5 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 5
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 5
Iteration 5 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 6
===========================================
Iteration 6 is in valid list - performing full update
Finding noisy pairs for iteration 6...
2025-04-14 04:17:03,380 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.37it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 04:17:03,930 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:17:04,075 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:17:04,089 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 04:17:04,089 - __main__ - INFO - Loading dataset
2025-04-14 04:17:04,984 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 04:17:26,518 - __main__ - INFO - Processing dataset
2025-04-14 04:17:26,766 - __main__ - INFO - Searching for noisy pairs starting from index 1531
2025-04-14 04:17:40,840 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 04:17:45,390 - __main__ - INFO - Noisy pair found at index 1624: chosen_logps=-127.5000, rejected_logps=-130.0000
2025-04-14 04:17:45,391 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 04:17:45,458 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_6
2025-04-14 04:17:45,459 - __main__ - INFO - New dataset offset: 1625
Updated dataset offset to: 1625 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 6
Worker node 1 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 04:18:00,919 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 04:18:01,243 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 04:18:01,365 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.43it/s]2025-04-14 04:18:01,477 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  6.94it/s]2025-04-14 04:18:01,622 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:18:01,637 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 04:18:01,637 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.32it/s]2025-04-14 04:18:01,787 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
2025-04-14 04:18:01,919 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:18:01,940 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:18:01,955 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 04:18:01,955 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:18:02,060 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:18:02,073 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 04:18:02,073 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:18:18,160 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 04:18:18,543 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 04:18:18,620 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 04:18:28,498 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 04:18:28,753 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 04:18:28,821 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 04:18:32,557 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 04:18:32,607 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 04:18:32,708 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 04:18:35,317 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 04:18:36,007 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 04:18:36,057 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 04:18:36,493 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 04:18:36,509 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 04:18:37,063 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 04:18:39,863 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 04:18:39,876 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 04:18:40,178 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 04:18:40,295 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 04:18:40,607 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 04:18:41,111 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 04:18:43,786 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 04:18:43,925 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 04:18:44,105 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 04:18:44,445 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 04:18:44,536 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 04:18:44,942 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 04:18:47,474 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 04:18:47,955 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 04:18:48,097 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 04:18:48,555 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 04:18:48,578 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 04:18:49,060 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 04:18:51,326 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 04:18:51,775 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 04:18:52,102 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 04:18:52,308 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 04:18:53,108 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 04:18:53,172 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 04:18:55,311 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 04:18:56,098 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 04:18:56,539 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 04:18:56,790 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 04:18:57,291 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 04:18:57,681 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 04:18:59,798 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 04:19:00,473 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 04:19:00,677 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 04:19:01,210 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 04:19:01,214 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 04:19:01,490 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 04:19:01,498 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 04:19:01,968 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 04:19:01,974 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 04:19:04,297 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 04:19:05,044 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 04:19:05,273 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 04:19:08,416 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 04:19:08,481 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 04:19:09,121 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 04:19:09,547 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 04:19:09,763 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 04:19:09,777 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 04:20:30,254 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 04:20:33,785 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 04:20:34,680 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 04:20:35,475 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 04:20:35,741 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 04:20:37,211 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 04:20:40,478 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 04:20:40,911 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 04:20:44,422 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 04:20:44,577 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 04:20:45,044 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 04:20:45,763 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 04:20:47,978 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 04:20:48,058 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 04:20:48,235 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 04:20:52,397 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 04:20:53,264 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 04:20:54,608 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 04:20:57,035 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 04:20:58,501 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 04:20:59,243 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 04:20:59,661 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 04:21:01,102 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 04:21:03,900 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 04:21:04,386 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 04:21:04,431 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 04:21:04,811 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 04:21:05,869 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 04:21:06,914 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 04:21:08,127 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 04:22:58.451126419 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78b225746446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78b1dabcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78b1dabcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x78b1dabcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x78b1dabd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78b1dabd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78b2258ad5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78b226294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78b226326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:22:58.455098159 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78e870ede446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78e8261cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78e8261cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x78e8261cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x78e8261d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78e8261d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78e8710455c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78e871a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78e871b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:22:58,693 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_0.npy
2025-04-14 04:22:58,693 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 154 negative comparisons
2025-04-14 04:22:59,221 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_1.npy
2025-04-14 04:22:59,221 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 153 negative comparisons
2025-04-14 04:23:01,643 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 04:23:02,107 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 04:23:03.177726746 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70f04d25c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70f0025cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70f0025cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x70f0025cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x70f0025d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70f0025d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70f04d3c35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70f04de94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70f04df26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:23:03.194120930 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e86cbb78446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e8680fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e8680fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7e8680fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7e8680fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e8680fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e86cbcdf5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e86cc894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e86cc926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:23:03.196749303 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72a6aec31446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72a663fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72a663fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x72a663fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x72a663fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72a663fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72a6aed985c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72a6af894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72a6af926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:23:03,485 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_2.npy
2025-04-14 04:23:03,485 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 156 negative comparisons
2025-04-14 04:23:06,390 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 6
Worker node 2 successfully completed gradient computation for iteration 6
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 04:23:23,681 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.31it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.72it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
2025-04-14 04:23:24,226 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:23:24,366 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:23:24,379 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 04:23:40,898 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 04:23:43,799 - __main__ - INFO - Loaded gradients from node 0: 154 negative comparisons
2025-04-14 04:23:44,030 - __main__ - INFO - Loaded gradients from node 1: 153 negative comparisons
2025-04-14 04:23:44,254 - __main__ - INFO - Loaded gradients from node 2: 156 negative comparisons
2025-04-14 04:23:44,412 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 04:23:44,599 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 04:23:44,785 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119136714
2025-04-14 04:23:44,971 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107350734
2025-04-14 04:23:45,158 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84837198
2025-04-14 04:23:45,344 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47194867
2025-04-14 04:23:45,530 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252437
2025-04-14 04:23:45,717 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8805503
2025-04-14 04:23:45,904 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551255
2025-04-14 04:23:46,092 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553348
2025-04-14 04:23:46,436 - __main__ - INFO - Non-zero entries after thresholding: 47194867
2025-04-14 04:23:46,437 - __main__ - INFO - Testing stepsize: 0
2025-04-14 04:23:47,167 - __main__ - INFO -   chosen_logps: -127.93055, rejected_logps: -129.48669
2025-04-14 04:23:47,167 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 04:23:47,532 - __main__ - INFO -   chosen_logps: -127.35094, rejected_logps: -129.48666
2025-04-14 04:23:47,533 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 04:23:47,898 - __main__ - INFO -   chosen_logps: -125.47334, rejected_logps: -129.96547
2025-04-14 04:23:47,898 - __main__ - INFO - Testing stepsize: 1
2025-04-14 04:23:48,264 - __main__ - INFO -   chosen_logps: -123.46382, rejected_logps: -130.27679
2025-04-14 04:23:48,264 - __main__ - INFO - Testing stepsize: 2
2025-04-14 04:23:48,636 - __main__ - INFO -   chosen_logps: -120.27090, rejected_logps: -131.32402
2025-04-14 04:23:48,636 - __main__ - INFO - Testing stepsize: 5
2025-04-14 04:23:49,009 - __main__ - INFO -   chosen_logps: -110.01485, rejected_logps: -134.16281
2025-04-14 04:23:49,009 - __main__ - INFO - Testing stepsize: 10
2025-04-14 04:23:49,382 - __main__ - INFO -   chosen_logps: -95.60356, rejected_logps: -138.81491
2025-04-14 04:23:49,382 - __main__ - INFO - Testing stepsize: 15
2025-04-14 04:23:49,757 - __main__ - INFO -   chosen_logps: -84.37030, rejected_logps: -143.55559
2025-04-14 04:23:50,491 - __main__ - INFO - Update scale: 0.00900277777777778
2025-04-14 04:23:50,571 - __main__ - INFO - Model weights updated successfully
2025-04-14 04:23:51,416 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:23:51,453 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:23:52,334 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 04:23:52,369 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 04:23:52,369 - __main__ - INFO - Skipping full model checkpoint for iteration 6
2025-04-14 04:23:52,369 - __main__ - INFO - Iteration 6 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 6
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 6
Iteration 6 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 7
===========================================
Iteration 7 is in valid list - performing full update
Finding noisy pairs for iteration 7...
2025-04-14 04:24:13,324 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 04:24:13,869 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:24:14,014 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:24:14,028 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 04:24:14,028 - __main__ - INFO - Loading dataset
2025-04-14 04:24:14,991 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 04:24:36,500 - __main__ - INFO - Processing dataset
2025-04-14 04:24:36,746 - __main__ - INFO - Searching for noisy pairs starting from index 1625
2025-04-14 04:24:44,348 - __main__ - INFO - Noisy pair found at index 1663: chosen_logps=-49.7500, rejected_logps=-50.5000
2025-04-14 04:24:44,349 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 04:24:44,416 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_7
2025-04-14 04:24:44,417 - __main__ - INFO - New dataset offset: 1664
Updated dataset offset to: 1664 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 7
Worker node 2 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 04:24:58,664 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]2025-04-14 04:24:59,014 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 04:24:59,042 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]2025-04-14 04:24:59,215 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]2025-04-14 04:24:59,358 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:24:59,373 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 04:24:59,373 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 04:24:59,559 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:24:59,598 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:24:59,713 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:24:59,728 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 04:24:59,728 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:24:59,740 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:24:59,752 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 04:24:59,752 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:25:15,959 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 04:25:16,307 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 04:25:16,335 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 04:25:26,169 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 04:25:26,347 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 04:25:26,522 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 04:25:30,403 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 04:25:30,457 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 04:25:30,716 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 04:25:33,220 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 04:25:33,509 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 04:25:33,650 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 04:25:34,341 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 04:25:34,402 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 04:25:34,747 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 04:25:37,826 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 04:25:38,064 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 04:25:38,119 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 04:25:38,157 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 04:25:38,306 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 04:25:38,612 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 04:25:41,908 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 04:25:41,998 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 04:25:41,996 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 04:25:42,051 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 04:25:42,457 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 04:25:42,472 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 04:25:45,740 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 04:25:45,880 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 04:25:45,915 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 04:25:45,977 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 04:25:46,595 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 04:25:46,656 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 04:25:49,292 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 04:25:49,656 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 04:25:49,746 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 04:25:49,855 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 04:25:50,812 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 04:25:50,988 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 04:25:53,270 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 04:25:53,767 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 04:25:54,123 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 04:25:54,623 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 04:25:54,959 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 04:25:55,051 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 04:25:57,587 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 04:25:58,023 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 04:25:58,722 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 04:25:59,055 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 04:25:59,062 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 04:25:59,161 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 04:25:59,166 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 04:25:59,452 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 04:25:59,459 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 04:26:02,239 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 04:26:02,390 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 04:26:02,771 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 04:26:06,408 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 04:26:06,509 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 04:26:06,823 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 04:26:06,910 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 04:26:07,104 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 04:26:07,224 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 04:27:28,237 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 04:27:29,508 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 04:27:32,651 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 04:27:33,410 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 04:27:34,295 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 04:27:34,446 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 04:27:38,247 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 04:27:40,463 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 04:27:41,597 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 04:27:41,711 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 04:27:42,046 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 04:27:43,846 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 04:27:44,446 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 04:27:45,306 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 04:27:46,386 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 04:27:48,861 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 04:27:49,105 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 04:27:49,945 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 04:27:54,600 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 04:27:54,812 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 04:27:59,241 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 04:27:59,342 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 04:28:00,084 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 04:28:02,047 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 04:28:02,213 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 04:28:02,398 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 04:28:02,708 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 04:28:03,084 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 04:28:03,105 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 04:28:06,545 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 04:29:55,789 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_0.npy
2025-04-14 04:29:55,790 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 83 negative comparisons
[E414 04:29:55.742937494 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x707b6bb93446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x707b20fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x707b20fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x707b20fcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x707b20fd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x707b20fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x707b6bcee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x707b6c694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x707b6c726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:29:55.754492853 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fdf1db28446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7fded2dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7fded2dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7fded2dcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7fded2dd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fded2dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7fdf1dc8f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7fdf1e694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7fdf1e726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:29:56.963647791 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7391fff5c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7391b53cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7391b53cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7391b53cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7391b53d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7391b53d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7392000c35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x739200a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x739200b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:29:56.965369572 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7973b2d02446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x797367fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x797367fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x797367fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x797367fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x797367fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7973b2e695c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7973b3894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7973b3926850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank6]:[E414 04:29:56.979967992 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76801794d446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x767fccdcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x767fccdcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x767fccdcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x767fccdd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x767fccdd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x768017ab45c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x768018494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x768018526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:29:56,347 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_1.npy
2025-04-14 04:29:56,347 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 80 negative comparisons
2025-04-14 04:29:58,951 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 04:29:59,158 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 04:30:02,688 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_2.npy
2025-04-14 04:30:02,689 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 84 negative comparisons
[E414 04:30:02.640828680 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c03f6804446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c03abbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c03abbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7c03abbcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7c03abbd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c03abbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c03f696b5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c03f7494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c03f7526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:30:02.651210017 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e4a41d47446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e49f71cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e49f71cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7e49f71cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7e49f71d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e49f71d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e4a41eae5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e4a42a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e4a42b26850 in /lib/x86_64-linux-gnu/libc.so.6)

Worker node 1 successfully completed gradient computation for iteration 7
Waiting for worker nodes to complete gradient computation...
2025-04-14 04:30:05,824 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 7
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 04:30:24,002 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 04:30:24,550 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:30:24,691 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:30:24,703 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 04:30:41,159 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 04:30:44,013 - __main__ - INFO - Loaded gradients from node 0: 83 negative comparisons
2025-04-14 04:30:44,242 - __main__ - INFO - Loaded gradients from node 1: 80 negative comparisons
2025-04-14 04:30:44,472 - __main__ - INFO - Loaded gradients from node 2: 84 negative comparisons
2025-04-14 04:30:44,630 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 04:30:44,817 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 04:30:45,003 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119131087
2025-04-14 04:30:45,190 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107344760
2025-04-14 04:30:45,376 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84826626
2025-04-14 04:30:45,563 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47188907
2025-04-14 04:30:45,750 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255211
2025-04-14 04:30:45,937 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8807553
2025-04-14 04:30:46,124 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551651
2025-04-14 04:30:46,310 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554373
2025-04-14 04:30:46,651 - __main__ - INFO - Non-zero entries after thresholding: 47188907
2025-04-14 04:30:46,651 - __main__ - INFO - Using relaxed comparison (count_negative: 247)
2025-04-14 04:30:46,651 - __main__ - INFO - Testing stepsize: 0
2025-04-14 04:30:47,371 - __main__ - INFO -   chosen_logps: -49.78992, rejected_logps: -50.12742
2025-04-14 04:30:47,371 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 04:30:47,729 - __main__ - INFO -   chosen_logps: -49.78342, rejected_logps: -50.19092
2025-04-14 04:30:47,729 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 04:30:48,088 - __main__ - INFO -   chosen_logps: -49.51645, rejected_logps: -50.42824
2025-04-14 04:30:48,088 - __main__ - INFO - Testing stepsize: 1
2025-04-14 04:30:48,451 - __main__ - INFO -   chosen_logps: -49.19465, rejected_logps: -51.06187
2025-04-14 04:30:48,451 - __main__ - INFO - Testing stepsize: 2
2025-04-14 04:30:48,817 - __main__ - INFO -   chosen_logps: -48.30906, rejected_logps: -51.63037
2025-04-14 04:30:48,817 - __main__ - INFO - Testing stepsize: 5
2025-04-14 04:30:49,185 - __main__ - INFO -   chosen_logps: -45.63045, rejected_logps: -53.89760
2025-04-14 04:30:49,185 - __main__ - INFO - Testing stepsize: 10
2025-04-14 04:30:49,554 - __main__ - INFO -   chosen_logps: -42.07114, rejected_logps: -57.61943
2025-04-14 04:30:49,554 - __main__ - INFO - Testing stepsize: 15
2025-04-14 04:30:49,925 - __main__ - INFO -   chosen_logps: -39.46904, rejected_logps: -61.62484
2025-04-14 04:30:50,645 - __main__ - INFO - Update scale: 0.004802777777777778
2025-04-14 04:30:50,646 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 04:30:50,726 - __main__ - INFO - Model weights updated successfully
2025-04-14 04:30:51,560 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:30:51,594 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:30:52,442 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 04:30:52,476 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 04:30:52,477 - __main__ - INFO - Skipping full model checkpoint for iteration 7
2025-04-14 04:30:52,477 - __main__ - INFO - Iteration 7 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 7
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 7
Iteration 7 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 8
===========================================
Iteration 8 is in valid list - performing full update
Finding noisy pairs for iteration 8...
2025-04-14 04:31:13,471 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 04:31:14,020 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:31:14,165 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:31:14,179 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 04:31:14,179 - __main__ - INFO - Loading dataset
2025-04-14 04:31:14,921 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 04:31:36,492 - __main__ - INFO - Processing dataset
2025-04-14 04:31:36,741 - __main__ - INFO - Searching for noisy pairs starting from index 1664
2025-04-14 04:31:45,210 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 04:31:53,996 - __main__ - INFO - Noisy pair found at index 1746: chosen_logps=-23.8750, rejected_logps=-24.2500
2025-04-14 04:31:53,997 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 04:31:54,064 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_8
2025-04-14 04:31:54,065 - __main__ - INFO - New dataset offset: 1747
Updated dataset offset to: 1747 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 8
Worker node 2 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 04:32:08,987 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 04:32:09,225 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 04:32:09,289 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.40it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 04:32:09,545 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.64it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 04:32:09,689 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:32:09,704 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 04:32:09,704 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 04:32:09,771 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:32:09,845 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:32:09,925 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:32:09,940 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 04:32:09,940 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:32:09,987 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:32:10,000 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 04:32:10,000 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:32:26,218 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 04:32:26,514 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 04:32:26,545 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 04:32:36,373 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 04:32:36,541 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 04:32:36,688 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 04:32:40,220 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 04:32:40,332 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 04:32:40,551 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 04:32:43,534 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 04:32:43,862 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 04:32:44,165 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 04:32:44,188 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 04:32:44,248 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 04:32:44,532 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 04:32:46,993 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 04:32:47,751 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 04:32:47,927 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 04:32:48,220 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 04:32:48,386 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 04:32:48,431 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 04:32:51,449 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 04:32:51,660 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 04:32:51,969 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 04:32:52,048 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 04:32:52,124 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 04:32:52,268 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 04:32:55,660 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 04:32:55,763 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 04:32:55,874 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 04:32:56,191 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 04:32:56,206 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 04:32:56,285 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 04:32:59,379 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 04:32:59,746 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 04:32:59,794 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 04:33:00,204 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 04:33:00,260 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 04:33:00,454 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 04:33:03,394 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 04:33:03,816 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 04:33:03,890 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 04:33:04,280 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 04:33:04,618 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 04:33:05,006 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 04:33:07,423 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 04:33:07,860 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 04:33:08,108 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 04:33:08,447 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 04:33:08,455 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 04:33:09,015 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 04:33:09,019 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 04:33:09,156 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 04:33:09,159 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 04:33:11,680 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 04:33:12,543 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 04:33:12,850 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 04:33:15,820 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 04:33:16,144 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 04:33:16,448 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 04:33:16,851 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 04:33:16,923 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 04:33:17,027 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 04:34:40,371 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 04:34:40,705 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 04:34:44,520 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 04:34:44,734 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 04:34:45,020 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 04:34:45,323 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 04:34:48,342 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 04:34:48,681 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 04:34:50,266 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 04:34:52,005 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 04:34:53,504 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 04:34:53,619 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 04:34:55,171 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 04:34:57,807 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 04:35:00,014 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 04:35:01,516 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 04:35:01,956 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 04:35:04,601 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 04:35:04,752 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 04:35:06,343 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 04:35:06,880 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 04:35:08,836 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 04:35:09,023 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 04:35:12,002 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 04:35:12,879 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 04:35:13,336 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 04:35:13,348 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 04:35:13,549 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 04:35:13,945 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 04:35:17,685 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 04:37:07,359 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_0.npy
2025-04-14 04:37:07,359 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 96 negative comparisons
[E414 04:37:07.491364542 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74b5604ea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74b5157cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74b5157cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x74b5157cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x74b5157d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74b5157d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74b5606515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74b561094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74b561126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:37:07,832 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_1.npy
2025-04-14 04:37:07,832 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 97 negative comparisons
2025-04-14 04:37:10,330 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 04:37:10,792 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 04:37:15.855393852 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c36527a1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c3607bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c3607bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7c3607bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7c3607bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c3607bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c36529085c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c3653494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c3653526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:37:16,032 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_2.npy
2025-04-14 04:37:16,032 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 109 negative comparisons
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 8
2025-04-14 04:37:19,214 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 8
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 04:37:37,443 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 04:37:37,992 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:37:38,134 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:37:38,147 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 04:37:54,615 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 04:37:57,587 - __main__ - INFO - Loaded gradients from node 0: 96 negative comparisons
2025-04-14 04:37:57,828 - __main__ - INFO - Loaded gradients from node 1: 97 negative comparisons
2025-04-14 04:37:58,069 - __main__ - INFO - Loaded gradients from node 2: 109 negative comparisons
2025-04-14 04:37:58,235 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 04:37:58,432 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 04:37:58,629 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133787
2025-04-14 04:37:58,826 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352580
2025-04-14 04:37:59,023 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84833113
2025-04-14 04:37:59,221 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47194616
2025-04-14 04:37:59,419 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22256958
2025-04-14 04:37:59,618 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802876
2025-04-14 04:37:59,816 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551888
2025-04-14 04:38:00,016 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554672
2025-04-14 04:38:00,375 - __main__ - INFO - Non-zero entries after thresholding: 47194616
2025-04-14 04:38:00,376 - __main__ - INFO - Using relaxed comparison (count_negative: 302)
2025-04-14 04:38:00,376 - __main__ - INFO - Testing stepsize: 0
2025-04-14 04:38:01,129 - __main__ - INFO -   chosen_logps: -23.92797, rejected_logps: -23.81992
2025-04-14 04:38:01,129 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 04:38:01,518 - __main__ - INFO -   chosen_logps: -23.88580, rejected_logps: -23.80825
2025-04-14 04:38:01,518 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 04:38:01,908 - __main__ - INFO -   chosen_logps: -23.61373, rejected_logps: -24.24776
2025-04-14 04:38:01,908 - __main__ - INFO - Testing stepsize: 1
2025-04-14 04:38:02,301 - __main__ - INFO -   chosen_logps: -23.16189, rejected_logps: -24.39346
2025-04-14 04:38:02,301 - __main__ - INFO - Testing stepsize: 2
2025-04-14 04:38:02,697 - __main__ - INFO -   chosen_logps: -22.42567, rejected_logps: -25.35430
2025-04-14 04:38:02,697 - __main__ - INFO - Testing stepsize: 5
2025-04-14 04:38:03,094 - __main__ - INFO -   chosen_logps: -20.17164, rejected_logps: -27.35047
2025-04-14 04:38:03,094 - __main__ - INFO - Testing stepsize: 10
2025-04-14 04:38:03,494 - __main__ - INFO -   chosen_logps: -16.89402, rejected_logps: -31.22015
2025-04-14 04:38:03,494 - __main__ - INFO - Testing stepsize: 15
2025-04-14 04:38:03,895 - __main__ - INFO -   chosen_logps: -14.61326, rejected_logps: -35.30553
2025-04-14 04:38:04,676 - __main__ - INFO - Update scale: 0.005872222222222223
2025-04-14 04:38:04,677 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 04:38:04,758 - __main__ - INFO - Model weights updated successfully
2025-04-14 04:38:05,712 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:38:05,746 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:38:07,050 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 04:38:07,087 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 04:38:07,087 - __main__ - INFO - Skipping full model checkpoint for iteration 8
2025-04-14 04:38:07,088 - __main__ - INFO - Iteration 8 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 8
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 8
Iteration 8 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 9
===========================================
Iteration 9 is in valid list - performing full update
Finding noisy pairs for iteration 9...
2025-04-14 04:38:28,215 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.80it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.73it/s]
2025-04-14 04:38:28,755 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:38:28,898 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:38:28,912 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 04:38:28,912 - __main__ - INFO - Loading dataset
2025-04-14 04:38:29,622 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 04:38:51,270 - __main__ - INFO - Processing dataset
2025-04-14 04:38:51,518 - __main__ - INFO - Searching for noisy pairs starting from index 1747
2025-04-14 04:39:01,790 - __main__ - INFO - Processed 53 samples without finding a noisy pair
2025-04-14 04:39:19,990 - __main__ - INFO - Noisy pair found at index 1892: chosen_logps=-58.0000, rejected_logps=-59.7500
2025-04-14 04:39:19,995 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 04:39:20,062 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_9
2025-04-14 04:39:20,063 - __main__ - INFO - New dataset offset: 1893
Updated dataset offset to: 1893 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 9
Worker node 2 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 04:39:35,581 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]2025-04-14 04:39:35,832 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]2025-04-14 04:39:35,934 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 04:39:36,130 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.29it/s]2025-04-14 04:39:36,274 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:39:36,289 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 04:39:36,289 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:39:36,377 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
2025-04-14 04:39:36,492 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:39:36,530 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:39:36,545 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 04:39:36,545 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:39:36,632 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:39:36,645 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 04:39:36,645 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:39:52,812 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 04:39:53,245 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 04:39:53,322 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 04:40:03,000 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 04:40:03,306 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 04:40:03,468 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 04:40:06,806 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 04:40:07,132 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 04:40:07,646 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 04:40:10,400 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 04:40:10,405 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 04:40:10,571 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 04:40:10,764 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 04:40:11,111 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 04:40:11,634 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 04:40:14,363 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 04:40:14,650 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 04:40:14,881 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 04:40:14,895 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 04:40:15,122 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 04:40:15,694 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 04:40:18,215 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 04:40:18,482 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 04:40:18,625 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 04:40:18,804 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 04:40:19,209 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 04:40:19,594 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 04:40:22,506 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 04:40:22,647 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 04:40:22,755 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 04:40:23,026 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 04:40:23,361 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 04:40:23,823 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 04:40:25,823 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 04:40:26,469 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 04:40:26,500 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 04:40:27,323 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 04:40:27,338 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 04:40:27,915 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 04:40:29,974 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 04:40:30,666 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 04:40:31,044 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 04:40:31,055 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 04:40:31,876 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 04:40:32,101 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 04:40:34,388 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 04:40:34,658 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 04:40:34,661 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 04:40:34,942 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 04:40:35,399 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 04:40:36,169 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 04:40:36,178 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 04:40:36,307 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 04:40:36,309 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 04:40:38,204 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 04:40:39,638 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 04:40:39,965 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 04:40:42,283 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 04:40:42,285 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 04:40:43,519 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 04:40:44,130 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 04:40:44,180 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 04:40:44,296 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 04:42:08,418 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 04:42:08,501 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 04:42:10,741 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 04:42:11,132 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 04:42:15,019 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 04:42:15,049 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 04:42:15,186 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 04:42:15,834 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 04:42:20,206 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 04:42:20,318 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 04:42:21,314 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 04:42:21,347 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 04:42:22,662 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 04:42:22,779 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 04:42:24,618 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 04:42:26,216 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 04:42:27,999 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 04:42:30,555 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 04:42:31,822 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 04:42:32,186 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 04:42:34,109 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 04:42:35,493 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 04:42:36,405 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 04:42:37,790 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 04:42:39,661 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 04:42:39,874 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 04:42:40,226 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 04:42:41,193 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 04:42:43,507 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 04:42:43,964 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 04:44:34,177 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_0.npy
2025-04-14 04:44:34,177 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 121 negative comparisons
[E414 04:44:34.110411061 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76f79feea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76f7551cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76f7551cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x76f7551cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x76f7551d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76f7551d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x76f7a00515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x76f7a0a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x76f7a0b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:44:34.113395148 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74707e268446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7470335cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7470335cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7470335cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7470335d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7470335d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74707e3cf5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74707ee94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74707ef26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:44:34.113876962 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7674e62a9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76749b5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76749b5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x76749b5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x76749b5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76749b5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7674e64105c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7674e6e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7674e6f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:44:34.121906501 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d1d41b87446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7d1cf6fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7d1cf6fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7d1cf6fcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7d1cf6fd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d1cf6fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7d1d41cee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7d1d42694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7d1d42726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:44:34.125328772 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7eb2470d5446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7eb1fc3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7eb1fc3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7eb1fc3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7eb1fc3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7eb1fc3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7eb24723c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7eb247c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7eb247d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:44:34.128324084 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79efae688446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79ef639cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79ef639cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x79ef639cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x79ef639d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79ef639d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79efae7ef5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x79efaf294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x79efaf326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:44:34,780 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_1.npy
2025-04-14 04:44:34,780 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 137 negative comparisons
[E414 04:44:34.639577866 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7519c51e8446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75197a5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75197a5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x75197a5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x75197a5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75197a5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7519c534f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7519c5c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7519c5d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 04:44:34.659770437 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b1dc44d7446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b1d797cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b1d797cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7b1d797cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7b1d797d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b1d797d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b1dc463e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b1dc5094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b1dc5126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:44:37,182 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 04:44:37,866 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 04:44:40,969 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_2.npy
2025-04-14 04:44:40,970 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 120 negative comparisons
Waiting for worker nodes to complete gradient computation...
2025-04-14 04:44:44,076 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 9
Worker node 2 successfully completed gradient computation for iteration 9
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 04:45:00,724 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.37it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
2025-04-14 04:45:01,271 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:45:01,412 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:45:01,424 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 04:45:17,988 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 04:45:20,866 - __main__ - INFO - Loaded gradients from node 0: 121 negative comparisons
2025-04-14 04:45:21,096 - __main__ - INFO - Loaded gradients from node 1: 137 negative comparisons
2025-04-14 04:45:21,327 - __main__ - INFO - Loaded gradients from node 2: 120 negative comparisons
2025-04-14 04:45:21,486 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 04:45:21,675 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 04:45:21,864 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119129665
2025-04-14 04:45:22,052 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107344713
2025-04-14 04:45:22,241 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84834319
2025-04-14 04:45:22,429 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47195279
2025-04-14 04:45:22,616 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22259943
2025-04-14 04:45:22,804 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8801166
2025-04-14 04:45:22,991 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551404
2025-04-14 04:45:23,180 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555642
2025-04-14 04:45:23,522 - __main__ - INFO - Non-zero entries after thresholding: 47195279
2025-04-14 04:45:23,522 - __main__ - INFO - Testing stepsize: 0
2025-04-14 04:45:24,260 - __main__ - INFO -   chosen_logps: -57.56355, rejected_logps: -59.94021
2025-04-14 04:45:24,260 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 04:45:24,637 - __main__ - INFO -   chosen_logps: -57.27507, rejected_logps: -60.03181
2025-04-14 04:45:24,638 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 04:45:25,015 - __main__ - INFO -   chosen_logps: -56.65867, rejected_logps: -60.49914
2025-04-14 04:45:25,015 - __main__ - INFO - Testing stepsize: 1
2025-04-14 04:45:25,396 - __main__ - INFO -   chosen_logps: -55.58580, rejected_logps: -60.90021
2025-04-14 04:45:25,396 - __main__ - INFO - Testing stepsize: 2
2025-04-14 04:45:25,782 - __main__ - INFO -   chosen_logps: -53.62145, rejected_logps: -61.88840
2025-04-14 04:45:25,782 - __main__ - INFO - Testing stepsize: 5
2025-04-14 04:45:26,168 - __main__ - INFO -   chosen_logps: -48.75394, rejected_logps: -64.80874
2025-04-14 04:45:26,168 - __main__ - INFO - Testing stepsize: 10
2025-04-14 04:45:26,556 - __main__ - INFO -   chosen_logps: -42.41586, rejected_logps: -69.53920
2025-04-14 04:45:26,556 - __main__ - INFO - Testing stepsize: 15
2025-04-14 04:45:26,945 - __main__ - INFO -   chosen_logps: -38.10660, rejected_logps: -74.27895
2025-04-14 04:45:27,700 - __main__ - INFO - Update scale: 0.007350000000000001
2025-04-14 04:45:27,781 - __main__ - INFO - Model weights updated successfully
2025-04-14 04:45:28,850 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:45:28,882 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:45:29,772 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 04:45:29,805 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 04:45:29,805 - __main__ - INFO - Skipping full model checkpoint for iteration 9
2025-04-14 04:45:29,805 - __main__ - INFO - Iteration 9 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 9
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 9
Iteration 9 completed successfully
Cleaning up iteration files...
Multi-node training completed successfully!
Copying scripts to worker nodes...
Starting with dataset offset: 0
===========================================
Processing iteration 0
===========================================
Iteration 0 is in valid list - performing full update
Finding noisy pairs for iteration 0...
2025-04-14 04:45:54,367 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 04:45:54,916 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:45:55,062 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:45:55,076 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 04:45:55,076 - __main__ - INFO - Loading dataset
2025-04-14 04:45:55,911 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 04:46:17,528 - __main__ - INFO - Processing dataset
2025-04-14 04:46:17,777 - __main__ - INFO - Searching for noisy pairs starting from index 0
2025-04-14 04:46:18,414 - __main__ - INFO - Processed 0 samples without finding a noisy pair
2025-04-14 04:46:38,810 - __main__ - INFO - Processed 100 samples without finding a noisy pair
2025-04-14 04:46:56,956 - __main__ - INFO - Processed 200 samples without finding a noisy pair
2025-04-14 04:47:15,992 - __main__ - INFO - Processed 300 samples without finding a noisy pair
2025-04-14 04:47:22,712 - __main__ - INFO - Noisy pair found at index 337: chosen_logps=-90.0000, rejected_logps=-92.0000
2025-04-14 04:47:22,713 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 04:47:22,780 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_0
2025-04-14 04:47:22,781 - __main__ - INFO - New dataset offset: 338
Updated dataset offset to: 338 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 0
Worker node 2 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 04:47:38,188 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]2025-04-14 04:47:38,446 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 04:47:38,501 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.40it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 04:47:38,745 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.50it/s]2025-04-14 04:47:38,889 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:47:38,904 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 04:47:38,904 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 04:47:38,991 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:47:39,058 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:47:39,144 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:47:39,159 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 04:47:39,159 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:47:39,202 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:47:39,214 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 04:47:39,214 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:47:55,463 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 04:47:55,697 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 04:47:55,752 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 04:48:05,717 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 04:48:05,862 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 04:48:05,916 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 04:48:09,488 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 04:48:09,661 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 04:48:09,667 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 04:48:12,789 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 04:48:12,885 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 04:48:13,060 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 04:48:13,442 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 04:48:13,514 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 04:48:13,586 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 04:48:16,704 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 04:48:16,861 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 04:48:16,901 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 04:48:17,520 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 04:48:17,753 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 04:48:17,991 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 04:48:20,632 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 04:48:20,706 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 04:48:20,904 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 04:48:21,626 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 04:48:21,665 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 04:48:21,911 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 04:48:25,077 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 04:48:25,080 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 04:48:25,274 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 04:48:25,533 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 04:48:25,546 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 04:48:25,871 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 04:48:28,710 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 04:48:28,779 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 04:48:29,158 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 04:48:29,344 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 04:48:29,847 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 04:48:29,879 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 04:48:32,612 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 04:48:33,270 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 04:48:33,347 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 04:48:33,488 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 04:48:34,230 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 04:48:34,288 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 04:48:36,710 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 04:48:36,817 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 04:48:37,703 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 04:48:37,707 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 04:48:37,857 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 04:48:38,448 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 04:48:38,449 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 04:48:38,856 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 04:48:38,866 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 04:48:40,667 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 04:48:41,428 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 04:48:41,721 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 04:48:44,922 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 04:48:45,201 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 04:48:45,697 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 04:48:45,761 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 04:48:46,684 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 04:48:46,959 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 04:50:03,474 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 04:50:05,140 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 04:50:06,125 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 04:50:07,207 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 04:50:07,837 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 04:50:08,057 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 04:50:10,752 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 04:50:11,357 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 04:50:14,975 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 04:50:15,568 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 04:50:16,544 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 04:50:16,866 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 04:50:18,825 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 04:50:19,694 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 04:50:22,777 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 04:50:24,641 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 04:50:25,018 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 04:50:26,702 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 04:50:27,259 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 04:50:28,497 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 04:50:30,513 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 04:50:32,232 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 04:50:32,603 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 04:50:34,884 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 04:50:34,918 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 04:50:36,147 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 04:50:36,359 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 04:50:37,812 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 04:50:39,584 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 04:50:44,302 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
[E414 04:52:22.291823105 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7efa31088446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ef9e63cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ef9e63cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7ef9e63cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7ef9e63d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ef9e63d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7efa311ef5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7efa31c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7efa31d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:52:22,580 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_0.npy
2025-04-14 04:52:22,580 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 121 negative comparisons
2025-04-14 04:52:25,389 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 04:52:31.743389031 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71a3eb380446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71a3a07cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71a3a07cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x71a3a07cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x71a3a07d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71a3a07d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x71a3eb4e75c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x71a3ec094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x71a3ec126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:52:32,049 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_2.npy
2025-04-14 04:52:32,049 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 138 negative comparisons
Waiting for worker nodes to complete gradient computation...
2025-04-14 04:52:35,027 - __main__ - INFO - Node 2: Gradient computation completed successfully
[E414 04:52:40.663155610 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77fd12ca5446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x77fcc7fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x77fcc7fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x77fcc7fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x77fcc7fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x77fcc7fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x77fd12e0c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x77fd13894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x77fd13926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 04:52:41,119 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_1.npy
2025-04-14 04:52:41,120 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 138 negative comparisons
Worker node 2 successfully completed gradient computation for iteration 0
2025-04-14 04:52:43,970 - __main__ - INFO - Node 1: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 0
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 04:53:00,920 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 04:53:01,467 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:53:01,609 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:53:01,621 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 04:53:18,091 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 04:53:21,026 - __main__ - INFO - Loaded gradients from node 0: 121 negative comparisons
2025-04-14 04:53:21,260 - __main__ - INFO - Loaded gradients from node 1: 138 negative comparisons
2025-04-14 04:53:21,493 - __main__ - INFO - Loaded gradients from node 2: 138 negative comparisons
2025-04-14 04:53:21,653 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 04:53:21,841 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 04:53:22,030 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119131517
2025-04-14 04:53:22,219 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107354227
2025-04-14 04:53:22,408 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84838365
2025-04-14 04:53:22,597 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47196409
2025-04-14 04:53:22,786 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22251544
2025-04-14 04:53:22,975 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803323
2025-04-14 04:53:23,164 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552218
2025-04-14 04:53:23,352 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555293
2025-04-14 04:53:23,698 - __main__ - INFO - Non-zero entries after thresholding: 47196409
2025-04-14 04:53:23,698 - __main__ - INFO - Testing stepsize: 0
2025-04-14 04:53:24,332 - __main__ - INFO -   chosen_logps: -89.95941, rejected_logps: -91.92721
2025-04-14 04:53:24,332 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 04:53:24,598 - __main__ - INFO -   chosen_logps: -89.68518, rejected_logps: -92.05220
2025-04-14 04:53:24,598 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 04:53:24,864 - __main__ - INFO -   chosen_logps: -88.79623, rejected_logps: -92.93697
2025-04-14 04:53:24,864 - __main__ - INFO - Testing stepsize: 1
2025-04-14 04:53:25,133 - __main__ - INFO -   chosen_logps: -87.79433, rejected_logps: -93.62262
2025-04-14 04:53:25,133 - __main__ - INFO - Testing stepsize: 2
2025-04-14 04:53:25,409 - __main__ - INFO -   chosen_logps: -85.70681, rejected_logps: -94.85851
2025-04-14 04:53:25,409 - __main__ - INFO - Testing stepsize: 5
2025-04-14 04:53:25,685 - __main__ - INFO -   chosen_logps: -79.61147, rejected_logps: -98.90518
2025-04-14 04:53:25,685 - __main__ - INFO - Testing stepsize: 10
2025-04-14 04:53:25,962 - __main__ - INFO -   chosen_logps: -70.54774, rejected_logps: -106.04916
2025-04-14 04:53:25,963 - __main__ - INFO - Testing stepsize: 15
2025-04-14 04:53:26,240 - __main__ - INFO -   chosen_logps: -62.19793, rejected_logps: -113.09426
2025-04-14 04:53:26,841 - __main__ - INFO - Update scale: 0.0077194444444444454
2025-04-14 04:53:26,922 - __main__ - INFO - Model weights updated successfully
2025-04-14 04:53:27,760 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:53:27,796 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:53:28,677 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 04:53:28,710 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 04:53:28,710 - __main__ - INFO - Skipping full model checkpoint for iteration 0
2025-04-14 04:53:28,710 - __main__ - INFO - Iteration 0 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 0
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 0
Iteration 0 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 1
===========================================
Iteration 1 is in valid list - performing full update
Finding noisy pairs for iteration 1...
2025-04-14 04:53:49,880 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 04:53:50,428 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:53:50,574 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:53:50,588 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 04:53:50,588 - __main__ - INFO - Loading dataset
Traceback (most recent call last):
  File "/home/user/dpo/env/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 406, in hf_raise_for_status
    response.raise_for_status()
  File "/home/user/dpo/env/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/trl-lib/ultrafeedback_binarized/paths-info/47124cb5778f5d50de1c7676a412828f3ea7c555

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/dpo/compo/compute_gradients.py", line 1006, in <module>
    main()
  File "/home/user/dpo/compo/compute_gradients.py", line 840, in main
    noisy_batches, new_offset = find_noisy_pairs(
  File "/home/user/dpo/compo/compute_gradients.py", line 485, in find_noisy_pairs
    dataset = load_dataset("trl-lib/ultrafeedback_binarized", split="train")
  File "/home/user/dpo/env/lib/python3.10/site-packages/datasets/load.py", line 2129, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/user/dpo/env/lib/python3.10/site-packages/datasets/load.py", line 1886, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
  File "/home/user/dpo/env/lib/python3.10/site-packages/datasets/builder.py", line 342, in __init__
    self.config, self.config_id = self._create_builder_config(
  File "/home/user/dpo/env/lib/python3.10/site-packages/datasets/builder.py", line 597, in _create_builder_config
    builder_config._resolve_data_files(
  File "/home/user/dpo/env/lib/python3.10/site-packages/datasets/builder.py", line 206, in _resolve_data_files
    self.data_files = self.data_files.resolve(base_path, download_config)
  File "/home/user/dpo/env/lib/python3.10/site-packages/datasets/data_files.py", line 818, in resolve
    out[key] = data_files_patterns_list.resolve(base_path, download_config)
  File "/home/user/dpo/env/lib/python3.10/site-packages/datasets/data_files.py", line 771, in resolve
    resolve_pattern(
  File "/home/user/dpo/env/lib/python3.10/site-packages/datasets/data_files.py", line 388, in resolve_pattern
    for filepath, info in fs.glob(pattern, detail=True, **glob_kwargs).items()
  File "/home/user/dpo/env/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py", line 521, in glob
    return super().glob(path, **kwargs)
  File "/home/user/dpo/env/lib/python3.10/site-packages/fsspec/spec.py", line 611, in glob
    allpaths = self.find(root, maxdepth=depth, withdirs=True, detail=True, **kwargs)
  File "/home/user/dpo/env/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py", line 556, in find
    return super().find(
  File "/home/user/dpo/env/lib/python3.10/site-packages/fsspec/spec.py", line 502, in find
    out[path] = self.info(path)
  File "/home/user/dpo/env/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py", line 719, in info
    paths_info = self._api.get_paths_info(
  File "/home/user/dpo/env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/user/dpo/env/lib/python3.10/site-packages/huggingface_hub/hf_api.py", line 3303, in get_paths_info
    hf_raise_for_status(response)
  File "/home/user/dpo/env/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 477, in hf_raise_for_status
    raise _format(HfHubHTTPError, str(e), response) from e
huggingface_hub.errors.HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/trl-lib/ultrafeedback_binarized/paths-info/47124cb5778f5d50de1c7676a412828f3ea7c555 (Request ID: Root=1-67fccd22-6cb73075184538810912f089;e2d82dd4-7dca-4999-9099-d898bc78ceb0)

Internal Error - We're working hard to fix this as soon as possible!
ERROR: Failed to find noisy pairs for iteration 1. Exiting.
Copying scripts to worker nodes...
Starting with dataset offset: 0
===========================================
Processing iteration 0
===========================================
Iteration 0 is in valid list - performing full update
Finding noisy pairs for iteration 0...
2025-04-14 04:54:25,944 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 04:54:26,491 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:54:26,635 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 04:54:26,650 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 04:54:26,650 - __main__ - INFO - Loading dataset
Using the latest cached version of the dataset since trl-lib/ultrafeedback_binarized couldn't be found on the Hugging Face Hub
2025-04-14 04:54:28,877 - datasets.load - WARNING - Using the latest cached version of the dataset since trl-lib/ultrafeedback_binarized couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /home/user/.cache/huggingface/datasets/trl-lib___ultrafeedback_binarized/default/0.0.0/47124cb5778f5d50de1c7676a412828f3ea7c555 (last modified on Mon Apr 14 00:08:41 2025).
2025-04-14 04:54:28,879 - datasets.packaged_modules.cache.cache - WARNING - Found the latest cached dataset configuration 'default' at /home/user/.cache/huggingface/datasets/trl-lib___ultrafeedback_binarized/default/0.0.0/47124cb5778f5d50de1c7676a412828f3ea7c555 (last modified on Mon Apr 14 00:08:41 2025).
2025-04-14 04:54:28,935 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 04:54:50,429 - __main__ - INFO - Processing dataset
2025-04-14 04:54:50,675 - __main__ - INFO - Searching for noisy pairs starting from index 0
2025-04-14 04:54:51,305 - __main__ - INFO - Processed 0 samples without finding a noisy pair
2025-04-14 04:55:11,646 - __main__ - INFO - Processed 100 samples without finding a noisy pair
2025-04-14 04:55:29,744 - __main__ - INFO - Processed 200 samples without finding a noisy pair
2025-04-14 04:55:48,759 - __main__ - INFO - Processed 300 samples without finding a noisy pair
2025-04-14 04:55:55,476 - __main__ - INFO - Noisy pair found at index 337: chosen_logps=-90.0000, rejected_logps=-92.0000
2025-04-14 04:55:55,477 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 04:55:55,544 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_0
2025-04-14 04:55:55,545 - __main__ - INFO - New dataset offset: 338
Updated dataset offset to: 338 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 0
Worker node 2 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 04:56:11,275 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 04:56:11,335 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 04:56:11,406 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 04:56:11,830 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
2025-04-14 04:56:11,880 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:56:11,952 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 04:56:11,974 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:56:11,989 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 04:56:11,989 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:56:12,034 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:56:12,049 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 04:56:12,049 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:56:12,092 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 04:56:12,105 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 04:56:12,105 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 04:56:28,512 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 04:56:28,571 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 04:56:28,663 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 04:56:38,623 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 04:56:38,682 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 04:56:38,900 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 04:56:42,452 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 04:56:42,559 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 04:56:42,758 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 04:56:45,531 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 04:56:45,654 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 04:56:45,825 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 04:56:46,400 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 04:56:46,612 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 04:56:46,694 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 04:56:49,077 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 04:56:49,596 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 04:56:50,085 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 04:56:50,405 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 04:56:50,527 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 04:56:50,532 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 04:56:53,595 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 04:56:53,643 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 04:56:54,068 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 04:56:54,459 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 04:56:54,497 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 04:56:54,571 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 04:56:57,577 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 04:56:57,681 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 04:56:57,862 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 04:56:58,400 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 04:56:58,446 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 04:56:58,684 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 04:57:01,566 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 04:57:01,598 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 04:57:01,706 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 04:57:02,375 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 04:57:02,388 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 04:57:02,599 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 04:57:05,662 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 04:57:05,753 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 04:57:06,287 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 04:57:06,482 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 04:57:06,739 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 04:57:06,791 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 04:57:09,525 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 04:57:09,654 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 04:57:10,326 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 04:57:10,511 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 04:57:10,516 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 04:57:10,866 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 04:57:10,874 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 04:57:11,061 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 04:57:11,070 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 04:57:13,735 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 04:57:14,144 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 04:57:14,538 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 04:57:17,853 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 04:57:18,156 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 04:57:18,233 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 04:57:18,264 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 04:57:18,462 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 04:57:19,007 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 04:58:37,152 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 04:58:37,417 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 04:58:39,538 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 04:58:41,844 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 04:58:42,463 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 04:58:43,194 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 04:58:44,178 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 04:58:45,502 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 04:58:46,070 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 04:58:48,109 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 04:58:48,668 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 04:58:48,735 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 04:58:52,142 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 04:58:56,018 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 04:58:57,443 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 04:58:57,630 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 04:59:00,511 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 04:59:00,593 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 04:59:01,249 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 04:59:01,573 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 04:59:04,745 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 04:59:05,553 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 04:59:05,629 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 04:59:05,649 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 04:59:06,735 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 04:59:07,546 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 04:59:08,560 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 04:59:09,186 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 04:59:14,383 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 04:59:17,289 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 05:00:57,153 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_1.npy
2025-04-14 05:00:57,153 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 118 negative comparisons
2025-04-14 05:00:59,939 - __main__ - INFO - Node 1: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 0
[E414 05:01:07.909860804 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b9c58c71446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b9c0dfcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b9c0dfcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7b9c0dfcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7b9c0dfd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b9c0dfd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b9c58dd85c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b9c59894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b9c59926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:01:07.919471468 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76cb84f78446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76cb3a3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76cb3a3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x76cb3a3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x76cb3a3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76cb3a3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x76cb850df5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x76cb85c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x76cb85d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:01:07,166 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_2.npy
2025-04-14 05:01:07,167 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 132 negative comparisons
2025-04-14 05:01:10,028 - __main__ - INFO - Node 2: Gradient computation completed successfully
2025-04-14 05:01:12,813 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_0.npy
2025-04-14 05:01:12,813 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 121 negative comparisons
2025-04-14 05:01:15,662 - __main__ - INFO - Node 0: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 0
Waiting for worker nodes to complete gradient computation...
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 05:01:32,833 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 05:01:33,381 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:01:33,522 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:01:33,534 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 05:01:50,080 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 05:01:52,979 - __main__ - INFO - Loaded gradients from node 0: 121 negative comparisons
2025-04-14 05:01:53,213 - __main__ - INFO - Loaded gradients from node 1: 118 negative comparisons
2025-04-14 05:01:53,447 - __main__ - INFO - Loaded gradients from node 2: 132 negative comparisons
2025-04-14 05:01:53,607 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 05:01:53,798 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 05:01:53,988 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119136774
2025-04-14 05:01:54,179 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107354659
2025-04-14 05:01:54,369 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84842119
2025-04-14 05:01:54,560 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47193250
2025-04-14 05:01:54,752 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22251882
2025-04-14 05:01:54,944 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803763
2025-04-14 05:01:55,136 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551067
2025-04-14 05:01:55,329 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555412
2025-04-14 05:01:55,677 - __main__ - INFO - Non-zero entries after thresholding: 47193250
2025-04-14 05:01:55,677 - __main__ - INFO - Testing stepsize: 0
2025-04-14 05:01:56,311 - __main__ - INFO -   chosen_logps: -89.95943, rejected_logps: -91.92721
2025-04-14 05:01:56,311 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 05:01:56,579 - __main__ - INFO -   chosen_logps: -89.57048, rejected_logps: -92.17313
2025-04-14 05:01:56,579 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 05:01:56,846 - __main__ - INFO -   chosen_logps: -88.75697, rejected_logps: -92.69975
2025-04-14 05:01:56,846 - __main__ - INFO - Testing stepsize: 1
2025-04-14 05:01:57,117 - __main__ - INFO -   chosen_logps: -87.67739, rejected_logps: -93.20884
2025-04-14 05:01:57,117 - __main__ - INFO - Testing stepsize: 2
2025-04-14 05:01:57,391 - __main__ - INFO -   chosen_logps: -85.75169, rejected_logps: -94.27171
2025-04-14 05:01:57,391 - __main__ - INFO - Testing stepsize: 5
2025-04-14 05:01:57,667 - __main__ - INFO -   chosen_logps: -79.72222, rejected_logps: -98.26913
2025-04-14 05:01:57,667 - __main__ - INFO - Testing stepsize: 10
2025-04-14 05:01:57,944 - __main__ - INFO -   chosen_logps: -70.71577, rejected_logps: -104.26468
2025-04-14 05:01:57,944 - __main__ - INFO - Testing stepsize: 15
2025-04-14 05:01:58,223 - __main__ - INFO -   chosen_logps: -63.05754, rejected_logps: -110.91824
2025-04-14 05:01:58,822 - __main__ - INFO - Update scale: 0.0072138888888888895
2025-04-14 05:01:58,902 - __main__ - INFO - Model weights updated successfully
2025-04-14 05:01:59,745 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:01:59,779 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:02:00,695 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 05:02:00,728 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 05:02:00,728 - __main__ - INFO - Skipping full model checkpoint for iteration 0
2025-04-14 05:02:00,728 - __main__ - INFO - Iteration 0 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 0
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 0
Iteration 0 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 1
===========================================
Iteration 1 is in valid list - performing full update
Finding noisy pairs for iteration 1...
2025-04-14 05:02:22,185 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
2025-04-14 05:02:22,730 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:02:22,875 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:02:22,890 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 05:02:22,890 - __main__ - INFO - Loading dataset
Using the latest cached version of the dataset since trl-lib/ultrafeedback_binarized couldn't be found on the Hugging Face Hub
2025-04-14 05:03:47,738 - datasets.load - WARNING - Using the latest cached version of the dataset since trl-lib/ultrafeedback_binarized couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /home/user/.cache/huggingface/datasets/trl-lib___ultrafeedback_binarized/default/0.0.0/47124cb5778f5d50de1c7676a412828f3ea7c555 (last modified on Mon Apr 14 00:08:41 2025).
2025-04-14 05:03:47,740 - datasets.packaged_modules.cache.cache - WARNING - Found the latest cached dataset configuration 'default' at /home/user/.cache/huggingface/datasets/trl-lib___ultrafeedback_binarized/default/0.0.0/47124cb5778f5d50de1c7676a412828f3ea7c555 (last modified on Mon Apr 14 00:08:41 2025).
2025-04-14 05:03:47,803 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 05:04:09,250 - __main__ - INFO - Processing dataset
2025-04-14 05:04:09,497 - __main__ - INFO - Searching for noisy pairs starting from index 338
2025-04-14 05:04:16,232 - __main__ - INFO - Noisy pair found at index 369: chosen_logps=-153.0000, rejected_logps=-154.0000
2025-04-14 05:04:16,233 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 05:04:16,303 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_1
2025-04-14 05:04:16,304 - __main__ - INFO - New dataset offset: 370
Updated dataset offset to: 370 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 1
Worker node 2 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 05:04:30,804 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 05:04:30,997 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 05:04:31,087 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]2025-04-14 05:04:31,352 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 05:04:31,495 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:04:31,510 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 05:04:31,510 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:04:31,543 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 05:04:31,643 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:04:31,697 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:04:31,712 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 05:04:31,712 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:04:31,783 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:04:31,796 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 05:04:31,796 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:04:48,086 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 05:04:48,257 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 05:04:48,365 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 05:04:58,318 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 05:04:58,343 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 05:04:58,632 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 05:05:02,075 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 05:05:02,117 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 05:05:02,383 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 05:05:05,345 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 05:05:05,472 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 05:05:05,500 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 05:05:05,975 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 05:05:06,099 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 05:05:06,273 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 05:05:09,344 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 05:05:09,346 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 05:05:09,378 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 05:05:09,975 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 05:05:10,382 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 05:05:10,408 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 05:05:13,096 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 05:05:13,229 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 05:05:13,353 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 05:05:13,909 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 05:05:14,320 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 05:05:14,707 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 05:05:17,206 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 05:05:17,336 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 05:05:17,590 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 05:05:17,947 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 05:05:18,278 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 05:05:18,639 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 05:05:21,314 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 05:05:21,404 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 05:05:21,568 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 05:05:21,889 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 05:05:22,338 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 05:05:22,555 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 05:05:25,469 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 05:05:25,495 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 05:05:25,964 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 05:05:26,179 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 05:05:26,725 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 05:05:27,060 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 05:05:29,416 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 05:05:29,619 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 05:05:29,624 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 05:05:30,408 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 05:05:30,413 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 05:05:30,948 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 05:05:30,951 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 05:05:31,732 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 05:05:31,741 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 05:05:33,316 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 05:05:34,102 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 05:05:34,284 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 05:05:38,243 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 05:05:38,317 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 05:05:38,449 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 05:05:38,546 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 05:05:38,863 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 05:05:39,187 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 05:06:56,211 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 05:06:56,462 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 05:06:56,487 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 05:06:59,525 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 05:07:00,140 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 05:07:00,143 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 05:07:04,300 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 05:07:04,352 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 05:07:06,631 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 05:07:06,831 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 05:07:07,487 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 05:07:09,943 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 05:07:10,447 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 05:07:11,303 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 05:07:15,936 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 05:07:16,737 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 05:07:19,198 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 05:07:19,310 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 05:07:19,697 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 05:07:19,743 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 05:07:22,670 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 05:07:22,926 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 05:07:23,268 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 05:07:24,323 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 05:07:27,044 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 05:07:27,541 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 05:07:27,939 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 05:07:28,453 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 05:07:29,475 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 05:07:31,468 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 05:09:16,165 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_0.npy
2025-04-14 05:09:16,165 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 119 negative comparisons
[E414 05:09:17.911294988 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7cde413d5446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7cddf67cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7cddf67cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7cddf67cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7cddf67d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7cddf67d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7cde4153c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7cde41e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7cde41f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:09:17.913939780 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74694273d446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7468f7bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7468f7bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7468f7bcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7468f7bd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7468f7bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7469428a45c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x746943294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x746943326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:09:17,205 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_1.npy
2025-04-14 05:09:17,205 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 130 negative comparisons
2025-04-14 05:09:19,039 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 05:09:20,159 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 05:09:21.048032291 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78ca700b9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78ca253cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78ca253cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x78ca253cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x78ca253d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78ca253d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78ca702205c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78ca70c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78ca70d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:09:21.053972230 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7615a4609446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7615599cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7615599cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7615599cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7615599d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7615599d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7615a47705c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7615a5294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7615a5326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:09:21,339 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_2.npy
2025-04-14 05:09:21,340 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 117 negative comparisons
2025-04-14 05:09:24,075 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 1
Worker node 2 successfully completed gradient computation for iteration 1
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 05:09:41,123 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 05:09:41,668 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:09:41,815 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:09:41,827 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 05:09:58,278 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 05:10:01,270 - __main__ - INFO - Loaded gradients from node 0: 119 negative comparisons
2025-04-14 05:10:01,504 - __main__ - INFO - Loaded gradients from node 1: 130 negative comparisons
2025-04-14 05:10:01,739 - __main__ - INFO - Loaded gradients from node 2: 117 negative comparisons
2025-04-14 05:10:01,900 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 05:10:02,090 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 05:10:02,280 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119132220
2025-04-14 05:10:02,471 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107345881
2025-04-14 05:10:02,662 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84832077
2025-04-14 05:10:02,852 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47193651
2025-04-14 05:10:03,043 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252905
2025-04-14 05:10:03,236 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8805309
2025-04-14 05:10:03,428 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552857
2025-04-14 05:10:03,620 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555901
2025-04-14 05:10:03,968 - __main__ - INFO - Non-zero entries after thresholding: 47193651
2025-04-14 05:10:03,968 - __main__ - INFO - Using relaxed comparison (count_negative: 366)
2025-04-14 05:10:03,968 - __main__ - INFO - Testing stepsize: 0
2025-04-14 05:10:04,598 - __main__ - INFO -   chosen_logps: -152.85394, rejected_logps: -153.87030
2025-04-14 05:10:04,598 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 05:10:04,859 - __main__ - INFO -   chosen_logps: -152.80133, rejected_logps: -153.87804
2025-04-14 05:10:04,859 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 05:10:05,122 - __main__ - INFO -   chosen_logps: -152.22330, rejected_logps: -154.30359
2025-04-14 05:10:05,122 - __main__ - INFO - Testing stepsize: 1
2025-04-14 05:10:05,387 - __main__ - INFO -   chosen_logps: -151.78796, rejected_logps: -154.72331
2025-04-14 05:10:05,387 - __main__ - INFO - Testing stepsize: 2
2025-04-14 05:10:05,654 - __main__ - INFO -   chosen_logps: -150.52238, rejected_logps: -155.75468
2025-04-14 05:10:05,654 - __main__ - INFO - Testing stepsize: 5
2025-04-14 05:10:05,923 - __main__ - INFO -   chosen_logps: -147.70029, rejected_logps: -158.59070
2025-04-14 05:10:05,924 - __main__ - INFO - Testing stepsize: 10
2025-04-14 05:10:06,195 - __main__ - INFO -   chosen_logps: -142.60431, rejected_logps: -163.40274
2025-04-14 05:10:06,195 - __main__ - INFO - Testing stepsize: 15
2025-04-14 05:10:06,466 - __main__ - INFO -   chosen_logps: -138.57504, rejected_logps: -168.48029
2025-04-14 05:10:07,056 - __main__ - INFO - Update scale: 0.007116666666666668
2025-04-14 05:10:07,058 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 05:10:07,137 - __main__ - INFO - Model weights updated successfully
2025-04-14 05:10:07,981 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:10:08,018 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:10:08,907 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 05:10:08,942 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 05:10:08,942 - __main__ - INFO - Skipping full model checkpoint for iteration 1
2025-04-14 05:10:08,942 - __main__ - INFO - Iteration 1 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 1
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 1
Iteration 1 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 2
===========================================
Iteration 2 is in valid list - performing full update
Finding noisy pairs for iteration 2...
2025-04-14 05:10:29,898 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.74it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
2025-04-14 05:10:30,442 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:10:30,586 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:10:30,601 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 05:10:30,601 - __main__ - INFO - Loading dataset
Using the latest cached version of the dataset since trl-lib/ultrafeedback_binarized couldn't be found on the Hugging Face Hub
2025-04-14 05:10:41,220 - datasets.load - WARNING - Using the latest cached version of the dataset since trl-lib/ultrafeedback_binarized couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /home/user/.cache/huggingface/datasets/trl-lib___ultrafeedback_binarized/default/0.0.0/47124cb5778f5d50de1c7676a412828f3ea7c555 (last modified on Mon Apr 14 00:08:41 2025).
2025-04-14 05:10:41,221 - datasets.packaged_modules.cache.cache - WARNING - Found the latest cached dataset configuration 'default' at /home/user/.cache/huggingface/datasets/trl-lib___ultrafeedback_binarized/default/0.0.0/47124cb5778f5d50de1c7676a412828f3ea7c555 (last modified on Mon Apr 14 00:08:41 2025).
2025-04-14 05:10:41,270 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 05:11:03,081 - __main__ - INFO - Processing dataset
2025-04-14 05:11:03,330 - __main__ - INFO - Searching for noisy pairs starting from index 370
2025-04-14 05:11:09,504 - __main__ - INFO - Processed 30 samples without finding a noisy pair
2025-04-14 05:11:30,061 - __main__ - INFO - Processed 130 samples without finding a noisy pair
2025-04-14 05:11:51,570 - __main__ - INFO - Processed 230 samples without finding a noisy pair
2025-04-14 05:12:10,916 - __main__ - INFO - Processed 330 samples without finding a noisy pair
2025-04-14 05:12:31,180 - __main__ - INFO - Processed 430 samples without finding a noisy pair
2025-04-14 05:12:50,846 - __main__ - INFO - Processed 530 samples without finding a noisy pair
2025-04-14 05:13:09,519 - __main__ - INFO - Processed 630 samples without finding a noisy pair
2025-04-14 05:13:15,005 - __main__ - INFO - Noisy pair found at index 1030: chosen_logps=-280.0000, rejected_logps=-282.0000
2025-04-14 05:13:15,006 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 05:13:15,074 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_2
2025-04-14 05:13:15,075 - __main__ - INFO - New dataset offset: 1031
Updated dataset offset to: 1031 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 2
Worker node 2 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 05:13:30,649 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.40it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.64it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
2025-04-14 05:13:31,139 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 05:13:31,141 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 05:13:31,194 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.37it/s]2025-04-14 05:13:31,347 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.24it/s]/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:13:31,362 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 05:13:31,362 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.72it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 05:13:31,684 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:13:31,687 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:13:31,828 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:13:31,827 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:13:31,840 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 05:13:31,840 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:13:31,843 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 05:13:31,843 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:13:47,908 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 05:13:48,417 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 05:13:48,424 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 05:13:57,841 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 05:13:58,498 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 05:13:58,500 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 05:14:01,572 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 05:14:02,329 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 05:14:02,354 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 05:14:04,845 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 05:14:05,191 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 05:14:05,337 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 05:14:05,906 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 05:14:06,320 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 05:14:06,341 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 05:14:08,500 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 05:14:09,475 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 05:14:09,574 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 05:14:09,820 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 05:14:10,181 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 05:14:10,450 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 05:14:12,736 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 05:14:13,453 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 05:14:13,629 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 05:14:13,864 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 05:14:14,160 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 05:14:14,360 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 05:14:16,755 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 05:14:17,090 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 05:14:17,874 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 05:14:18,223 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 05:14:18,220 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 05:14:18,459 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 05:14:20,654 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 05:14:21,586 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 05:14:21,670 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 05:14:22,303 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 05:14:22,669 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 05:14:22,692 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 05:14:24,989 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 05:14:25,570 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 05:14:25,849 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 05:14:26,367 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 05:14:26,684 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 05:14:26,870 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 05:14:29,538 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 05:14:29,727 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 05:14:29,819 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 05:14:31,002 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 05:14:31,010 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 05:14:31,030 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 05:14:31,035 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 05:14:31,118 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 05:14:31,120 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 05:14:33,885 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 05:14:33,961 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 05:14:34,063 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 05:14:38,479 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 05:14:38,595 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 05:14:38,721 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 05:14:38,945 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 05:14:39,035 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 05:14:39,066 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 05:15:54,341 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 05:15:56,498 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 05:15:57,144 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 05:15:58,513 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 05:15:59,191 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 05:16:03,171 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 05:16:03,335 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 05:16:04,898 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 05:16:05,538 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 05:16:06,842 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 05:16:08,082 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 05:16:11,017 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 05:16:11,208 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 05:16:11,358 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 05:16:12,685 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 05:16:15,037 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 05:16:15,441 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 05:16:16,079 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 05:16:18,296 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 05:16:21,569 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 05:16:23,428 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 05:16:23,830 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 05:16:24,413 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 05:16:27,008 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 05:16:27,228 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 05:16:27,252 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 05:16:28,793 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 05:16:29,043 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 05:16:29,839 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 05:16:31,905 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 05:18:16,082 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_0.npy
2025-04-14 05:18:16,082 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 133 negative comparisons
[E414 05:18:16.541830289 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d83bc2b7446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7d83715cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7d83715cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7d83715cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7d83715d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d83715d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7d83bc41e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7d83bce94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7d83bcf26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:18:16.557402508 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7090bcd5c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7090721cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7090721cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7090721cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7090721d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7090721d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7090bcec35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7090bd894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7090bd926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:18:16.562357585 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7de310752446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7de2c5bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7de2c5bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7de2c5bcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7de2c5bd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7de2c5bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7de3108b95c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7de311294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7de311326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:18:16,839 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_1.npy
2025-04-14 05:18:16,839 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 114 negative comparisons
2025-04-14 05:18:19,127 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 05:18:19,759 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 05:18:21.796247444 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x770d60687446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x770d159cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x770d159cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x770d159cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x770d159d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x770d159d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x770d607ee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x770d61294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x770d61326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:18:21.813786750 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e425813f446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e420d5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e420d5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7e420d5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7e420d5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e420d5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e42582a65c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e4258e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e4258f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:18:21.823663322 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f2eaa364446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f2e5f7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f2e5f7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f2e5f7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f2e5f7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f2e5f7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f2eaa4cb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f2eab094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f2eab126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:18:22,148 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_2.npy
2025-04-14 05:18:22,149 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 109 negative comparisons
2025-04-14 05:18:25,121 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 2
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 2
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 05:18:42,491 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 05:18:43,037 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:18:43,177 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:18:43,189 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 05:18:59,686 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 05:19:02,556 - __main__ - INFO - Loaded gradients from node 0: 133 negative comparisons
2025-04-14 05:19:02,789 - __main__ - INFO - Loaded gradients from node 1: 114 negative comparisons
2025-04-14 05:19:03,021 - __main__ - INFO - Loaded gradients from node 2: 109 negative comparisons
2025-04-14 05:19:03,181 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 05:19:03,372 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 05:19:03,562 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119136805
2025-04-14 05:19:03,752 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107359152
2025-04-14 05:19:03,942 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84838935
2025-04-14 05:19:04,132 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47198735
2025-04-14 05:19:04,322 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22256981
2025-04-14 05:19:04,511 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8807067
2025-04-14 05:19:04,699 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550455
2025-04-14 05:19:04,888 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554013
2025-04-14 05:19:05,232 - __main__ - INFO - Non-zero entries after thresholding: 47198735
2025-04-14 05:19:05,232 - __main__ - INFO - Using relaxed comparison (count_negative: 356)
2025-04-14 05:19:05,232 - __main__ - INFO - Testing stepsize: 0
2025-04-14 05:19:05,853 - __main__ - INFO -   chosen_logps: -279.81628, rejected_logps: -281.88550
2025-04-14 05:19:05,853 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 05:19:06,104 - __main__ - INFO -   chosen_logps: -279.38617, rejected_logps: -282.03943
2025-04-14 05:19:06,105 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 05:19:06,355 - __main__ - INFO -   chosen_logps: -277.93048, rejected_logps: -283.07220
2025-04-14 05:19:06,355 - __main__ - INFO - Testing stepsize: 1
2025-04-14 05:19:06,608 - __main__ - INFO -   chosen_logps: -275.81036, rejected_logps: -284.04547
2025-04-14 05:19:06,608 - __main__ - INFO - Testing stepsize: 2
2025-04-14 05:19:06,865 - __main__ - INFO -   chosen_logps: -272.17972, rejected_logps: -286.39716
2025-04-14 05:19:06,865 - __main__ - INFO - Testing stepsize: 5
2025-04-14 05:19:07,124 - __main__ - INFO -   chosen_logps: -261.06531, rejected_logps: -293.22366
2025-04-14 05:19:07,124 - __main__ - INFO - Testing stepsize: 10
2025-04-14 05:19:07,385 - __main__ - INFO -   chosen_logps: -244.02866, rejected_logps: -305.27042
2025-04-14 05:19:07,386 - __main__ - INFO - Testing stepsize: 15
2025-04-14 05:19:07,648 - __main__ - INFO -   chosen_logps: -228.77626, rejected_logps: -317.16455
2025-04-14 05:19:08,232 - __main__ - INFO - Update scale: 0.006922222222222223
2025-04-14 05:19:08,234 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 05:19:08,315 - __main__ - INFO - Model weights updated successfully
2025-04-14 05:19:09,158 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:19:09,193 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:19:10,462 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 05:19:10,500 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 05:19:10,500 - __main__ - INFO - Skipping full model checkpoint for iteration 2
2025-04-14 05:19:10,500 - __main__ - INFO - Iteration 2 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 2
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 2
Iteration 2 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 3
===========================================
Iteration 3 is in valid list - performing full update
Finding noisy pairs for iteration 3...
2025-04-14 05:19:31,613 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 05:19:32,162 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:19:32,307 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:19:32,321 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 05:19:32,321 - __main__ - INFO - Loading dataset
2025-04-14 05:19:33,273 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 05:19:54,743 - __main__ - INFO - Processing dataset
2025-04-14 05:19:54,989 - __main__ - INFO - Searching for noisy pairs starting from index 1031
2025-04-14 05:20:09,936 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 05:20:23,558 - __main__ - INFO - Noisy pair found at index 1163: chosen_logps=-147.0000, rejected_logps=-149.0000
2025-04-14 05:20:23,559 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 05:20:23,626 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_3
2025-04-14 05:20:23,627 - __main__ - INFO - New dataset offset: 1164
Updated dataset offset to: 1164 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 3
Worker node 1 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 05:20:39,222 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.43it/s]2025-04-14 05:20:39,463 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.03it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
2025-04-14 05:20:39,766 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.31it/s]2025-04-14 05:20:39,918 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:20:39,933 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 05:20:39,933 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.46it/s]
2025-04-14 05:20:39,948 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 05:20:40,017 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]2025-04-14 05:20:40,159 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:20:40,171 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 05:20:40,172 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 05:20:40,495 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:20:40,639 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:20:40,653 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 05:20:40,653 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:20:56,459 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 05:20:56,763 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 05:20:57,188 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 05:21:06,689 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 05:21:06,967 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 05:21:07,179 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 05:21:10,891 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 05:21:10,986 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 05:21:11,000 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 05:21:13,792 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 05:21:13,999 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 05:21:14,009 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 05:21:14,866 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 05:21:14,922 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 05:21:14,955 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 05:21:18,039 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 05:21:18,209 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 05:21:18,408 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 05:21:18,673 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 05:21:18,776 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 05:21:19,267 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 05:21:22,142 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 05:21:22,193 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 05:21:22,219 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 05:21:22,507 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 05:21:22,627 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 05:21:23,135 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 05:21:26,055 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 05:21:26,452 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 05:21:26,469 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 05:21:26,528 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 05:21:26,907 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 05:21:27,257 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 05:21:30,020 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 05:21:30,149 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 05:21:30,386 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 05:21:30,926 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 05:21:30,952 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 05:21:31,185 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 05:21:34,002 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 05:21:34,227 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 05:21:34,443 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 05:21:34,895 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 05:21:34,943 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 05:21:35,320 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 05:21:38,334 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 05:21:38,379 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 05:21:38,482 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 05:21:39,499 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 05:21:39,501 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 05:21:39,578 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 05:21:39,587 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 05:21:39,641 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 05:21:39,642 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 05:21:42,027 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 05:21:42,787 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 05:21:43,141 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 05:21:46,664 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 05:21:47,080 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 05:21:47,214 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 05:21:47,645 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 05:21:47,856 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 05:21:47,861 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 05:23:10,156 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 05:23:10,603 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 05:23:13,935 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 05:23:14,169 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 05:23:14,665 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 05:23:17,790 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 05:23:18,815 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 05:23:19,823 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 05:23:23,095 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 05:23:23,849 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 05:23:24,128 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 05:23:25,365 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 05:23:27,148 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 05:23:27,743 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 05:23:28,053 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 05:23:30,796 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 05:23:32,185 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 05:23:35,030 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 05:23:35,192 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 05:23:35,649 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 05:23:35,972 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 05:23:38,987 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 05:23:42,291 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 05:23:43,724 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 05:23:43,852 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 05:23:44,294 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 05:23:44,312 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 05:23:44,969 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 05:23:48,133 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 05:23:48,539 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 05:25:37.189674119 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x714348460446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7142fd7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7142fd7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7142fd7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7142fd7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7142fd7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7143485c75c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x714349094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x714349126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:25:37.216902589 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x782831c02446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7827e6fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7827e6fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7827e6fcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7827e6fd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7827e6fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x782831d695c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x782832694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x782832726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:25:37,495 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_0.npy
2025-04-14 05:25:37,495 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 132 negative comparisons
[E414 05:25:39.952966944 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76204f787446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x762004bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x762004bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x762004bcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x762004bd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x762004bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x76204f8ee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x762050294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x762050326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:25:39,207 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_1.npy
2025-04-14 05:25:39,208 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 145 negative comparisons
2025-04-14 05:25:40,301 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 05:25:42,107 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 05:25:45,389 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_2.npy
2025-04-14 05:25:45,389 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 161 negative comparisons
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 3
2025-04-14 05:25:48,573 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 3
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 05:26:05,719 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 05:26:06,267 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:26:06,407 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:26:06,420 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 05:26:22,993 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 05:26:25,894 - __main__ - INFO - Loaded gradients from node 0: 132 negative comparisons
2025-04-14 05:26:26,127 - __main__ - INFO - Loaded gradients from node 1: 145 negative comparisons
2025-04-14 05:26:26,362 - __main__ - INFO - Loaded gradients from node 2: 161 negative comparisons
2025-04-14 05:26:26,522 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 05:26:26,712 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 05:26:26,902 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133131
2025-04-14 05:26:27,092 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352481
2025-04-14 05:26:27,282 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84838712
2025-04-14 05:26:27,473 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47196206
2025-04-14 05:26:27,663 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22256950
2025-04-14 05:26:27,852 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802455
2025-04-14 05:26:28,043 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550488
2025-04-14 05:26:28,234 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554511
2025-04-14 05:26:28,580 - __main__ - INFO - Non-zero entries after thresholding: 47196206
2025-04-14 05:26:28,581 - __main__ - INFO - Testing stepsize: 0
2025-04-14 05:26:29,332 - __main__ - INFO -   chosen_logps: -147.81319, rejected_logps: -147.74109
2025-04-14 05:26:29,332 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 05:26:29,714 - __main__ - INFO -   chosen_logps: -147.43326, rejected_logps: -147.88174
2025-04-14 05:26:29,714 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 05:26:30,098 - __main__ - INFO -   chosen_logps: -145.84421, rejected_logps: -148.21526
2025-04-14 05:26:30,098 - __main__ - INFO - Testing stepsize: 1
2025-04-14 05:26:30,488 - __main__ - INFO -   chosen_logps: -144.93642, rejected_logps: -148.91739
2025-04-14 05:26:30,488 - __main__ - INFO - Testing stepsize: 2
2025-04-14 05:26:30,883 - __main__ - INFO -   chosen_logps: -141.88519, rejected_logps: -150.04965
2025-04-14 05:26:30,883 - __main__ - INFO - Testing stepsize: 5
2025-04-14 05:26:31,304 - __main__ - INFO -   chosen_logps: -133.81961, rejected_logps: -153.55898
2025-04-14 05:26:31,304 - __main__ - INFO - Testing stepsize: 10
2025-04-14 05:26:31,728 - __main__ - INFO -   chosen_logps: -122.00832, rejected_logps: -159.27235
2025-04-14 05:26:31,728 - __main__ - INFO - Testing stepsize: 15
2025-04-14 05:26:32,197 - __main__ - INFO -   chosen_logps: -112.46274, rejected_logps: -165.08556
2025-04-14 05:26:33,027 - __main__ - INFO - Update scale: 0.008516666666666667
2025-04-14 05:26:33,107 - __main__ - INFO - Model weights updated successfully
2025-04-14 05:26:33,948 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:26:33,981 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:26:34,864 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 05:26:34,897 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 05:26:34,897 - __main__ - INFO - Skipping full model checkpoint for iteration 3
2025-04-14 05:26:34,898 - __main__ - INFO - Iteration 3 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 3
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 3
Iteration 3 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 4
===========================================
Iteration 4 is in valid list - performing full update
Finding noisy pairs for iteration 4...
2025-04-14 05:26:55,763 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 05:26:56,311 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:26:56,456 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:26:56,470 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 05:26:56,470 - __main__ - INFO - Loading dataset
2025-04-14 05:26:57,381 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 05:27:18,749 - __main__ - INFO - Processing dataset
2025-04-14 05:27:18,994 - __main__ - INFO - Searching for noisy pairs starting from index 1164
2025-04-14 05:27:25,909 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 05:27:47,254 - __main__ - INFO - Processed 136 samples without finding a noisy pair
2025-04-14 05:27:57,361 - __main__ - INFO - Noisy pair found at index 1344: chosen_logps=-268.0000, rejected_logps=-270.0000
2025-04-14 05:27:57,362 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 05:27:57,429 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_4
2025-04-14 05:27:57,430 - __main__ - INFO - New dataset offset: 1345
Updated dataset offset to: 1345 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 4
Worker node 2 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 05:28:13,196 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 05:28:13,251 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.22it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.40it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.64it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 05:28:13,741 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:28:13,796 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:28:13,882 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:28:13,895 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 05:28:13,895 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:28:13,951 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:28:13,966 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 05:28:13,966 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:28:14,072 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.23it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
2025-04-14 05:28:14,619 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:28:14,763 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:28:14,778 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 05:28:14,778 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:28:30,497 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 05:28:30,556 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 05:28:31,318 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 05:28:40,434 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 05:28:40,707 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 05:28:41,557 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 05:28:44,254 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 05:28:44,565 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 05:28:45,344 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 05:28:46,966 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 05:28:47,849 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 05:28:48,138 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 05:28:48,543 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 05:28:48,671 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 05:28:49,325 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 05:28:51,415 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 05:28:51,751 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 05:28:51,912 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 05:28:52,393 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 05:28:52,625 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 05:28:53,320 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 05:28:55,192 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 05:28:55,840 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 05:28:55,872 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 05:28:56,272 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 05:28:56,621 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 05:28:57,187 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 05:28:59,032 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 05:28:59,762 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 05:29:00,069 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 05:29:00,129 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 05:29:00,672 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 05:29:01,189 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 05:29:02,875 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 05:29:03,478 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 05:29:04,032 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 05:29:04,205 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 05:29:04,493 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 05:29:05,330 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 05:29:07,220 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 05:29:07,600 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 05:29:08,097 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 05:29:08,441 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 05:29:08,449 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 05:29:09,530 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 05:29:11,266 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 05:29:11,668 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 05:29:12,599 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 05:29:12,704 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 05:29:12,711 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 05:29:12,826 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 05:29:12,833 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 05:29:13,868 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 05:29:13,877 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 05:29:15,102 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 05:29:16,477 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 05:29:16,779 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 05:29:20,127 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 05:29:20,343 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 05:29:20,900 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 05:29:20,973 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 05:29:20,990 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 05:29:21,173 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 05:30:37,932 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 05:30:39,040 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 05:30:40,478 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 05:30:42,978 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 05:30:43,224 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 05:30:43,925 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 05:30:47,215 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 05:30:47,550 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 05:30:50,506 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 05:30:50,762 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 05:30:52,550 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 05:30:52,985 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 05:30:53,021 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 05:30:54,802 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 05:30:55,883 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 05:30:58,870 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 05:30:58,946 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 05:31:03,342 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 05:31:03,448 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 05:31:03,641 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 05:31:06,440 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 05:31:06,669 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 05:31:07,189 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 05:31:11,488 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 05:31:11,778 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 05:31:11,893 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 05:31:11,920 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 05:31:15,774 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 05:31:19,256 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 05:31:19,416 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 05:33:00,481 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_0.npy
2025-04-14 05:33:00,482 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 182 negative comparisons
[E414 05:33:00.453049321 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7911663a1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79111b7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79111b7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x79111b7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x79111b7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79111b7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7911665085c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x791166e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x791166f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:33:03,626 - __main__ - INFO - Node 0: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
[rank5]:[E414 05:33:10.516784665 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72001a2b2446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71ffcf5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71ffcf5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x71ffcf5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x71ffcf5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71ffcf5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72001a4195c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72001ae94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72001af26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:33:10.515782378 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x798ce84e3446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x798c9d7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x798c9d7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x798c9d7cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x798c9d7d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x798c9d7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x798ce864a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x798ce9094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x798ce9126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:33:10.515844995 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a7842b5c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7a77f7fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7a77f7fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7a77f7fcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7a77f7fd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a77f7fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7a7842cc35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7a7843894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7a7843926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:33:10,902 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_2.npy
2025-04-14 05:33:10,902 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 207 negative comparisons
2025-04-14 05:33:13,791 - __main__ - INFO - Node 2: Gradient computation completed successfully
[E414 05:33:14.611760575 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72438aba5446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72433ffcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72433ffcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x72433ffcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x72433ffd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72433ffd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72438ad0c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72438b694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72438b726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:33:14,845 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_1.npy
2025-04-14 05:33:14,845 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 202 negative comparisons
2025-04-14 05:33:17,675 - __main__ - INFO - Node 1: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 4
Worker node 1 successfully completed gradient computation for iteration 4
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 05:33:35,139 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
2025-04-14 05:33:35,685 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:33:35,825 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:33:35,841 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 05:33:52,396 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 05:33:55,357 - __main__ - INFO - Loaded gradients from node 0: 182 negative comparisons
2025-04-14 05:33:55,595 - __main__ - INFO - Loaded gradients from node 1: 202 negative comparisons
2025-04-14 05:33:55,834 - __main__ - INFO - Loaded gradients from node 2: 207 negative comparisons
2025-04-14 05:33:55,998 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 05:33:56,193 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 05:33:56,388 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119132745
2025-04-14 05:33:56,584 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107350123
2025-04-14 05:33:56,782 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84833606
2025-04-14 05:33:56,977 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47197893
2025-04-14 05:33:57,173 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22256851
2025-04-14 05:33:57,369 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806862
2025-04-14 05:33:57,564 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1549615
2025-04-14 05:33:57,761 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553440
2025-04-14 05:33:58,117 - __main__ - INFO - Non-zero entries after thresholding: 47197893
2025-04-14 05:33:58,117 - __main__ - INFO - Testing stepsize: 0
2025-04-14 05:33:58,782 - __main__ - INFO -   chosen_logps: -267.78427, rejected_logps: -269.47675
2025-04-14 05:33:58,782 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 05:33:59,073 - __main__ - INFO -   chosen_logps: -267.35129, rejected_logps: -269.66431
2025-04-14 05:33:59,073 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 05:33:59,365 - __main__ - INFO -   chosen_logps: -265.38269, rejected_logps: -271.71149
2025-04-14 05:33:59,365 - __main__ - INFO - Testing stepsize: 1
2025-04-14 05:33:59,660 - __main__ - INFO -   chosen_logps: -263.39816, rejected_logps: -272.88052
2025-04-14 05:33:59,660 - __main__ - INFO - Testing stepsize: 2
2025-04-14 05:33:59,957 - __main__ - INFO -   chosen_logps: -259.50946, rejected_logps: -276.50861
2025-04-14 05:33:59,957 - __main__ - INFO - Testing stepsize: 5
2025-04-14 05:34:00,255 - __main__ - INFO -   chosen_logps: -247.50961, rejected_logps: -287.91403
2025-04-14 05:34:00,255 - __main__ - INFO - Testing stepsize: 10
2025-04-14 05:34:00,556 - __main__ - INFO -   chosen_logps: -229.51414, rejected_logps: -308.33228
2025-04-14 05:34:00,556 - __main__ - INFO - Testing stepsize: 15
2025-04-14 05:34:00,859 - __main__ - INFO -   chosen_logps: -213.17737, rejected_logps: -332.54254
2025-04-14 05:34:01,498 - __main__ - INFO - Update scale: 0.011491666666666667
2025-04-14 05:34:01,580 - __main__ - INFO - Model weights updated successfully
2025-04-14 05:34:02,427 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:34:02,463 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:34:03,349 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 05:34:03,385 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 05:34:03,385 - __main__ - INFO - Skipping full model checkpoint for iteration 4
2025-04-14 05:34:03,385 - __main__ - INFO - Iteration 4 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 4
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 4
Iteration 4 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 5
===========================================
Iteration 5 is in valid list - performing full update
Finding noisy pairs for iteration 5...
2025-04-14 05:34:24,576 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 05:34:25,123 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:34:25,268 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:34:25,282 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 05:34:25,282 - __main__ - INFO - Loading dataset
2025-04-14 05:34:26,233 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 05:34:48,257 - __main__ - INFO - Processing dataset
2025-04-14 05:34:48,505 - __main__ - INFO - Searching for noisy pairs starting from index 1345
2025-04-14 05:34:59,724 - __main__ - INFO - Processed 55 samples without finding a noisy pair
2025-04-14 05:35:20,405 - __main__ - INFO - Processed 155 samples without finding a noisy pair
2025-04-14 05:35:25,859 - __main__ - INFO - Noisy pair found at index 1530: chosen_logps=-360.0000, rejected_logps=-362.0000
2025-04-14 05:35:25,860 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 05:35:25,928 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_5
2025-04-14 05:35:25,929 - __main__ - INFO - New dataset offset: 1531
Updated dataset offset to: 1531 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 5
Worker node 1 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 05:35:41,269 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]2025-04-14 05:35:41,647 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 05:35:41,700 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 05:35:41,814 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 05:35:41,957 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:35:41,972 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 05:35:41,972 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 05:35:42,193 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:35:42,256 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:35:42,346 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:35:42,361 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 05:35:42,361 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:35:42,397 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:35:42,409 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 05:35:42,409 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:35:58,538 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 05:35:58,901 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 05:35:58,960 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 05:36:08,881 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 05:36:09,073 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 05:36:09,702 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 05:36:12,728 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 05:36:12,975 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 05:36:13,478 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 05:36:15,697 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 05:36:16,065 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 05:36:16,379 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 05:36:16,653 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 05:36:17,067 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 05:36:17,379 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 05:36:19,936 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 05:36:20,075 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 05:36:20,598 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 05:36:20,618 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 05:36:20,950 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 05:36:21,477 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 05:36:23,846 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 05:36:24,036 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 05:36:24,503 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 05:36:24,652 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 05:36:24,829 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 05:36:25,392 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 05:36:28,215 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 05:36:28,359 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 05:36:28,443 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 05:36:28,721 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 05:36:28,788 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 05:36:29,493 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 05:36:32,001 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 05:36:32,079 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 05:36:32,701 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 05:36:32,925 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 05:36:33,022 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 05:36:33,377 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 05:36:35,753 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 05:36:36,132 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 05:36:36,642 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 05:36:37,077 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 05:36:37,306 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 05:36:37,656 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 05:36:39,987 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 05:36:40,252 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 05:36:40,530 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 05:36:41,475 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 05:36:41,487 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 05:36:41,600 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 05:36:41,602 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 05:36:41,627 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 05:36:41,634 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 05:36:44,537 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 05:36:44,909 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 05:36:45,143 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 05:36:48,759 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 05:36:48,800 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 05:36:48,921 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 05:36:49,045 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 05:36:49,737 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 05:36:49,890 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 05:38:08,334 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 05:38:08,409 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 05:38:11,251 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 05:38:12,222 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 05:38:12,437 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 05:38:14,749 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 05:38:15,552 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 05:38:18,488 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 05:38:20,511 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 05:38:20,532 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 05:38:21,161 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 05:38:24,080 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 05:38:24,760 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 05:38:25,513 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 05:38:27,707 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 05:38:28,284 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 05:38:28,473 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 05:38:29,700 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 05:38:31,918 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 05:38:33,384 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 05:38:33,614 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 05:38:36,698 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 05:38:37,234 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 05:38:40,087 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 05:38:41,183 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 05:38:41,309 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 05:38:41,544 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 05:38:41,656 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 05:38:44,632 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 05:38:44,825 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 05:40:29.654569051 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72e38e5f1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72e3439cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72e3439cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x72e3439cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x72e3439d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72e3439d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72e38e7585c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72e38f094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72e38f126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:40:29.677673221 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c242c7c9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c23e1bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c23e1bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7c23e1bcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7c23e1bd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c23e1bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c242c9305c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c242d294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c242d326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:40:29.685601843 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79e5fadde446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79e5b01cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79e5b01cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x79e5b01cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x79e5b01d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79e5b01d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79e5faf455c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x79e5fb894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x79e5fb926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:40:30,006 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_0.npy
2025-04-14 05:40:30,006 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 90 negative comparisons
2025-04-14 05:40:32,783 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 05:40:34.029997348 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x735fd0d2a446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x735f85fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x735f85fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x735f85fcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x735f85fd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x735f85fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x735fd0e915c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x735fd1894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x735fd1926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:40:34,336 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_1.npy
2025-04-14 05:40:34,337 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 75 negative comparisons
2025-04-14 05:40:36,881 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_2.npy
2025-04-14 05:40:36,881 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 93 negative comparisons
[E414 05:40:36.829377890 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77bb0ba17446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x77bac0dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x77bac0dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x77bac0dcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x77bac0dd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x77bac0dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x77bb0bb7e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x77bb0c694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x77bb0c726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:40:36.837929995 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x791af1617446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x791aa69cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x791aa69cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x791aa69cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x791aa69d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x791aa69d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x791af177e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x791af2294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x791af2326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:40:36.840085118 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x714a5aaa1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x714a0fdcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x714a0fdcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x714a0fdcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x714a0fdd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x714a0fdd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x714a5ac085c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x714a5b694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x714a5b726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:40:36.846246061 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x705fb9410446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x705f6e7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x705f6e7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x705f6e7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x705f6e7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x705f6e7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x705fb95775c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x705fba094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x705fba126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:40:36.848604731 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d0b2e01c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7d0ae33cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7d0ae33cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7d0ae33cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7d0ae33d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d0ae33d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7d0b2e1835c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7d0b2ec94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7d0b2ed26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:40:37,439 - __main__ - INFO - Node 1: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
2025-04-14 05:40:40,028 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 5
Worker node 2 successfully completed gradient computation for iteration 5
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 05:40:58,203 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
2025-04-14 05:40:58,759 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:40:58,926 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:40:58,941 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 05:41:15,478 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 05:41:18,991 - __main__ - INFO - Loaded gradients from node 0: 90 negative comparisons
2025-04-14 05:41:19,271 - __main__ - INFO - Loaded gradients from node 1: 75 negative comparisons
2025-04-14 05:41:19,551 - __main__ - INFO - Loaded gradients from node 2: 93 negative comparisons
2025-04-14 05:41:19,718 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 05:41:19,919 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 05:41:20,118 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119132843
2025-04-14 05:41:20,318 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107346834
2025-04-14 05:41:20,518 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84827123
2025-04-14 05:41:20,717 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47197318
2025-04-14 05:41:20,917 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22257356
2025-04-14 05:41:21,116 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8801824
2025-04-14 05:41:21,316 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551667
2025-04-14 05:41:21,515 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555713
2025-04-14 05:41:21,878 - __main__ - INFO - Non-zero entries after thresholding: 47197318
2025-04-14 05:41:21,878 - __main__ - INFO - Using relaxed comparison (count_negative: 258)
2025-04-14 05:41:21,878 - __main__ - INFO - Testing stepsize: 0
2025-04-14 05:41:22,573 - __main__ - INFO -   chosen_logps: -360.31531, rejected_logps: -361.51367
2025-04-14 05:41:22,573 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 05:41:22,883 - __main__ - INFO -   chosen_logps: -359.54822, rejected_logps: -361.43320
2025-04-14 05:41:22,883 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 05:41:23,193 - __main__ - INFO -   chosen_logps: -357.84064, rejected_logps: -363.32040
2025-04-14 05:41:23,193 - __main__ - INFO - Testing stepsize: 1
2025-04-14 05:41:23,509 - __main__ - INFO -   chosen_logps: -354.90564, rejected_logps: -363.69946
2025-04-14 05:41:23,509 - __main__ - INFO - Testing stepsize: 2
2025-04-14 05:41:23,826 - __main__ - INFO -   chosen_logps: -349.88416, rejected_logps: -365.81561
2025-04-14 05:41:23,826 - __main__ - INFO - Testing stepsize: 5
2025-04-14 05:41:24,145 - __main__ - INFO -   chosen_logps: -334.11182, rejected_logps: -374.68994
2025-04-14 05:41:24,146 - __main__ - INFO - Testing stepsize: 10
2025-04-14 05:41:24,465 - __main__ - INFO -   chosen_logps: -311.61200, rejected_logps: -389.53183
2025-04-14 05:41:24,465 - __main__ - INFO - Testing stepsize: 15
2025-04-14 05:41:24,786 - __main__ - INFO -   chosen_logps: -291.88953, rejected_logps: -407.65445
2025-04-14 05:41:25,463 - __main__ - INFO - Update scale: 0.0050166666666666675
2025-04-14 05:41:25,464 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 05:41:25,545 - __main__ - INFO - Model weights updated successfully
2025-04-14 05:41:26,388 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:41:26,425 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:41:27,331 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 05:41:27,364 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 05:41:27,364 - __main__ - INFO - Skipping full model checkpoint for iteration 5
2025-04-14 05:41:27,364 - __main__ - INFO - Iteration 5 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 5
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 5
Iteration 5 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 6
===========================================
Iteration 6 is in valid list - performing full update
Finding noisy pairs for iteration 6...
2025-04-14 05:41:48,309 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 05:41:48,859 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:41:49,011 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:41:49,026 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 05:41:49,026 - __main__ - INFO - Loading dataset
2025-04-14 05:41:49,727 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 05:42:11,323 - __main__ - INFO - Processing dataset
2025-04-14 05:42:11,571 - __main__ - INFO - Searching for noisy pairs starting from index 1531
2025-04-14 05:42:25,658 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 05:42:30,212 - __main__ - INFO - Noisy pair found at index 1624: chosen_logps=-127.5000, rejected_logps=-130.0000
2025-04-14 05:42:30,213 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 05:42:30,282 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_6
2025-04-14 05:42:30,283 - __main__ - INFO - New dataset offset: 1625
Updated dataset offset to: 1625 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 6
Worker node 1 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 05:42:46,052 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 05:42:46,142 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 05:42:46,597 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
2025-04-14 05:42:46,698 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:42:46,749 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:42:46,764 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 05:42:46,764 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:42:46,841 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 05:42:46,839 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:42:46,851 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 05:42:46,852 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.13it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.38it/s]
2025-04-14 05:42:47,399 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:42:47,543 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:42:47,558 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 05:42:47,559 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:43:03,327 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 05:43:03,409 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 05:43:04,062 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 05:43:13,418 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 05:43:13,419 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 05:43:14,317 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 05:43:17,224 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 05:43:17,235 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 05:43:18,070 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 05:43:20,517 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 05:43:20,563 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 05:43:21,118 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 05:43:21,162 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 05:43:21,571 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 05:43:21,949 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 05:43:24,278 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 05:43:24,588 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 05:43:25,102 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 05:43:25,261 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 05:43:25,692 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 05:43:25,749 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 05:43:28,368 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 05:43:28,391 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 05:43:29,033 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 05:43:29,316 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 05:43:29,364 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 05:43:29,627 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 05:43:32,276 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 05:43:32,609 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 05:43:33,141 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 05:43:33,270 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 05:43:33,397 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 05:43:34,013 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 05:43:36,297 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 05:43:36,351 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 05:43:36,991 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 05:43:37,260 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 05:43:37,665 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 05:43:38,146 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 05:43:40,614 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 05:43:41,317 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 05:43:41,377 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 05:43:41,441 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 05:43:42,011 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 05:43:42,426 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 05:43:44,903 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 05:43:45,332 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 05:43:45,654 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 05:43:45,798 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 05:43:45,807 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 05:43:46,620 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 05:43:46,624 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 05:43:46,668 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 05:43:46,672 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 05:43:49,176 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 05:43:49,552 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 05:43:49,686 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 05:43:53,760 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 05:43:54,013 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 05:43:54,037 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 05:43:54,103 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 05:43:54,109 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 05:43:54,347 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 05:45:16,730 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 05:45:17,356 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 05:45:17,531 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 05:45:20,920 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 05:45:23,703 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 05:45:23,910 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 05:45:24,655 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 05:45:25,588 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 05:45:29,263 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 05:45:29,710 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 05:45:29,811 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 05:45:32,037 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 05:45:32,451 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 05:45:33,193 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 05:45:34,132 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 05:45:36,153 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 05:45:36,253 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 05:45:39,503 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 05:45:42,115 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 05:45:42,429 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 05:45:44,885 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 05:45:45,119 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 05:45:46,572 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 05:45:48,686 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 05:45:48,942 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 05:45:49,582 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 05:45:49,941 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 05:45:50,303 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 05:45:53,159 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 05:45:54,283 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 05:47:43,077 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_0.npy
2025-04-14 05:47:43,077 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 150 negative comparisons
2025-04-14 05:47:46,025 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 05:47:49,320 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_2.npy
2025-04-14 05:47:49,320 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 146 negative comparisons
[E414 05:47:50.006492223 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71c6df260446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71c6945cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71c6945cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x71c6945cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x71c6945d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c6945d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x71c6df3c75c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x71c6dfe94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x71c6dff26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:47:50.015837193 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7eea03664446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ee9b89cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ee9b89cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7ee9b89cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7ee9b89d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ee9b89d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7eea037cb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7eea04294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7eea04326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:47:50,288 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_1.npy
2025-04-14 05:47:50,288 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 155 negative comparisons
2025-04-14 05:47:52,193 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
2025-04-14 05:47:53,073 - __main__ - INFO - Node 1: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 6
Worker node 1 successfully completed gradient computation for iteration 6
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 05:48:11,569 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 05:48:12,118 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:48:12,258 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:48:12,274 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 05:48:28,766 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 05:48:31,662 - __main__ - INFO - Loaded gradients from node 0: 150 negative comparisons
2025-04-14 05:48:31,902 - __main__ - INFO - Loaded gradients from node 1: 155 negative comparisons
2025-04-14 05:48:32,142 - __main__ - INFO - Loaded gradients from node 2: 146 negative comparisons
2025-04-14 05:48:32,302 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 05:48:32,492 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 05:48:32,682 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119137088
2025-04-14 05:48:32,872 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107348705
2025-04-14 05:48:33,063 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84831967
2025-04-14 05:48:33,253 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47193240
2025-04-14 05:48:33,443 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255373
2025-04-14 05:48:33,633 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8805827
2025-04-14 05:48:33,824 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551932
2025-04-14 05:48:34,014 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555727
2025-04-14 05:48:34,359 - __main__ - INFO - Non-zero entries after thresholding: 47193240
2025-04-14 05:48:34,360 - __main__ - INFO - Testing stepsize: 0
2025-04-14 05:48:35,092 - __main__ - INFO -   chosen_logps: -127.93054, rejected_logps: -129.48668
2025-04-14 05:48:35,092 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 05:48:35,460 - __main__ - INFO -   chosen_logps: -127.53350, rejected_logps: -129.48679
2025-04-14 05:48:35,460 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 05:48:35,828 - __main__ - INFO -   chosen_logps: -125.81117, rejected_logps: -129.88644
2025-04-14 05:48:35,828 - __main__ - INFO - Testing stepsize: 1
2025-04-14 05:48:36,199 - __main__ - INFO -   chosen_logps: -124.02200, rejected_logps: -130.46831
2025-04-14 05:48:36,199 - __main__ - INFO - Testing stepsize: 2
2025-04-14 05:48:36,576 - __main__ - INFO -   chosen_logps: -121.18645, rejected_logps: -131.27075
2025-04-14 05:48:36,576 - __main__ - INFO - Testing stepsize: 5
2025-04-14 05:48:36,955 - __main__ - INFO -   chosen_logps: -111.71980, rejected_logps: -133.71909
2025-04-14 05:48:36,955 - __main__ - INFO - Testing stepsize: 10
2025-04-14 05:48:37,351 - __main__ - INFO -   chosen_logps: -98.38503, rejected_logps: -137.87665
2025-04-14 05:48:37,351 - __main__ - INFO - Testing stepsize: 15
2025-04-14 05:48:37,777 - __main__ - INFO -   chosen_logps: -88.67009, rejected_logps: -142.47369
2025-04-14 05:48:38,631 - __main__ - INFO - Update scale: 0.008769444444444444
2025-04-14 05:48:38,721 - __main__ - INFO - Model weights updated successfully
2025-04-14 05:48:39,697 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:48:39,739 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:48:40,787 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 05:48:40,824 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 05:48:40,824 - __main__ - INFO - Skipping full model checkpoint for iteration 6
2025-04-14 05:48:40,824 - __main__ - INFO - Iteration 6 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 6
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 6
Iteration 6 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 7
===========================================
Iteration 7 is in valid list - performing full update
Finding noisy pairs for iteration 7...
2025-04-14 05:49:01,994 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.37it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
2025-04-14 05:49:02,543 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:49:02,687 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:49:02,702 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 05:49:02,702 - __main__ - INFO - Loading dataset
2025-04-14 05:49:03,463 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 05:49:24,746 - __main__ - INFO - Processing dataset
2025-04-14 05:49:24,988 - __main__ - INFO - Searching for noisy pairs starting from index 1625
2025-04-14 05:49:32,583 - __main__ - INFO - Noisy pair found at index 1663: chosen_logps=-49.7500, rejected_logps=-50.5000
2025-04-14 05:49:32,584 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 05:49:32,650 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_7
2025-04-14 05:49:32,651 - __main__ - INFO - New dataset offset: 1664
Updated dataset offset to: 1664 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 7
Worker node 1 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 05:49:47,163 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 05:49:47,175 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 05:49:47,226 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  6.95it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.31it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 05:49:47,720 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:49:47,719 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:49:47,782 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:49:47,863 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:49:47,874 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:49:47,878 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 05:49:47,878 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:49:47,889 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 05:49:47,889 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:49:47,924 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:49:47,937 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 05:49:47,937 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:50:04,491 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 05:50:04,495 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 05:50:04,502 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 05:50:14,613 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 05:50:14,683 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 05:50:14,872 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 05:50:18,351 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 05:50:18,637 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 05:50:18,744 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 05:50:21,740 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 05:50:21,879 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 05:50:21,939 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 05:50:22,279 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 05:50:22,590 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 05:50:22,783 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 05:50:25,449 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 05:50:25,922 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 05:50:25,954 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 05:50:26,150 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 05:50:26,447 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 05:50:26,756 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 05:50:29,529 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 05:50:29,673 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 05:50:29,897 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 05:50:30,000 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 05:50:30,775 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 05:50:30,862 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 05:50:33,381 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 05:50:33,710 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 05:50:33,857 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 05:50:34,224 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 05:50:34,949 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 05:50:35,174 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 05:50:37,521 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 05:50:37,971 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 05:50:38,312 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 05:50:38,738 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 05:50:38,980 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 05:50:39,553 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 05:50:41,620 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 05:50:42,352 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 05:50:42,503 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 05:50:42,522 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 05:50:43,276 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 05:50:43,742 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 05:50:46,245 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 05:50:46,411 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 05:50:47,091 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 05:50:47,092 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 05:50:47,113 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 05:50:47,591 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 05:50:47,595 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 05:50:47,717 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 05:50:47,727 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 05:50:49,895 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 05:50:50,655 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 05:50:51,002 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 05:50:54,475 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 05:50:54,790 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 05:50:54,868 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 05:50:55,115 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 05:50:55,328 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 05:50:55,463 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 05:52:19,110 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 05:52:19,137 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 05:52:19,948 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 05:52:21,464 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 05:52:22,860 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 05:52:24,604 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 05:52:25,058 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 05:52:25,467 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 05:52:26,428 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 05:52:29,439 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 05:52:29,692 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 05:52:32,111 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 05:52:32,120 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 05:52:33,177 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 05:52:33,720 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 05:52:36,225 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 05:52:36,896 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 05:52:40,745 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 05:52:41,670 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 05:52:41,822 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 05:52:42,290 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 05:52:45,063 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 05:52:45,952 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 05:52:46,697 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 05:52:50,105 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 05:52:50,404 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 05:52:51,118 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 05:52:51,477 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 05:52:52,996 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 05:52:53,175 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 05:54:43.551685269 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7937115d7446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7936c69cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7936c69cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7936c69cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7936c69d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7936c69d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79371173e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x793712094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x793712126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:54:43,814 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_0.npy
2025-04-14 05:54:43,814 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 88 negative comparisons
2025-04-14 05:54:44,394 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_1.npy
2025-04-14 05:54:44,394 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 74 negative comparisons
2025-04-14 05:54:46,765 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 05:54:47,165 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 05:54:48.208651302 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x719b45d43446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x719afb1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x719afb1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x719afb1cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x719afb1d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x719afb1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x719b45eaa5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x719b46a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x719b46b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 05:54:48.223705529 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e4e98e93446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e4e4e1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e4e4e1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7e4e4e1cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7e4e4e1d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e4e4e1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e4e98fee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e4e99a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e4e99b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 05:54:48,537 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_2.npy
2025-04-14 05:54:48,537 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 90 negative comparisons
2025-04-14 05:54:51,425 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 7
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 7
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 05:55:09,626 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 05:55:10,171 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:55:10,312 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:55:10,324 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 05:55:26,865 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 05:55:29,765 - __main__ - INFO - Loaded gradients from node 0: 88 negative comparisons
2025-04-14 05:55:30,005 - __main__ - INFO - Loaded gradients from node 1: 74 negative comparisons
2025-04-14 05:55:30,244 - __main__ - INFO - Loaded gradients from node 2: 90 negative comparisons
2025-04-14 05:55:30,405 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 05:55:30,597 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 05:55:30,789 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119129340
2025-04-14 05:55:30,981 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107345917
2025-04-14 05:55:31,173 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84826059
2025-04-14 05:55:31,365 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47188655
2025-04-14 05:55:31,556 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22254235
2025-04-14 05:55:31,749 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8805644
2025-04-14 05:55:31,940 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1553181
2025-04-14 05:55:32,132 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554393
2025-04-14 05:55:32,482 - __main__ - INFO - Non-zero entries after thresholding: 47188655
2025-04-14 05:55:32,482 - __main__ - INFO - Using relaxed comparison (count_negative: 252)
2025-04-14 05:55:32,482 - __main__ - INFO - Testing stepsize: 0
2025-04-14 05:55:33,212 - __main__ - INFO -   chosen_logps: -49.78998, rejected_logps: -50.12824
2025-04-14 05:55:33,212 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 05:55:33,575 - __main__ - INFO -   chosen_logps: -49.78430, rejected_logps: -50.12907
2025-04-14 05:55:33,575 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 05:55:33,937 - __main__ - INFO -   chosen_logps: -49.51426, rejected_logps: -50.19656
2025-04-14 05:55:33,937 - __main__ - INFO - Testing stepsize: 1
2025-04-14 05:55:34,304 - __main__ - INFO -   chosen_logps: -49.09166, rejected_logps: -50.73550
2025-04-14 05:55:34,304 - __main__ - INFO - Testing stepsize: 2
2025-04-14 05:55:34,675 - __main__ - INFO -   chosen_logps: -48.03524, rejected_logps: -51.07263
2025-04-14 05:55:34,675 - __main__ - INFO - Testing stepsize: 5
2025-04-14 05:55:35,046 - __main__ - INFO -   chosen_logps: -45.26083, rejected_logps: -53.11525
2025-04-14 05:55:35,046 - __main__ - INFO - Testing stepsize: 10
2025-04-14 05:55:35,421 - __main__ - INFO -   chosen_logps: -41.63029, rejected_logps: -56.27155
2025-04-14 05:55:35,421 - __main__ - INFO - Testing stepsize: 15
2025-04-14 05:55:35,795 - __main__ - INFO -   chosen_logps: -39.01762, rejected_logps: -59.50907
2025-04-14 05:55:36,528 - __main__ - INFO - Update scale: 0.004900000000000001
2025-04-14 05:55:36,529 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 05:55:36,609 - __main__ - INFO - Model weights updated successfully
2025-04-14 05:55:37,452 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:55:37,487 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:55:38,382 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 05:55:38,417 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 05:55:38,417 - __main__ - INFO - Skipping full model checkpoint for iteration 7
2025-04-14 05:55:38,418 - __main__ - INFO - Iteration 7 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 7
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 7
Iteration 7 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 8
===========================================
Iteration 8 is in valid list - performing full update
Finding noisy pairs for iteration 8...
2025-04-14 05:55:59,948 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
2025-04-14 05:56:00,495 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:56:00,640 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 05:56:00,654 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 05:56:00,654 - __main__ - INFO - Loading dataset
2025-04-14 05:56:01,617 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 05:56:23,305 - __main__ - INFO - Processing dataset
2025-04-14 05:56:23,554 - __main__ - INFO - Searching for noisy pairs starting from index 1664
2025-04-14 05:56:32,026 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 05:56:40,802 - __main__ - INFO - Noisy pair found at index 1746: chosen_logps=-23.8750, rejected_logps=-24.2500
2025-04-14 05:56:40,803 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 05:56:40,871 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_8
2025-04-14 05:56:40,872 - __main__ - INFO - New dataset offset: 1747
Updated dataset offset to: 1747 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 8
Worker node 1 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 05:56:56,322 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 05:56:56,526 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 05:56:56,644 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]2025-04-14 05:56:56,881 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.30it/s]2025-04-14 05:56:57,025 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:56:57,039 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 05:56:57,039 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:56:57,068 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 05:56:57,200 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 05:56:57,221 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:56:57,236 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 05:56:57,236 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:56:57,340 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 05:56:57,352 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 05:56:57,352 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 05:57:13,576 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 05:57:13,818 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 05:57:13,896 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 05:57:23,664 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 05:57:23,866 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 05:57:24,158 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 05:57:27,769 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 05:57:27,967 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 05:57:28,094 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 05:57:30,508 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 05:57:30,937 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 05:57:31,593 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 05:57:31,665 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 05:57:31,869 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 05:57:32,121 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 05:57:35,145 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 05:57:35,152 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 05:57:35,472 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 05:57:35,755 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 05:57:35,881 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 05:57:36,012 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 05:57:38,986 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 05:57:39,054 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 05:57:39,540 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 05:57:39,556 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 05:57:39,751 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 05:57:40,004 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 05:57:43,167 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 05:57:43,326 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 05:57:43,498 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 05:57:43,582 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 05:57:43,991 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 05:57:44,023 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 05:57:46,725 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 05:57:47,312 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 05:57:47,491 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 05:57:47,541 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 05:57:47,990 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 05:57:48,136 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 05:57:51,082 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 05:57:51,157 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 05:57:51,454 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 05:57:51,593 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 05:57:52,176 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 05:57:52,221 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 05:57:54,730 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 05:57:55,462 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 05:57:55,622 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 05:57:56,005 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 05:57:56,010 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 05:57:56,544 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 05:57:56,547 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 05:57:56,546 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 05:57:56,551 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 05:57:59,345 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 05:57:59,628 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 05:57:59,963 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 05:58:03,438 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 05:58:03,550 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 05:58:04,205 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 05:58:04,555 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 05:58:04,837 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 05:58:04,915 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 05:59:26,950 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 05:59:28,512 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 05:59:33,031 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 05:59:33,244 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 05:59:33,410 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 05:59:33,610 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 05:59:36,851 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 05:59:36,926 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 05:59:40,091 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 05:59:41,516 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 05:59:41,681 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 05:59:41,860 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 05:59:44,009 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 05:59:45,882 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 05:59:45,908 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 05:59:47,343 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 05:59:49,287 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 05:59:51,208 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 05:59:51,766 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 05:59:51,851 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 05:59:56,024 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 05:59:56,792 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 05:59:57,907 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 06:00:00,196 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 06:00:00,364 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 06:00:00,754 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 06:00:00,935 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 06:00:02,141 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 06:00:02,576 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 06:00:05,778 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 06:01:54.376970112 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73f4b9478446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x73f46e7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x73f46e7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x73f46e7cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x73f46e7d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73f46e7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x73f4b95df5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x73f4ba094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x73f4ba126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 06:01:54.380708220 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76c78b91e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76c740bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76c740bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x76c740bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x76c740bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76c740bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x76c78ba855c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x76c78c494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x76c78c526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:01:54,749 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_1.npy
2025-04-14 06:01:54,749 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 101 negative comparisons
[E414 06:01:55.258840148 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x706d88898446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x706d3dbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x706d3dbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x706d3dbcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x706d3dbd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x706d3dbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x706d889ff5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x706d89494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x706d89526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:01:55,614 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_0.npy
2025-04-14 06:01:55,614 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 101 negative comparisons
2025-04-14 06:01:57,793 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 06:01:58,676 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 06:02:02.441936229 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75de0a980446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75ddbfdcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75ddbfdcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x75ddbfdcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x75ddbfdd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75ddbfdd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x75de0aae75c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x75de0b694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x75de0b726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 06:02:02.446169647 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b04cf72a446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b04849cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b04849cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7b04849cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7b04849d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b04849d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b04cf8915c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b04d0294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b04d0326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:02:02,758 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_2.npy
2025-04-14 06:02:02,758 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 85 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 8
Waiting for worker nodes to complete gradient computation...
2025-04-14 06:02:05,477 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 8
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 06:02:23,871 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 06:02:24,420 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:02:24,560 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:02:24,572 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 06:02:41,076 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 06:02:44,004 - __main__ - INFO - Loaded gradients from node 0: 101 negative comparisons
2025-04-14 06:02:44,241 - __main__ - INFO - Loaded gradients from node 1: 101 negative comparisons
2025-04-14 06:02:44,479 - __main__ - INFO - Loaded gradients from node 2: 85 negative comparisons
2025-04-14 06:02:44,642 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 06:02:44,835 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 06:02:45,029 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133702
2025-04-14 06:02:45,222 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352550
2025-04-14 06:02:45,416 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84840933
2025-04-14 06:02:45,609 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47197164
2025-04-14 06:02:45,803 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22256602
2025-04-14 06:02:45,996 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8805661
2025-04-14 06:02:46,189 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550896
2025-04-14 06:02:46,384 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554609
2025-04-14 06:02:46,734 - __main__ - INFO - Non-zero entries after thresholding: 47197164
2025-04-14 06:02:46,735 - __main__ - INFO - Using relaxed comparison (count_negative: 287)
2025-04-14 06:02:46,735 - __main__ - INFO - Testing stepsize: 0
2025-04-14 06:02:47,489 - __main__ - INFO -   chosen_logps: -23.92811, rejected_logps: -23.81993
2025-04-14 06:02:47,489 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 06:02:47,877 - __main__ - INFO -   chosen_logps: -23.88588, rejected_logps: -23.80195
2025-04-14 06:02:47,877 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 06:02:48,268 - __main__ - INFO -   chosen_logps: -23.67970, rejected_logps: -24.07883
2025-04-14 06:02:48,268 - __main__ - INFO - Testing stepsize: 1
2025-04-14 06:02:48,657 - __main__ - INFO -   chosen_logps: -23.14525, rejected_logps: -24.53150
2025-04-14 06:02:48,657 - __main__ - INFO - Testing stepsize: 2
2025-04-14 06:02:49,050 - __main__ - INFO -   chosen_logps: -22.51778, rejected_logps: -25.19389
2025-04-14 06:02:49,050 - __main__ - INFO - Testing stepsize: 5
2025-04-14 06:02:49,445 - __main__ - INFO -   chosen_logps: -20.10992, rejected_logps: -27.44759
2025-04-14 06:02:49,445 - __main__ - INFO - Testing stepsize: 10
2025-04-14 06:02:49,842 - __main__ - INFO -   chosen_logps: -16.98201, rejected_logps: -31.02171
2025-04-14 06:02:49,842 - __main__ - INFO - Testing stepsize: 15
2025-04-14 06:02:50,239 - __main__ - INFO -   chosen_logps: -14.68574, rejected_logps: -35.11880
2025-04-14 06:02:51,011 - __main__ - INFO - Update scale: 0.0055805555555555565
2025-04-14 06:02:51,012 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 06:02:51,094 - __main__ - INFO - Model weights updated successfully
2025-04-14 06:02:51,943 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:02:51,978 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:02:52,871 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 06:02:52,906 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 06:02:52,906 - __main__ - INFO - Skipping full model checkpoint for iteration 8
2025-04-14 06:02:52,906 - __main__ - INFO - Iteration 8 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 8
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 8
Iteration 8 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 9
===========================================
Iteration 9 is in valid list - performing full update
Finding noisy pairs for iteration 9...
2025-04-14 06:03:13,970 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.38it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
2025-04-14 06:03:14,518 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:03:14,662 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:03:14,677 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 06:03:14,677 - __main__ - INFO - Loading dataset
2025-04-14 06:03:15,398 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 06:03:37,011 - __main__ - INFO - Processing dataset
2025-04-14 06:03:37,257 - __main__ - INFO - Searching for noisy pairs starting from index 1747
2025-04-14 06:03:47,524 - __main__ - INFO - Processed 53 samples without finding a noisy pair
2025-04-14 06:04:05,727 - __main__ - INFO - Noisy pair found at index 1892: chosen_logps=-58.0000, rejected_logps=-59.7500
2025-04-14 06:04:05,728 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 06:04:05,795 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_9
2025-04-14 06:04:05,796 - __main__ - INFO - New dataset offset: 1893
Updated dataset offset to: 1893 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 9
Worker node 1 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 06:04:21,225 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 06:04:21,478 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.49it/s]2025-04-14 06:04:21,631 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 06:04:21,784 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 06:04:21,929 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:04:21,944 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 06:04:21,944 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.25it/s]2025-04-14 06:04:22,024 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
2025-04-14 06:04:22,176 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:04:22,191 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 06:04:22,191 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:04:22,187 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:04:22,330 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:04:22,343 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 06:04:22,343 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:04:38,489 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 06:04:38,837 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 06:04:38,969 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 06:04:48,654 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 06:04:48,793 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 06:04:49,321 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 06:04:52,356 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 06:04:52,759 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 06:04:53,209 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 06:04:55,947 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 06:04:55,963 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 06:04:56,266 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 06:04:56,340 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 06:04:56,743 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 06:04:57,086 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 06:04:59,759 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 06:05:00,216 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 06:05:00,322 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 06:05:00,532 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 06:05:00,647 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 06:05:01,055 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 06:05:03,976 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 06:05:04,079 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 06:05:04,271 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 06:05:04,415 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 06:05:04,529 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 06:05:04,919 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 06:05:07,715 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 06:05:08,114 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 06:05:08,296 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 06:05:08,383 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 06:05:08,508 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 06:05:08,885 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 06:05:11,579 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 06:05:11,711 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 06:05:12,026 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 06:05:12,273 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 06:05:12,638 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 06:05:12,935 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 06:05:15,537 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 06:05:15,563 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 06:05:15,842 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 06:05:16,503 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 06:05:16,994 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 06:05:17,291 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 06:05:19,866 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 06:05:19,904 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 06:05:20,180 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 06:05:20,558 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 06:05:20,568 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 06:05:21,233 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 06:05:21,237 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 06:05:21,704 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 06:05:21,714 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 06:05:23,855 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 06:05:24,162 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 06:05:25,134 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 06:05:27,941 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 06:05:28,228 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 06:05:28,625 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 06:05:29,009 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 06:05:29,769 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 06:05:29,899 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 06:06:53,155 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 06:06:53,801 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 06:06:54,394 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 06:06:55,144 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 06:06:57,039 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 06:06:57,977 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 06:07:02,145 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 06:07:03,812 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 06:07:04,597 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 06:07:04,855 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 06:07:07,266 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 06:07:08,622 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 06:07:10,533 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 06:07:11,997 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 06:07:12,708 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 06:07:12,825 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 06:07:14,211 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 06:07:16,155 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 06:07:16,558 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 06:07:16,893 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 06:07:19,571 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 06:07:20,553 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 06:07:22,132 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 06:07:24,353 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 06:07:25,412 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 06:07:25,658 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 06:07:25,866 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 06:07:26,564 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 06:07:29,127 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 06:07:29,563 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 06:09:19,004 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_0.npy
2025-04-14 06:09:19,004 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 121 negative comparisons
[E414 06:09:20.857264219 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x770bc0ba5446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x770b75fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x770b75fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x770b75fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x770b75fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x770b75fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x770bc0d0c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x770bc1694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x770bc1726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:09:20,091 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_1.npy
2025-04-14 06:09:20,091 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 120 negative comparisons
2025-04-14 06:09:21,870 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 06:09:23,027 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 06:09:27,093 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_2.npy
2025-04-14 06:09:27,093 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 106 negative comparisons
[rank9]:[E414 06:09:27.039499618 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76bc7936c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76bc2ebcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76bc2ebcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x76bc2ebcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x76bc2ebd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76bc2ebd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x76bc798a15c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x76bc7a494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x76bc7a526850 in /lib/x86_64-linux-gnu/libc.so.6)

Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 9
2025-04-14 06:09:30,265 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 9
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 06:09:47,017 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 06:09:47,565 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:09:47,705 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:09:47,718 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 06:10:04,372 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 06:10:07,256 - __main__ - INFO - Loaded gradients from node 0: 121 negative comparisons
2025-04-14 06:10:07,490 - __main__ - INFO - Loaded gradients from node 1: 120 negative comparisons
2025-04-14 06:10:07,723 - __main__ - INFO - Loaded gradients from node 2: 106 negative comparisons
2025-04-14 06:10:07,882 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 06:10:08,071 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 06:10:08,260 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133639
2025-04-14 06:10:08,450 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107345751
2025-04-14 06:10:08,639 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84828748
2025-04-14 06:10:08,829 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47190087
2025-04-14 06:10:09,018 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252926
2025-04-14 06:10:09,207 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803887
2025-04-14 06:10:09,396 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1553487
2025-04-14 06:10:09,586 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 556416
2025-04-14 06:10:09,930 - __main__ - INFO - Non-zero entries after thresholding: 47190087
2025-04-14 06:10:09,931 - __main__ - INFO - Using relaxed comparison (count_negative: 347)
2025-04-14 06:10:09,931 - __main__ - INFO - Testing stepsize: 0
2025-04-14 06:10:10,677 - __main__ - INFO -   chosen_logps: -57.56464, rejected_logps: -59.94020
2025-04-14 06:10:10,677 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 06:10:11,055 - __main__ - INFO -   chosen_logps: -57.24481, rejected_logps: -60.03180
2025-04-14 06:10:11,055 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 06:10:11,436 - __main__ - INFO -   chosen_logps: -56.66299, rejected_logps: -60.46772
2025-04-14 06:10:11,436 - __main__ - INFO - Testing stepsize: 1
2025-04-14 06:10:11,819 - __main__ - INFO -   chosen_logps: -55.39258, rejected_logps: -60.71516
2025-04-14 06:10:11,819 - __main__ - INFO - Testing stepsize: 2
2025-04-14 06:10:12,206 - __main__ - INFO -   chosen_logps: -53.16549, rejected_logps: -61.61378
2025-04-14 06:10:12,206 - __main__ - INFO - Testing stepsize: 5
2025-04-14 06:10:12,594 - __main__ - INFO -   chosen_logps: -47.81076, rejected_logps: -64.03803
2025-04-14 06:10:12,594 - __main__ - INFO - Testing stepsize: 10
2025-04-14 06:10:12,985 - __main__ - INFO -   chosen_logps: -40.61960, rejected_logps: -68.07128
2025-04-14 06:10:12,985 - __main__ - INFO - Testing stepsize: 15
2025-04-14 06:10:13,375 - __main__ - INFO -   chosen_logps: -35.40605, rejected_logps: -72.15067
2025-04-14 06:10:14,132 - __main__ - INFO - Update scale: 0.006747222222222223
2025-04-14 06:10:14,133 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 06:10:14,212 - __main__ - INFO - Model weights updated successfully
2025-04-14 06:10:15,054 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:10:15,090 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:10:15,990 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 06:10:16,024 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 06:10:16,024 - __main__ - INFO - Skipping full model checkpoint for iteration 9
2025-04-14 06:10:16,024 - __main__ - INFO - Iteration 9 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 9
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 9
Iteration 9 completed successfully
Cleaning up iteration files...
Multi-node training completed successfully!
Copying scripts to worker nodes...
Starting with dataset offset: 0
===========================================
Processing iteration 0
===========================================
Iteration 0 is in valid list - performing full update
Finding noisy pairs for iteration 0...
2025-04-14 06:10:40,334 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 06:10:40,882 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:10:41,026 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:10:41,040 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 06:10:41,040 - __main__ - INFO - Loading dataset
2025-04-14 06:10:42,028 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 06:11:03,639 - __main__ - INFO - Processing dataset
2025-04-14 06:11:03,887 - __main__ - INFO - Searching for noisy pairs starting from index 0
2025-04-14 06:11:04,526 - __main__ - INFO - Processed 0 samples without finding a noisy pair
2025-04-14 06:11:24,922 - __main__ - INFO - Processed 100 samples without finding a noisy pair
2025-04-14 06:11:43,064 - __main__ - INFO - Processed 200 samples without finding a noisy pair
2025-04-14 06:12:02,098 - __main__ - INFO - Processed 300 samples without finding a noisy pair
2025-04-14 06:12:08,815 - __main__ - INFO - Noisy pair found at index 337: chosen_logps=-90.0000, rejected_logps=-92.0000
2025-04-14 06:12:08,816 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 06:12:08,883 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_0
2025-04-14 06:12:08,884 - __main__ - INFO - New dataset offset: 338
Updated dataset offset to: 338 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 0
Worker node 2 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 06:12:24,222 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.23it/s]2025-04-14 06:12:24,551 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
2025-04-14 06:12:24,683 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 06:12:24,771 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]2025-04-14 06:12:24,915 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:12:24,930 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 06:12:24,930 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.28it/s]2025-04-14 06:12:25,096 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 06:12:25,239 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:12:25,248 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:12:25,263 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 06:12:25,263 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:12:25,381 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:12:25,394 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 06:12:25,394 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:12:41,501 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 06:12:41,804 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 06:12:41,955 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 06:12:51,686 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 06:12:51,980 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 06:12:52,035 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 06:12:55,488 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 06:12:55,753 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 06:12:56,119 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 06:12:58,529 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 06:12:58,723 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 06:12:58,857 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 06:12:59,512 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 06:12:59,658 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 06:13:00,496 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 06:13:02,688 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 06:13:02,866 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 06:13:02,901 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 06:13:03,395 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 06:13:03,399 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 06:13:04,319 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 06:13:06,690 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 06:13:06,741 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 06:13:07,315 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 06:13:07,457 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 06:13:07,767 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 06:13:08,196 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 06:13:10,512 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 06:13:10,840 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 06:13:11,263 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 06:13:11,453 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 06:13:11,530 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 06:13:12,291 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 06:13:14,417 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 06:13:14,427 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 06:13:15,417 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 06:13:15,749 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 06:13:15,779 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 06:13:16,244 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 06:13:18,468 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 06:13:18,521 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 06:13:20,011 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 06:13:20,039 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 06:13:20,226 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 06:13:20,306 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 06:13:22,772 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 06:13:23,117 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 06:13:23,581 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 06:13:24,323 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 06:13:24,329 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 06:13:24,747 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 06:13:24,749 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 06:13:24,756 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 06:13:24,762 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 06:13:27,113 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 06:13:27,660 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 06:13:28,223 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 06:13:31,750 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 06:13:31,809 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 06:13:31,963 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 06:13:31,998 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 06:13:32,063 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 06:13:32,128 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 06:14:49,590 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 06:14:49,679 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 06:14:52,169 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 06:14:52,531 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 06:14:54,181 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 06:14:56,963 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 06:14:57,146 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 06:14:57,569 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 06:14:57,940 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 06:15:00,743 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 06:15:01,214 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 06:15:01,734 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 06:15:05,087 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 06:15:05,717 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 06:15:08,193 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 06:15:08,278 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 06:15:09,359 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 06:15:10,418 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 06:15:12,857 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 06:15:13,021 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 06:15:15,113 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 06:15:16,812 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 06:15:17,375 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 06:15:20,729 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 06:15:21,113 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 06:15:21,951 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 06:15:21,992 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 06:15:22,217 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 06:15:25,129 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 06:15:25,889 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 06:17:08,499 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_0.npy
2025-04-14 06:17:08,499 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 138 negative comparisons
[E414 06:17:09.840463375 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bce205ce446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7bcdd59cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7bcdd59cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7bcdd59cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7bcdd59d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bcdd59d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7bce207355c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7bce21094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7bce21126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:17:09,102 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_1.npy
2025-04-14 06:17:09,102 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 139 negative comparisons
2025-04-14 06:17:11,574 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 06:17:12,056 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 06:17:15.296661921 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7cd5938f6446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7cd548bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7cd548bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7cd548bcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7cd548bd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7cd548bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7cd593a5d5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7cd594494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7cd594526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 06:17:15.338958883 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71975d4be446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7197127cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7197127cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7197127cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7197127d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7197127d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x71975d6255c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x71975e094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x71975e126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:17:15,676 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_2.npy
2025-04-14 06:17:15,676 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 139 negative comparisons
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 0
2025-04-14 06:17:18,530 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 0
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 06:17:36,177 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 06:17:36,724 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:17:36,867 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:17:36,879 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 06:17:53,415 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 06:17:56,321 - __main__ - INFO - Loaded gradients from node 0: 138 negative comparisons
2025-04-14 06:17:56,555 - __main__ - INFO - Loaded gradients from node 1: 139 negative comparisons
2025-04-14 06:17:56,794 - __main__ - INFO - Loaded gradients from node 2: 139 negative comparisons
2025-04-14 06:17:56,954 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 06:17:57,144 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 06:17:57,334 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119136556
2025-04-14 06:17:57,525 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107347810
2025-04-14 06:17:57,715 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84822258
2025-04-14 06:17:57,906 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47185723
2025-04-14 06:17:58,096 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22254745
2025-04-14 06:17:58,287 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8807316
2025-04-14 06:17:58,477 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551915
2025-04-14 06:17:58,667 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 556182
2025-04-14 06:17:59,012 - __main__ - INFO - Non-zero entries after thresholding: 47185723
2025-04-14 06:17:59,012 - __main__ - INFO - Testing stepsize: 0
2025-04-14 06:17:59,648 - __main__ - INFO -   chosen_logps: -89.95941, rejected_logps: -91.92722
2025-04-14 06:17:59,648 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 06:17:59,915 - __main__ - INFO -   chosen_logps: -89.68468, rejected_logps: -92.17693
2025-04-14 06:17:59,915 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 06:18:00,181 - __main__ - INFO -   chosen_logps: -88.88667, rejected_logps: -92.66054
2025-04-14 06:18:00,181 - __main__ - INFO - Testing stepsize: 1
2025-04-14 06:18:00,452 - __main__ - INFO -   chosen_logps: -87.57240, rejected_logps: -93.30943
2025-04-14 06:18:00,452 - __main__ - INFO - Testing stepsize: 2
2025-04-14 06:18:00,726 - __main__ - INFO -   chosen_logps: -85.48500, rejected_logps: -94.58992
2025-04-14 06:18:00,726 - __main__ - INFO - Testing stepsize: 5
2025-04-14 06:18:01,002 - __main__ - INFO -   chosen_logps: -79.02312, rejected_logps: -98.63956
2025-04-14 06:18:01,002 - __main__ - INFO - Testing stepsize: 10
2025-04-14 06:18:01,280 - __main__ - INFO -   chosen_logps: -68.90965, rejected_logps: -105.27611
2025-04-14 06:18:01,280 - __main__ - INFO - Testing stepsize: 15
2025-04-14 06:18:01,559 - __main__ - INFO -   chosen_logps: -60.22590, rejected_logps: -112.36223
2025-04-14 06:18:02,161 - __main__ - INFO - Update scale: 0.008088888888888889
2025-04-14 06:18:02,243 - __main__ - INFO - Model weights updated successfully
2025-04-14 06:18:03,084 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:18:03,120 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:18:04,015 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 06:18:04,049 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 06:18:04,049 - __main__ - INFO - Skipping full model checkpoint for iteration 0
2025-04-14 06:18:04,050 - __main__ - INFO - Iteration 0 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 0
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 0
Iteration 0 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 1
===========================================
Iteration 1 is in valid list - performing full update
Finding noisy pairs for iteration 1...
2025-04-14 06:18:24,931 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.72it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
2025-04-14 06:18:25,476 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:18:25,621 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:18:25,636 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 06:18:25,636 - __main__ - INFO - Loading dataset
2025-04-14 06:18:26,336 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 06:18:47,958 - __main__ - INFO - Processing dataset
2025-04-14 06:18:48,208 - __main__ - INFO - Searching for noisy pairs starting from index 338
2025-04-14 06:18:54,938 - __main__ - INFO - Noisy pair found at index 369: chosen_logps=-153.0000, rejected_logps=-154.0000
2025-04-14 06:18:54,939 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 06:18:55,007 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_1
2025-04-14 06:18:55,008 - __main__ - INFO - New dataset offset: 370
Updated dataset offset to: 370 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 1
Worker node 1 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 06:19:09,429 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]2025-04-14 06:19:09,798 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 06:19:09,868 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 06:19:09,987 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  6.48it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  4.53it/s]2025-04-14 06:19:10,131 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:19:10,146 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 06:19:10,146 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  6.84it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  4.86it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.22it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.07it/s]
2025-04-14 06:19:10,443 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.34it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.16it/s]
2025-04-14 06:19:10,570 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:19:10,576 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:19:10,588 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 06:19:10,588 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:19:10,753 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:19:10,770 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 06:19:10,770 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:19:26,680 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 06:19:27,146 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 06:19:27,315 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 06:19:36,951 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 06:19:37,307 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 06:19:37,384 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 06:19:40,779 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 06:19:41,098 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 06:19:41,178 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 06:19:43,954 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 06:19:44,025 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 06:19:44,503 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 06:19:44,742 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 06:19:44,991 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 06:19:44,997 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 06:19:47,900 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 06:19:48,047 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 06:19:48,181 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 06:19:48,539 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 06:19:48,840 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 06:19:48,939 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 06:19:51,772 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 06:19:51,827 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 06:19:52,264 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 06:19:52,695 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 06:19:52,832 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 06:19:52,879 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 06:19:55,586 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 06:19:55,651 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 06:19:56,248 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 06:19:56,722 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 06:19:56,773 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 06:19:56,848 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 06:19:59,577 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 06:19:59,827 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 06:20:00,204 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 06:20:00,682 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 06:20:00,736 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 06:20:00,935 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 06:20:03,503 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 06:20:04,081 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 06:20:04,228 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 06:20:04,680 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 06:20:04,766 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 06:20:05,041 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 06:20:07,604 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 06:20:08,163 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 06:20:08,355 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 06:20:08,676 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 06:20:08,684 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 06:20:09,025 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 06:20:09,033 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 06:20:09,843 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 06:20:09,855 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 06:20:11,458 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 06:20:12,122 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 06:20:12,545 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 06:20:15,623 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 06:20:15,695 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 06:20:16,413 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 06:20:16,616 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 06:20:17,753 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 06:20:17,791 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 06:21:33,377 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 06:21:35,575 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 06:21:37,812 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 06:21:38,185 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 06:21:39,067 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 06:21:39,574 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 06:21:40,405 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 06:21:42,130 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 06:21:43,231 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 06:21:45,349 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 06:21:45,891 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 06:21:47,778 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 06:21:49,873 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 06:21:51,523 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 06:21:53,114 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 06:21:53,279 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 06:21:53,798 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 06:21:54,318 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 06:21:56,302 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 06:21:57,677 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 06:22:00,849 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 06:22:01,928 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 06:22:02,899 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 06:22:04,406 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 06:22:05,432 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 06:22:05,657 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 06:22:05,753 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 06:22:05,845 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 06:22:07,886 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 06:22:10,102 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 06:23:52.717414992 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x716812ba1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7167c7fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7167c7fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7167c7fcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7167c7fd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7167c7fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x716812d085c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x716813694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x716813726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 06:23:52.716508519 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7998b4df1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79986a1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79986a1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x79986a1cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x79986a1d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79986a1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7998b4f585c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7998b5894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7998b5926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:23:53,033 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_1.npy
2025-04-14 06:23:53,033 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 106 negative comparisons
2025-04-14 06:23:53,078 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_0.npy
2025-04-14 06:23:53,078 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 128 negative comparisons
[E414 06:23:53.924548654 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a69cd117446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7a69823cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7a69823cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7a69823cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7a69823d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a69823d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7a69cd27e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7a69cdc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7a69cdd26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:23:55,949 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 06:23:56,244 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 06:24:00.678484904 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ea26a31e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ea21f5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ea21f5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7ea21f5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7ea21f5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ea21f5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7ea26a4855c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7ea26ae94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7ea26af26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 06:24:00.685565903 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71a25ed3f446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71a2141cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71a2141cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x71a2141cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x71a2141d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71a2141d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x71a25eea65c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x71a25fa94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x71a25fb26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 06:24:00.703162897 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73b770c10446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x73b725fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x73b725fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x73b725fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x73b725fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73b725fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x73b770d775c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x73b771894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x73b771926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:24:00,972 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_2.npy
2025-04-14 06:24:00,972 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 120 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 1
Waiting for worker nodes to complete gradient computation...
2025-04-14 06:24:03,803 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 1
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 06:24:22,243 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 06:24:22,791 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:24:22,932 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:24:22,945 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 06:24:39,426 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 06:24:42,315 - __main__ - INFO - Loaded gradients from node 0: 128 negative comparisons
2025-04-14 06:24:42,551 - __main__ - INFO - Loaded gradients from node 1: 106 negative comparisons
2025-04-14 06:24:42,789 - __main__ - INFO - Loaded gradients from node 2: 120 negative comparisons
2025-04-14 06:24:42,949 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 06:24:43,138 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 06:24:43,329 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119134555
2025-04-14 06:24:43,519 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107348783
2025-04-14 06:24:43,709 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84834716
2025-04-14 06:24:43,899 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47190506
2025-04-14 06:24:44,090 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255040
2025-04-14 06:24:44,280 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802913
2025-04-14 06:24:44,470 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1553299
2025-04-14 06:24:44,660 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555536
2025-04-14 06:24:45,006 - __main__ - INFO - Non-zero entries after thresholding: 47190506
2025-04-14 06:24:45,006 - __main__ - INFO - Using relaxed comparison (count_negative: 354)
2025-04-14 06:24:45,006 - __main__ - INFO - Testing stepsize: 0
2025-04-14 06:24:45,635 - __main__ - INFO -   chosen_logps: -152.85399, rejected_logps: -153.87024
2025-04-14 06:24:45,636 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 06:24:45,897 - __main__ - INFO -   chosen_logps: -152.78215, rejected_logps: -153.87695
2025-04-14 06:24:45,897 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 06:24:46,158 - __main__ - INFO -   chosen_logps: -152.13867, rejected_logps: -154.22937
2025-04-14 06:24:46,158 - __main__ - INFO - Testing stepsize: 1
2025-04-14 06:24:46,422 - __main__ - INFO -   chosen_logps: -151.56781, rejected_logps: -154.67439
2025-04-14 06:24:46,422 - __main__ - INFO - Testing stepsize: 2
2025-04-14 06:24:46,691 - __main__ - INFO -   chosen_logps: -150.41403, rejected_logps: -155.67628
2025-04-14 06:24:46,691 - __main__ - INFO - Testing stepsize: 5
2025-04-14 06:24:46,960 - __main__ - INFO -   chosen_logps: -147.16176, rejected_logps: -157.69275
2025-04-14 06:24:46,960 - __main__ - INFO - Testing stepsize: 10
2025-04-14 06:24:47,231 - __main__ - INFO -   chosen_logps: -142.16150, rejected_logps: -161.84215
2025-04-14 06:24:47,231 - __main__ - INFO - Testing stepsize: 15
2025-04-14 06:24:47,503 - __main__ - INFO -   chosen_logps: -138.51944, rejected_logps: -166.17163
2025-04-14 06:24:48,095 - __main__ - INFO - Update scale: 0.006883333333333333
2025-04-14 06:24:48,096 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 06:24:48,176 - __main__ - INFO - Model weights updated successfully
2025-04-14 06:24:49,012 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:24:49,049 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:24:49,938 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 06:24:49,973 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 06:24:49,973 - __main__ - INFO - Skipping full model checkpoint for iteration 1
2025-04-14 06:24:49,973 - __main__ - INFO - Iteration 1 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 1
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 1
Iteration 1 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 2
===========================================
Iteration 2 is in valid list - performing full update
Finding noisy pairs for iteration 2...
2025-04-14 06:25:10,782 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.31it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
2025-04-14 06:25:11,333 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:25:11,477 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:25:11,491 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 06:25:11,491 - __main__ - INFO - Loading dataset
2025-04-14 06:25:13,194 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 06:25:34,837 - __main__ - INFO - Processing dataset
2025-04-14 06:25:35,085 - __main__ - INFO - Searching for noisy pairs starting from index 370
2025-04-14 06:25:41,251 - __main__ - INFO - Processed 30 samples without finding a noisy pair
2025-04-14 06:26:01,807 - __main__ - INFO - Processed 130 samples without finding a noisy pair
2025-04-14 06:26:23,319 - __main__ - INFO - Processed 230 samples without finding a noisy pair
2025-04-14 06:26:42,648 - __main__ - INFO - Processed 330 samples without finding a noisy pair
2025-04-14 06:27:02,905 - __main__ - INFO - Processed 430 samples without finding a noisy pair
2025-04-14 06:27:22,563 - __main__ - INFO - Processed 530 samples without finding a noisy pair
2025-04-14 06:27:41,222 - __main__ - INFO - Processed 630 samples without finding a noisy pair
2025-04-14 06:27:46,704 - __main__ - INFO - Noisy pair found at index 1030: chosen_logps=-280.0000, rejected_logps=-282.0000
2025-04-14 06:27:46,705 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 06:27:46,773 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_2
2025-04-14 06:27:46,774 - __main__ - INFO - New dataset offset: 1031
Updated dataset offset to: 1031 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 2
Worker node 2 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 06:28:02,256 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 06:28:02,510 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 06:28:02,522 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
2025-04-14 06:28:02,814 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.53it/s]2025-04-14 06:28:02,957 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:28:02,972 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 06:28:02,972 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
2025-04-14 06:28:03,065 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:28:03,077 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:28:03,207 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:28:03,220 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 06:28:03,220 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:28:03,234 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:28:03,249 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 06:28:03,249 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:28:19,505 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 06:28:19,766 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 06:28:19,783 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 06:28:29,507 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 06:28:29,894 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 06:28:29,938 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 06:28:33,659 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 06:28:33,697 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 06:28:33,750 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 06:28:36,215 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 06:28:36,535 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 06:28:36,670 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 06:28:37,712 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 06:28:37,885 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 06:28:37,964 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 06:28:40,186 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 06:28:40,873 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 06:28:41,051 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 06:28:41,750 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 06:28:41,780 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 06:28:42,054 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 06:28:44,752 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 06:28:44,817 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 06:28:45,015 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 06:28:45,886 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 06:28:45,914 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 06:28:45,932 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 06:28:49,060 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 06:28:49,215 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 06:28:49,318 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 06:28:49,828 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 06:28:49,988 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 06:28:50,243 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 06:28:52,765 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 06:28:53,034 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 06:28:53,206 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 06:28:53,826 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 06:28:54,337 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 06:28:54,518 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 06:28:56,487 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 06:28:56,975 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 06:28:57,542 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 06:28:58,287 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 06:28:58,296 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 06:28:58,691 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 06:29:01,082 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 06:29:01,513 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 06:29:01,820 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 06:29:02,242 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 06:29:02,250 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 06:29:02,641 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 06:29:02,644 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 06:29:03,095 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 06:29:03,098 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 06:29:05,179 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 06:29:05,640 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 06:29:05,763 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 06:29:09,381 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 06:29:09,680 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 06:29:09,795 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 06:29:10,181 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 06:29:10,450 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 06:29:10,724 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 06:30:25,071 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 06:30:26,706 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 06:30:29,194 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 06:30:29,608 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 06:30:30,115 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 06:30:32,125 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 06:30:33,455 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 06:30:33,578 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 06:30:36,639 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 06:30:38,944 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 06:30:41,610 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 06:30:42,132 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 06:30:42,375 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 06:30:42,920 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 06:30:44,873 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 06:30:45,826 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 06:30:45,826 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 06:30:49,612 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 06:30:50,267 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 06:30:51,019 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 06:30:51,290 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 06:30:53,949 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 06:30:54,828 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 06:30:55,253 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 06:30:57,696 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 06:30:57,793 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 06:30:58,366 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 06:31:00,297 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 06:31:00,781 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 06:31:01,717 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 06:32:44,808 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_0.npy
2025-04-14 06:32:44,808 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 119 negative comparisons
2025-04-14 06:32:46,831 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_1.npy
2025-04-14 06:32:46,831 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 110 negative comparisons
2025-04-14 06:32:47,693 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 06:32:49.844943765 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7cfb651f1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7cfb1a5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7cfb1a5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7cfb1a5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7cfb1a5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7cfb1a5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7cfb653585c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7cfb65e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7cfb65f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 06:32:49.855478657 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78c7f9f43446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78c7af3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78c7af3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x78c7af3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x78c7af3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78c7af3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78c7fa0aa5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78c7fac94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78c7fad26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:32:50,099 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 06:32:50,102 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_2.npy
2025-04-14 06:32:50,103 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 109 negative comparisons
2025-04-14 06:32:53,053 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 2
Worker node 2 successfully completed gradient computation for iteration 2
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 06:33:10,711 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 06:33:11,259 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:33:11,399 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:33:11,412 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 06:33:27,864 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 06:33:30,787 - __main__ - INFO - Loaded gradients from node 0: 119 negative comparisons
2025-04-14 06:33:31,024 - __main__ - INFO - Loaded gradients from node 1: 110 negative comparisons
2025-04-14 06:33:31,262 - __main__ - INFO - Loaded gradients from node 2: 109 negative comparisons
2025-04-14 06:33:31,425 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 06:33:31,618 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 06:33:31,811 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119130020
2025-04-14 06:33:32,004 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107342950
2025-04-14 06:33:32,197 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84829769
2025-04-14 06:33:32,391 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47187340
2025-04-14 06:33:32,586 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252421
2025-04-14 06:33:32,781 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806424
2025-04-14 06:33:32,975 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552537
2025-04-14 06:33:33,170 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 556032
2025-04-14 06:33:33,521 - __main__ - INFO - Non-zero entries after thresholding: 47187340
2025-04-14 06:33:33,521 - __main__ - INFO - Using relaxed comparison (count_negative: 338)
2025-04-14 06:33:33,521 - __main__ - INFO - Testing stepsize: 0
2025-04-14 06:33:34,144 - __main__ - INFO -   chosen_logps: -279.81625, rejected_logps: -281.88513
2025-04-14 06:33:34,144 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 06:33:34,397 - __main__ - INFO -   chosen_logps: -279.45544, rejected_logps: -282.05597
2025-04-14 06:33:34,397 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 06:33:34,649 - __main__ - INFO -   chosen_logps: -277.75903, rejected_logps: -282.95221
2025-04-14 06:33:34,650 - __main__ - INFO - Testing stepsize: 1
2025-04-14 06:33:34,905 - __main__ - INFO -   chosen_logps: -275.60339, rejected_logps: -283.78928
2025-04-14 06:33:34,905 - __main__ - INFO - Testing stepsize: 2
2025-04-14 06:33:35,165 - __main__ - INFO -   chosen_logps: -272.30835, rejected_logps: -285.96039
2025-04-14 06:33:35,165 - __main__ - INFO - Testing stepsize: 5
2025-04-14 06:33:35,426 - __main__ - INFO -   chosen_logps: -261.55585, rejected_logps: -292.10281
2025-04-14 06:33:35,426 - __main__ - INFO - Testing stepsize: 10
2025-04-14 06:33:35,689 - __main__ - INFO -   chosen_logps: -245.20715, rejected_logps: -302.22043
2025-04-14 06:33:35,689 - __main__ - INFO - Testing stepsize: 15
2025-04-14 06:33:35,953 - __main__ - INFO -   chosen_logps: -229.87569, rejected_logps: -313.31494
2025-04-14 06:33:36,533 - __main__ - INFO - Update scale: 0.006572222222222222
2025-04-14 06:33:36,534 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 06:33:36,615 - __main__ - INFO - Model weights updated successfully
2025-04-14 06:33:37,459 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:33:37,499 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:33:38,583 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 06:33:38,618 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 06:33:38,618 - __main__ - INFO - Skipping full model checkpoint for iteration 2
2025-04-14 06:33:38,618 - __main__ - INFO - Iteration 2 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 2
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 2
Iteration 2 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 3
===========================================
Iteration 3 is in valid list - performing full update
Finding noisy pairs for iteration 3...
2025-04-14 06:34:00,025 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 06:34:00,574 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:34:00,719 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:34:00,733 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 06:34:00,734 - __main__ - INFO - Loading dataset
2025-04-14 06:34:01,716 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 06:34:23,709 - __main__ - INFO - Processing dataset
2025-04-14 06:34:23,959 - __main__ - INFO - Searching for noisy pairs starting from index 1031
2025-04-14 06:34:38,964 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 06:34:52,577 - __main__ - INFO - Noisy pair found at index 1163: chosen_logps=-147.0000, rejected_logps=-149.0000
2025-04-14 06:34:52,578 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 06:34:52,646 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_3
2025-04-14 06:34:52,647 - __main__ - INFO - New dataset offset: 1164
Updated dataset offset to: 1164 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 3
Worker node 1 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 06:35:08,073 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]2025-04-14 06:35:08,417 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]2025-04-14 06:35:08,632 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.34it/s]2025-04-14 06:35:08,776 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:35:08,791 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 06:35:08,791 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
2025-04-14 06:35:08,941 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 06:35:08,972 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 06:35:09,125 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:35:09,141 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 06:35:09,141 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
2025-04-14 06:35:09,488 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:35:09,629 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:35:09,642 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 06:35:09,642 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:35:25,336 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 06:35:25,753 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 06:35:26,240 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 06:35:35,500 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 06:35:35,867 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 06:35:36,332 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 06:35:39,309 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 06:35:39,698 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 06:35:40,185 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 06:35:42,674 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 06:35:42,832 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 06:35:43,349 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 06:35:43,410 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 06:35:43,913 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 06:35:44,119 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 06:35:46,706 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 06:35:47,022 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 06:35:47,109 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 06:35:47,720 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 06:35:47,839 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 06:35:48,019 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 06:35:50,749 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 06:35:51,065 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 06:35:51,349 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 06:35:51,563 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 06:35:51,872 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 06:35:52,452 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 06:35:54,810 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 06:35:54,964 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 06:35:55,231 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 06:35:55,657 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 06:35:55,795 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 06:35:56,558 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 06:35:58,733 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 06:35:59,294 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 06:35:59,330 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 06:35:59,755 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 06:36:00,129 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 06:36:00,824 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 06:36:02,640 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 06:36:03,291 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 06:36:03,439 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 06:36:03,732 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 06:36:04,442 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 06:36:05,064 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 06:36:06,830 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 06:36:07,734 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 06:36:08,054 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 06:36:08,064 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 06:36:08,346 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 06:36:08,422 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 06:36:08,429 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 06:36:09,369 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 06:36:09,370 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 06:36:10,681 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 06:36:12,264 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 06:36:13,052 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 06:36:15,785 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 06:36:16,153 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 06:36:16,404 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 06:36:16,656 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 06:36:16,652 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 06:36:16,995 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 06:37:39,524 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 06:37:40,756 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 06:37:42,871 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 06:37:43,622 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 06:37:45,436 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 06:37:45,547 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 06:37:47,996 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 06:37:48,138 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 06:37:48,723 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 06:37:51,671 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 06:37:52,030 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 06:37:56,424 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 06:37:56,733 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 06:37:56,870 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 06:37:57,111 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 06:37:57,723 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 06:37:59,345 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 06:38:03,066 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 06:38:03,870 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 06:38:04,485 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 06:38:05,364 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 06:38:08,048 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 06:38:09,715 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 06:38:11,767 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 06:38:12,025 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 06:38:13,080 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 06:38:15,900 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 06:38:16,879 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 06:38:20,628 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 06:38:22,317 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 06:40:13,325 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_2.npy
2025-04-14 06:40:13,325 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 154 negative comparisons
2025-04-14 06:40:16,067 - __main__ - INFO - Node 2: Gradient computation completed successfully
[rank6]:[E414 06:40:21.453595619 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a7336817446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7a72ebbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7a72ebbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7a72ebbcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7a72ebbd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a72ebbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7a733697e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7a7337294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7a7337326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 06:40:21.449153392 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72e9d84a9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72e98d7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72e98d7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x72e98d7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x72e98d7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72e98d7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72e9d86105c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72e9d9094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72e9d9126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:40:21,786 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_0.npy
2025-04-14 06:40:21,786 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 152 negative comparisons
Worker node 2 successfully completed gradient computation for iteration 3
[E414 06:40:24.288472253 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x721c97298446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x721c4c5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x721c4c5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x721c4c5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x721c4c5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x721c4c5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x721c973ff5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x721c97e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x721c97f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 06:40:24.312932598 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bd2b2e31446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7bd2681cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7bd2681cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7bd2681cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7bd2681d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bd2681d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7bd2b2f985c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7bd2b3894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7bd2b3926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:40:24,735 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_1.npy
2025-04-14 06:40:24,735 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 147 negative comparisons
2025-04-14 06:40:24,829 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 06:40:27,577 - __main__ - INFO - Node 1: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 3
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 06:40:45,329 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 06:40:45,876 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:40:46,018 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:40:46,030 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 06:41:02,506 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 06:41:05,453 - __main__ - INFO - Loaded gradients from node 0: 152 negative comparisons
2025-04-14 06:41:05,688 - __main__ - INFO - Loaded gradients from node 1: 147 negative comparisons
2025-04-14 06:41:05,920 - __main__ - INFO - Loaded gradients from node 2: 154 negative comparisons
2025-04-14 06:41:06,078 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 06:41:06,264 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 06:41:06,451 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119132106
2025-04-14 06:41:06,638 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107351812
2025-04-14 06:41:06,825 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84841388
2025-04-14 06:41:07,012 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47196819
2025-04-14 06:41:07,199 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22250117
2025-04-14 06:41:07,386 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802809
2025-04-14 06:41:07,573 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552106
2025-04-14 06:41:07,760 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554579
2025-04-14 06:41:08,102 - __main__ - INFO - Non-zero entries after thresholding: 47196819
2025-04-14 06:41:08,102 - __main__ - INFO - Testing stepsize: 0
2025-04-14 06:41:08,851 - __main__ - INFO -   chosen_logps: -147.81314, rejected_logps: -147.74109
2025-04-14 06:41:08,851 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 06:41:09,229 - __main__ - INFO -   chosen_logps: -147.52881, rejected_logps: -147.87697
2025-04-14 06:41:09,229 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 06:41:09,608 - __main__ - INFO -   chosen_logps: -145.59018, rejected_logps: -148.22574
2025-04-14 06:41:09,608 - __main__ - INFO - Testing stepsize: 1
2025-04-14 06:41:09,990 - __main__ - INFO -   chosen_logps: -144.13835, rejected_logps: -148.83405
2025-04-14 06:41:09,991 - __main__ - INFO - Testing stepsize: 2
2025-04-14 06:41:10,376 - __main__ - INFO -   chosen_logps: -140.65883, rejected_logps: -149.88811
2025-04-14 06:41:10,377 - __main__ - INFO - Testing stepsize: 5
2025-04-14 06:41:10,764 - __main__ - INFO -   chosen_logps: -131.56924, rejected_logps: -153.22894
2025-04-14 06:41:10,765 - __main__ - INFO - Testing stepsize: 10
2025-04-14 06:41:11,156 - __main__ - INFO -   chosen_logps: -119.44395, rejected_logps: -158.53607
2025-04-14 06:41:11,156 - __main__ - INFO - Testing stepsize: 15
2025-04-14 06:41:11,549 - __main__ - INFO -   chosen_logps: -110.02155, rejected_logps: -163.65382
2025-04-14 06:41:12,307 - __main__ - INFO - Update scale: 0.008808333333333333
2025-04-14 06:41:12,391 - __main__ - INFO - Model weights updated successfully
2025-04-14 06:41:13,237 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:41:13,273 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:41:14,166 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 06:41:14,201 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 06:41:14,201 - __main__ - INFO - Skipping full model checkpoint for iteration 3
2025-04-14 06:41:14,201 - __main__ - INFO - Iteration 3 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 3
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 3
Iteration 3 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 4
===========================================
Iteration 4 is in valid list - performing full update
Finding noisy pairs for iteration 4...
2025-04-14 06:41:35,342 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 06:41:35,891 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:41:36,037 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:41:36,051 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 06:41:36,051 - __main__ - INFO - Loading dataset
2025-04-14 06:41:36,814 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 06:41:58,915 - __main__ - INFO - Processing dataset
2025-04-14 06:41:59,179 - __main__ - INFO - Searching for noisy pairs starting from index 1164
2025-04-14 06:42:06,187 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 06:42:27,588 - __main__ - INFO - Processed 136 samples without finding a noisy pair
2025-04-14 06:42:37,733 - __main__ - INFO - Noisy pair found at index 1344: chosen_logps=-268.0000, rejected_logps=-270.0000
2025-04-14 06:42:37,734 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 06:42:37,803 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_4
2025-04-14 06:42:37,804 - __main__ - INFO - New dataset offset: 1345
Updated dataset offset to: 1345 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 4
Worker node 1 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 06:42:53,552 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 06:42:53,604 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.31it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]2025-04-14 06:42:53,878 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]2025-04-14 06:42:54,106 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:42:54,161 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]2025-04-14 06:42:54,260 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:42:54,275 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 06:42:54,275 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:42:54,301 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:42:54,314 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 06:42:54,314 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 06:42:54,424 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:42:54,570 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:42:54,585 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 06:42:54,585 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:43:10,803 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 06:43:10,867 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 06:43:11,188 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 06:43:20,792 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 06:43:21,152 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 06:43:21,569 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 06:43:24,666 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 06:43:25,040 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 06:43:25,396 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 06:43:27,628 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 06:43:27,707 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 06:43:28,600 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 06:43:28,659 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 06:43:29,567 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 06:43:29,625 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 06:43:31,784 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 06:43:31,909 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 06:43:32,478 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 06:43:32,659 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 06:43:33,448 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 06:43:33,618 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 06:43:35,805 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 06:43:36,306 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 06:43:36,525 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 06:43:36,852 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 06:43:37,534 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 06:43:37,713 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 06:43:39,331 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 06:43:40,270 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 06:43:40,613 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 06:43:40,792 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 06:43:41,508 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 06:43:41,736 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 06:43:43,468 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 06:43:44,121 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 06:43:44,756 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 06:43:45,140 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 06:43:45,506 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 06:43:45,881 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 06:43:48,028 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 06:43:48,631 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 06:43:48,822 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 06:43:48,838 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 06:43:49,785 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 06:43:50,118 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 06:43:51,390 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 06:43:52,696 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 06:43:52,735 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 06:43:52,937 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 06:43:52,941 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 06:43:54,043 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 06:43:54,047 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 06:43:54,577 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 06:43:54,587 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 06:43:56,102 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 06:43:56,932 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 06:43:57,093 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 06:44:00,643 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 06:44:00,866 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 06:44:01,292 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 06:44:01,296 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 06:44:01,493 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 06:44:01,506 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 06:45:19,552 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 06:45:19,584 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 06:45:23,822 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 06:45:24,973 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 06:45:26,294 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 06:45:27,263 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 06:45:27,964 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 06:45:28,007 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 06:45:31,065 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 06:45:31,897 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 06:45:31,917 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 06:45:32,469 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 06:45:33,137 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 06:45:36,308 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 06:45:38,211 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 06:45:39,377 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 06:45:40,156 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 06:45:43,107 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 06:45:43,300 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 06:45:43,712 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 06:45:46,582 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 06:45:48,079 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 06:45:48,191 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 06:45:48,648 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 06:45:52,103 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 06:45:52,522 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 06:45:52,825 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 06:45:53,203 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 06:45:53,464 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 06:45:54,921 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 06:47:42.044313651 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7281fe5be446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7281b39cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7281b39cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7281b39cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7281b39d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7281b39d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7281fe7255c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7281ff094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7281ff126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:47:42,374 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_1.npy
2025-04-14 06:47:42,374 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 193 negative comparisons
[E414 06:47:43.082230183 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bb654498446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7bb6097cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7bb6097cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7bb6097cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7bb6097d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bb6097d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7bb6545ff5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7bb655094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7bb655126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 06:47:43.089789921 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x762dbae17446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x762d701cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x762d701cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x762d701cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x762d701d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x762d701d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x762dbaf7e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x762dbb894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x762dbb926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:47:43,349 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_0.npy
2025-04-14 06:47:43,350 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 218 negative comparisons
2025-04-14 06:47:45,237 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 06:47:46,193 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 06:47:47,006 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_2.npy
2025-04-14 06:47:47,007 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 192 negative comparisons
[E414 06:47:47.971985427 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73d7e2bef446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x73d797fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x73d797fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x73d797fcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x73d797fd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73d797fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x73d7e2d565c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x73d7e3894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x73d7e3926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:47:50,196 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 4
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 4
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 06:48:08,700 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.31it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 06:48:09,244 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:48:09,384 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:48:09,397 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 06:48:25,913 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 06:48:28,793 - __main__ - INFO - Loaded gradients from node 0: 218 negative comparisons
2025-04-14 06:48:29,027 - __main__ - INFO - Loaded gradients from node 1: 193 negative comparisons
2025-04-14 06:48:29,261 - __main__ - INFO - Loaded gradients from node 2: 192 negative comparisons
2025-04-14 06:48:29,420 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 06:48:29,611 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 06:48:29,801 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119134928
2025-04-14 06:48:29,993 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107351596
2025-04-14 06:48:30,184 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84834415
2025-04-14 06:48:30,376 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47196831
2025-04-14 06:48:30,569 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22254860
2025-04-14 06:48:30,760 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802881
2025-04-14 06:48:30,951 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551505
2025-04-14 06:48:31,141 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555745
2025-04-14 06:48:31,487 - __main__ - INFO - Non-zero entries after thresholding: 47196831
2025-04-14 06:48:31,487 - __main__ - INFO - Testing stepsize: 0
2025-04-14 06:48:32,139 - __main__ - INFO -   chosen_logps: -267.78009, rejected_logps: -269.47742
2025-04-14 06:48:32,139 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 06:48:32,426 - __main__ - INFO -   chosen_logps: -267.51007, rejected_logps: -270.03986
2025-04-14 06:48:32,426 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 06:48:32,711 - __main__ - INFO -   chosen_logps: -265.24463, rejected_logps: -271.41733
2025-04-14 06:48:32,711 - __main__ - INFO - Testing stepsize: 1
2025-04-14 06:48:32,999 - __main__ - INFO -   chosen_logps: -263.18579, rejected_logps: -273.95099
2025-04-14 06:48:32,999 - __main__ - INFO - Testing stepsize: 2
2025-04-14 06:48:33,291 - __main__ - INFO -   chosen_logps: -259.14178, rejected_logps: -276.48923
2025-04-14 06:48:33,291 - __main__ - INFO - Testing stepsize: 5
2025-04-14 06:48:33,584 - __main__ - INFO -   chosen_logps: -246.25790, rejected_logps: -287.70807
2025-04-14 06:48:33,584 - __main__ - INFO - Testing stepsize: 10
2025-04-14 06:48:33,879 - __main__ - INFO -   chosen_logps: -227.92212, rejected_logps: -310.14038
2025-04-14 06:48:33,879 - __main__ - INFO - Testing stepsize: 15
2025-04-14 06:48:34,175 - __main__ - INFO -   chosen_logps: -211.93767, rejected_logps: -333.77887
2025-04-14 06:48:34,802 - __main__ - INFO - Update scale: 0.011725000000000001
2025-04-14 06:48:34,883 - __main__ - INFO - Model weights updated successfully
2025-04-14 06:48:35,735 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:48:35,768 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:48:37,058 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 06:48:37,093 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 06:48:37,093 - __main__ - INFO - Skipping full model checkpoint for iteration 4
2025-04-14 06:48:37,093 - __main__ - INFO - Iteration 4 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 4
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 4
Iteration 4 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 5
===========================================
Iteration 5 is in valid list - performing full update
Finding noisy pairs for iteration 5...
2025-04-14 06:48:58,236 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 06:48:58,786 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:48:58,931 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:48:58,945 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 06:48:58,945 - __main__ - INFO - Loading dataset
2025-04-14 06:49:00,088 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 06:49:21,596 - __main__ - INFO - Processing dataset
2025-04-14 06:49:21,841 - __main__ - INFO - Searching for noisy pairs starting from index 1345
2025-04-14 06:49:33,053 - __main__ - INFO - Processed 55 samples without finding a noisy pair
2025-04-14 06:49:53,726 - __main__ - INFO - Processed 155 samples without finding a noisy pair
2025-04-14 06:49:59,180 - __main__ - INFO - Noisy pair found at index 1530: chosen_logps=-360.0000, rejected_logps=-362.0000
2025-04-14 06:49:59,181 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 06:49:59,249 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_5
2025-04-14 06:49:59,249 - __main__ - INFO - New dataset offset: 1531
Updated dataset offset to: 1531 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 5
Worker node 2 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 06:50:14,650 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 06:50:14,853 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 06:50:14,944 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.31it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]2025-04-14 06:50:15,210 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
2025-04-14 06:50:15,354 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:50:15,369 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 06:50:15,369 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:50:15,407 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
2025-04-14 06:50:15,501 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:50:15,560 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:50:15,575 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 06:50:15,575 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:50:15,641 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:50:15,654 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 06:50:15,654 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:50:31,952 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 06:50:32,127 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 06:50:32,201 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 06:50:41,970 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 06:50:42,224 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 06:50:42,431 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 06:50:45,774 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 06:50:46,245 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 06:50:46,400 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 06:50:48,686 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 06:50:49,198 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 06:50:49,533 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 06:50:49,727 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 06:50:50,180 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 06:50:50,359 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 06:50:52,801 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 06:50:53,526 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 06:50:53,618 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 06:50:53,716 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 06:50:53,964 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 06:50:54,353 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 06:50:57,019 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 06:50:57,306 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 06:50:57,421 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 06:50:57,623 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 06:50:57,949 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 06:50:58,295 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 06:51:01,001 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 06:51:01,082 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 06:51:01,325 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 06:51:02,005 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 06:51:02,014 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 06:51:02,347 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 06:51:04,875 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 06:51:05,368 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 06:51:05,426 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 06:51:05,931 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 06:51:06,282 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 06:51:06,824 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 06:51:08,583 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 06:51:09,212 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 06:51:09,531 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 06:51:09,583 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 06:51:10,825 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 06:51:10,895 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 06:51:12,513 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 06:51:13,410 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 06:51:14,024 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 06:51:14,027 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 06:51:14,441 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 06:51:15,197 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 06:51:15,198 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 06:51:15,246 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 06:51:15,250 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 06:51:16,993 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 06:51:17,967 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 06:51:18,371 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 06:51:21,327 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 06:51:21,397 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 06:51:22,431 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 06:51:22,753 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 06:51:22,948 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 06:51:23,098 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 06:52:39,900 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 06:52:42,474 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 06:52:46,160 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 06:52:46,192 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 06:52:46,414 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 06:52:47,204 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 06:52:50,249 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 06:52:51,041 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 06:52:53,368 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 06:52:54,287 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 06:52:55,823 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 06:52:57,994 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 06:52:58,467 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 06:52:58,682 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 06:53:01,435 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 06:53:02,467 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 06:53:03,172 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 06:53:04,873 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 06:53:05,438 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 06:53:07,343 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 06:53:08,868 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 06:53:09,298 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 06:53:10,421 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 06:53:11,182 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 06:53:12,972 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 06:53:13,063 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 06:53:13,786 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 06:53:14,616 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 06:53:15,281 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 06:53:18,479 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 06:55:02.508826995 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7417971f1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74174c5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74174c5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x74174c5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x74174c5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74174c5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7417973585c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x741797c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x741797d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 06:55:02.515208703 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78a59fb82446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78a554fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78a554fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x78a554fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x78a554fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78a554fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78a59fce95c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78a5a0694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78a5a0726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:55:02,787 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_0.npy
2025-04-14 06:55:02,787 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 95 negative comparisons
[E414 06:55:04.316230570 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x773c5416c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x773c099cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x773c099cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x773c099cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x773c099d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x773c099d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x773c5469f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x773c55094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x773c55126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:55:04,727 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_1.npy
2025-04-14 06:55:04,727 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 95 negative comparisons
2025-04-14 06:55:05,736 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 06:55:07,738 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 06:55:11.502638012 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f501523f446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f4fca5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f4fca5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f4fca5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f4fca5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f4fca5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f50153a65c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f5015e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f5015f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 06:55:11.502469045 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70a2f8a87446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70a2addcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70a2addcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x70a2addcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x70a2addd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70a2addd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70a2f8bee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70a2f9694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70a2f9726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 06:55:11,833 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_2.npy
2025-04-14 06:55:11,834 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 94 negative comparisons
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 5
2025-04-14 06:55:14,726 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 5
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 06:55:32,118 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 06:55:32,667 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:55:32,813 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:55:32,825 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 06:55:49,323 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 06:55:52,208 - __main__ - INFO - Loaded gradients from node 0: 95 negative comparisons
2025-04-14 06:55:52,441 - __main__ - INFO - Loaded gradients from node 1: 95 negative comparisons
2025-04-14 06:55:52,675 - __main__ - INFO - Loaded gradients from node 2: 94 negative comparisons
2025-04-14 06:55:52,834 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 06:55:53,024 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 06:55:53,214 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119135446
2025-04-14 06:55:53,403 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107348845
2025-04-14 06:55:53,593 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84831875
2025-04-14 06:55:53,783 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47199535
2025-04-14 06:55:53,972 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22256374
2025-04-14 06:55:54,162 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806210
2025-04-14 06:55:54,352 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1549985
2025-04-14 06:55:54,541 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554736
2025-04-14 06:55:54,886 - __main__ - INFO - Non-zero entries after thresholding: 47199535
2025-04-14 06:55:54,886 - __main__ - INFO - Using relaxed comparison (count_negative: 284)
2025-04-14 06:55:54,886 - __main__ - INFO - Testing stepsize: 0
2025-04-14 06:55:55,563 - __main__ - INFO -   chosen_logps: -360.31619, rejected_logps: -361.51294
2025-04-14 06:55:55,563 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 06:55:55,865 - __main__ - INFO -   chosen_logps: -360.04449, rejected_logps: -361.70435
2025-04-14 06:55:55,865 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 06:55:56,168 - __main__ - INFO -   chosen_logps: -358.04834, rejected_logps: -362.56628
2025-04-14 06:55:56,168 - __main__ - INFO - Testing stepsize: 1
2025-04-14 06:55:56,474 - __main__ - INFO -   chosen_logps: -355.70908, rejected_logps: -363.14319
2025-04-14 06:55:56,475 - __main__ - INFO - Testing stepsize: 2
2025-04-14 06:55:56,784 - __main__ - INFO -   chosen_logps: -350.40295, rejected_logps: -365.31476
2025-04-14 06:55:56,784 - __main__ - INFO - Testing stepsize: 5
2025-04-14 06:55:57,094 - __main__ - INFO -   chosen_logps: -336.23535, rejected_logps: -372.15302
2025-04-14 06:55:57,095 - __main__ - INFO - Testing stepsize: 10
2025-04-14 06:55:57,406 - __main__ - INFO -   chosen_logps: -316.26941, rejected_logps: -386.53485
2025-04-14 06:55:57,407 - __main__ - INFO - Testing stepsize: 15
2025-04-14 06:55:57,719 - __main__ - INFO -   chosen_logps: -299.63199, rejected_logps: -402.12347
2025-04-14 06:55:58,373 - __main__ - INFO - Update scale: 0.005522222222222223
2025-04-14 06:55:58,374 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 06:55:58,454 - __main__ - INFO - Model weights updated successfully
2025-04-14 06:55:59,297 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:55:59,334 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:56:00,233 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 06:56:00,265 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 06:56:00,266 - __main__ - INFO - Skipping full model checkpoint for iteration 5
2025-04-14 06:56:00,266 - __main__ - INFO - Iteration 5 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 5
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 5
Iteration 5 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 6
===========================================
Iteration 6 is in valid list - performing full update
Finding noisy pairs for iteration 6...
2025-04-14 06:56:21,300 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 06:56:21,849 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:56:21,994 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 06:56:22,008 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 06:56:22,008 - __main__ - INFO - Loading dataset
2025-04-14 06:56:23,736 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 06:56:45,116 - __main__ - INFO - Processing dataset
2025-04-14 06:56:45,359 - __main__ - INFO - Searching for noisy pairs starting from index 1531
2025-04-14 06:56:59,454 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 06:57:04,010 - __main__ - INFO - Noisy pair found at index 1624: chosen_logps=-127.5000, rejected_logps=-130.0000
2025-04-14 06:57:04,011 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 06:57:04,077 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_6
2025-04-14 06:57:04,078 - __main__ - INFO - New dataset offset: 1625
Updated dataset offset to: 1625 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 6
Worker node 2 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 06:57:19,468 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.21it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]2025-04-14 06:57:19,874 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
2025-04-14 06:57:19,929 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 06:57:20,016 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 06:57:20,161 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:57:20,176 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 06:57:20,176 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.52it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 06:57:20,425 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:57:20,486 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 06:57:20,578 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:57:20,593 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 06:57:20,594 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:57:20,626 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 06:57:20,639 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 06:57:20,639 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 06:57:36,765 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 06:57:37,176 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 06:57:37,193 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 06:57:47,218 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 06:57:47,384 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 06:57:47,671 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 06:57:51,080 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 06:57:51,186 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 06:57:51,652 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 06:57:54,248 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 06:57:54,568 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 06:57:54,753 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 06:57:55,096 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 06:57:55,144 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 06:57:55,608 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 06:57:58,574 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 06:57:59,086 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 06:57:59,163 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 06:57:59,212 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 06:57:59,320 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 06:57:59,821 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 06:58:02,364 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 06:58:02,431 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 06:58:02,808 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 06:58:03,044 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 06:58:03,303 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 06:58:03,697 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 06:58:06,617 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 06:58:06,726 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 06:58:07,096 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 06:58:07,158 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 06:58:07,322 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 06:58:07,595 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 06:58:10,515 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 06:58:10,554 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 06:58:11,065 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 06:58:11,146 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 06:58:11,310 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 06:58:11,830 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 06:58:14,859 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 06:58:14,856 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 06:58:15,421 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 06:58:15,540 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 06:58:15,620 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 06:58:16,154 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 06:58:18,650 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 06:58:18,934 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 06:58:19,260 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 06:58:19,622 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 06:58:19,629 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 06:58:19,630 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 06:58:19,639 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 06:58:20,221 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 06:58:20,224 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 06:58:22,843 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 06:58:22,955 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 06:58:23,599 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 06:58:26,925 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 06:58:27,402 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 06:58:27,506 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 06:58:27,614 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 06:58:27,631 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 06:58:27,690 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 06:59:49,602 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 06:59:51,128 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 06:59:53,880 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 06:59:56,799 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 06:59:57,452 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 06:59:58,234 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 06:59:58,334 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 06:59:58,552 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 07:00:00,236 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 07:00:02,557 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 07:00:03,867 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 07:00:04,359 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 07:00:07,340 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 07:00:08,074 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 07:00:09,749 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 07:00:10,724 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 07:00:13,248 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 07:00:13,827 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 07:00:14,227 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 07:00:15,099 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 07:00:18,158 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 07:00:18,730 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 07:00:19,128 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 07:00:22,079 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 07:00:22,300 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 07:00:22,703 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 07:00:23,134 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 07:00:23,168 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 07:00:26,779 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 07:00:26,944 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 07:02:17.996883456 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74db46578446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74dafb9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74dafb9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x74dafb9cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x74dafb9d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74dafb9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74db466df5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74db47094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74db47126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:02:17,188 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_0.npy
2025-04-14 07:02:17,190 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 155 negative comparisons
2025-04-14 07:02:20,174 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 07:02:23.223374530 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f9e9bfa1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f9e513cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f9e513cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f9e513cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f9e513d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f9e513d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f9e9c1085c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f9e9cc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f9e9cd26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:02:23.232103498 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f5101ba9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f50b6fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f50b6fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f50b6fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f50b6fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f50b6fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f5101d105c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f5102894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f5102926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:02:23,542 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_2.npy
2025-04-14 07:02:23,542 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 146 negative comparisons
[E414 07:02:23.479783710 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f248fca1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f2444fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f2444fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f2444fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f2444fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f2444fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f248fe085c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f2490894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f2490926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:02:23.480052387 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71c3c9238446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71c37e5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71c37e5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x71c37e5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x71c37e5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71c37e5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x71c3c939f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x71c3c9c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x71c3c9d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:02:23,862 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_1.npy
2025-04-14 07:02:23,862 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 151 negative comparisons
2025-04-14 07:02:26,440 - __main__ - INFO - Node 2: Gradient computation completed successfully
2025-04-14 07:02:26,726 - __main__ - INFO - Node 1: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 6
Worker node 1 successfully completed gradient computation for iteration 6
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 07:02:43,882 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 07:02:44,430 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:02:44,572 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:02:44,584 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 07:03:01,091 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 07:03:03,990 - __main__ - INFO - Loaded gradients from node 0: 155 negative comparisons
2025-04-14 07:03:04,222 - __main__ - INFO - Loaded gradients from node 1: 151 negative comparisons
2025-04-14 07:03:04,453 - __main__ - INFO - Loaded gradients from node 2: 146 negative comparisons
2025-04-14 07:03:04,612 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 07:03:04,801 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 07:03:04,990 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119128846
2025-04-14 07:03:05,179 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107343277
2025-04-14 07:03:05,367 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84829686
2025-04-14 07:03:05,555 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47195123
2025-04-14 07:03:05,743 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22256869
2025-04-14 07:03:05,930 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8807491
2025-04-14 07:03:06,118 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550834
2025-04-14 07:03:06,306 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554754
2025-04-14 07:03:06,648 - __main__ - INFO - Non-zero entries after thresholding: 47195123
2025-04-14 07:03:06,648 - __main__ - INFO - Testing stepsize: 0
2025-04-14 07:03:07,373 - __main__ - INFO -   chosen_logps: -127.93109, rejected_logps: -129.48669
2025-04-14 07:03:07,373 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 07:03:07,738 - __main__ - INFO -   chosen_logps: -127.41769, rejected_logps: -129.48651
2025-04-14 07:03:07,738 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 07:03:08,103 - __main__ - INFO -   chosen_logps: -125.97687, rejected_logps: -129.96329
2025-04-14 07:03:08,103 - __main__ - INFO - Testing stepsize: 1
2025-04-14 07:03:08,471 - __main__ - INFO -   chosen_logps: -123.92427, rejected_logps: -130.27495
2025-04-14 07:03:08,471 - __main__ - INFO - Testing stepsize: 2
2025-04-14 07:03:08,842 - __main__ - INFO -   chosen_logps: -120.54630, rejected_logps: -131.07491
2025-04-14 07:03:08,842 - __main__ - INFO - Testing stepsize: 5
2025-04-14 07:03:09,218 - __main__ - INFO -   chosen_logps: -110.90324, rejected_logps: -133.44049
2025-04-14 07:03:09,218 - __main__ - INFO - Testing stepsize: 10
2025-04-14 07:03:09,591 - __main__ - INFO -   chosen_logps: -98.19017, rejected_logps: -137.21378
2025-04-14 07:03:09,591 - __main__ - INFO - Testing stepsize: 15
2025-04-14 07:03:09,968 - __main__ - INFO -   chosen_logps: -88.59141, rejected_logps: -141.09467
2025-04-14 07:03:10,704 - __main__ - INFO - Update scale: 0.00878888888888889
2025-04-14 07:03:10,785 - __main__ - INFO - Model weights updated successfully
2025-04-14 07:03:12,022 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:03:12,074 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:03:13,152 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 07:03:13,186 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 07:03:13,186 - __main__ - INFO - Skipping full model checkpoint for iteration 6
2025-04-14 07:03:13,186 - __main__ - INFO - Iteration 6 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 6
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 6
Iteration 6 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 7
===========================================
Iteration 7 is in valid list - performing full update
Finding noisy pairs for iteration 7...
2025-04-14 07:03:34,533 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 07:03:35,083 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:03:35,228 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:03:35,242 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 07:03:35,242 - __main__ - INFO - Loading dataset
2025-04-14 07:03:36,565 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 07:03:58,204 - __main__ - INFO - Processing dataset
2025-04-14 07:03:58,453 - __main__ - INFO - Searching for noisy pairs starting from index 1625
2025-04-14 07:04:06,071 - __main__ - INFO - Noisy pair found at index 1663: chosen_logps=-49.7500, rejected_logps=-50.5000
2025-04-14 07:04:06,072 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 07:04:06,140 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_7
2025-04-14 07:04:06,141 - __main__ - INFO - New dataset offset: 1664
Updated dataset offset to: 1664 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 7
Worker node 2 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 07:04:20,428 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 07:04:20,926 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 07:04:20,926 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 07:04:20,975 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 07:04:21,119 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:04:21,134 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 07:04:21,134 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.39it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 07:04:21,477 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:04:21,483 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:04:21,623 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 07:04:21,631 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:04:21,636 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 07:04:21,636 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:04:21,646 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 07:04:21,646 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:04:37,778 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 07:04:38,236 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 07:04:38,256 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 07:04:48,113 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 07:04:48,250 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 07:04:48,672 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 07:04:51,968 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 07:04:52,360 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 07:04:52,467 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 07:04:55,161 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 07:04:55,537 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 07:04:55,974 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 07:04:56,288 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 07:04:56,342 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 07:04:56,433 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 07:04:59,298 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 07:04:59,540 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 07:05:00,010 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 07:05:00,103 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 07:05:00,241 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 07:05:00,378 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 07:05:03,487 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 07:05:03,652 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 07:05:03,832 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 07:05:03,973 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 07:05:04,076 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 07:05:04,199 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 07:05:07,206 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 07:05:07,851 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 07:05:07,887 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 07:05:08,144 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 07:05:08,246 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 07:05:08,278 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 07:05:11,000 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 07:05:11,402 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 07:05:11,662 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 07:05:11,957 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 07:05:12,353 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 07:05:12,534 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 07:05:15,328 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 07:05:15,672 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 07:05:15,878 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 07:05:16,145 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 07:05:16,711 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 07:05:16,774 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 07:05:19,498 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 07:05:19,625 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 07:05:19,740 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 07:05:20,111 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 07:05:20,115 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 07:05:20,876 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 07:05:20,880 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 07:05:21,077 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 07:05:21,085 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 07:05:23,485 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 07:05:24,150 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 07:05:24,180 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 07:05:27,935 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 07:05:28,112 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 07:05:28,422 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 07:05:28,640 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 07:05:28,712 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 07:05:29,049 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 07:06:50,395 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 07:06:50,560 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 07:06:53,592 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 07:06:56,270 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 07:06:56,297 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 07:07:00,251 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 07:07:00,942 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 07:07:01,570 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 07:07:02,685 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 07:07:03,568 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 07:07:07,013 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 07:07:07,137 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 07:07:08,253 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 07:07:09,155 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 07:07:11,083 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 07:07:12,303 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 07:07:13,068 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 07:07:13,605 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 07:07:14,293 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 07:07:15,880 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 07:07:16,534 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 07:07:18,600 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 07:07:19,798 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 07:07:20,049 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 07:07:23,161 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 07:07:23,667 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 07:07:23,675 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 07:07:24,158 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 07:07:24,188 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 07:07:28,639 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 07:09:15,935 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_0.npy
2025-04-14 07:09:15,935 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 76 negative comparisons
[E414 07:09:16.880623540 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7827d9b46446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78278efcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78278efcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x78278efcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x78278efd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78278efd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7827d9cad5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7827da694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7827da726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:09:16.890256465 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7684c8710446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76847d9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76847d9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x76847d9cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x76847d9d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76847d9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7684c88775c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7684c9294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7684c9326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:09:18,822 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_1.npy
2025-04-14 07:09:18,822 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 58 negative comparisons
2025-04-14 07:09:19,002 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 07:09:21,691 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 07:09:24.663072050 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x765107517446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7650bc7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7650bc7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7650bc7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7650bc7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7650bc7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x76510767e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x765108094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x765108126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:09:24.674080769 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e9d6771e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e9d1c9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e9d1c9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7e9d1c9cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7e9d1c9d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e9d1c9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e9d678855c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e9d68294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e9d68326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:09:25,001 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_2.npy
2025-04-14 07:09:25,001 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 85 negative comparisons
Waiting for worker nodes to complete gradient computation...
2025-04-14 07:09:27,972 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 7
Worker node 2 successfully completed gradient computation for iteration 7
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 07:09:46,296 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 07:09:46,845 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:09:46,987 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:09:46,999 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 07:10:03,456 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 07:10:06,340 - __main__ - INFO - Loaded gradients from node 0: 76 negative comparisons
2025-04-14 07:10:06,574 - __main__ - INFO - Loaded gradients from node 1: 58 negative comparisons
2025-04-14 07:10:06,808 - __main__ - INFO - Loaded gradients from node 2: 85 negative comparisons
2025-04-14 07:10:06,968 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 07:10:07,159 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 07:10:07,350 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119135790
2025-04-14 07:10:07,540 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107348171
2025-04-14 07:10:07,731 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84837898
2025-04-14 07:10:07,922 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47192771
2025-04-14 07:10:08,113 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255141
2025-04-14 07:10:08,304 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802545
2025-04-14 07:10:08,496 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551373
2025-04-14 07:10:08,688 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555473
2025-04-14 07:10:09,036 - __main__ - INFO - Non-zero entries after thresholding: 47192771
2025-04-14 07:10:09,036 - __main__ - INFO - Using relaxed comparison (count_negative: 219)
2025-04-14 07:10:09,036 - __main__ - INFO - Testing stepsize: 0
2025-04-14 07:10:09,764 - __main__ - INFO -   chosen_logps: -49.79002, rejected_logps: -50.12825
2025-04-14 07:10:09,764 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 07:10:10,126 - __main__ - INFO -   chosen_logps: -49.78971, rejected_logps: -50.19066
2025-04-14 07:10:10,126 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 07:10:10,489 - __main__ - INFO -   chosen_logps: -49.53848, rejected_logps: -50.35967
2025-04-14 07:10:10,490 - __main__ - INFO - Testing stepsize: 1
2025-04-14 07:10:10,855 - __main__ - INFO -   chosen_logps: -48.95001, rejected_logps: -50.69352
2025-04-14 07:10:10,855 - __main__ - INFO - Testing stepsize: 2
2025-04-14 07:10:11,225 - __main__ - INFO -   chosen_logps: -48.03540, rejected_logps: -51.12804
2025-04-14 07:10:11,225 - __main__ - INFO - Testing stepsize: 5
2025-04-14 07:10:11,596 - __main__ - INFO -   chosen_logps: -45.43065, rejected_logps: -53.13843
2025-04-14 07:10:11,596 - __main__ - INFO - Testing stepsize: 10
2025-04-14 07:10:11,968 - __main__ - INFO -   chosen_logps: -41.69740, rejected_logps: -56.26250
2025-04-14 07:10:11,968 - __main__ - INFO - Testing stepsize: 15
2025-04-14 07:10:12,344 - __main__ - INFO -   chosen_logps: -38.72301, rejected_logps: -59.08648
2025-04-14 07:10:13,075 - __main__ - INFO - Update scale: 0.004258333333333334
2025-04-14 07:10:13,076 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 07:10:13,157 - __main__ - INFO - Model weights updated successfully
2025-04-14 07:10:14,004 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:10:14,041 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:10:14,920 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 07:10:14,955 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 07:10:14,955 - __main__ - INFO - Skipping full model checkpoint for iteration 7
2025-04-14 07:10:14,955 - __main__ - INFO - Iteration 7 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 7
Worker node 2 successfully applied weights for iteration 7
Iteration 7 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 8
===========================================
Iteration 8 is in valid list - performing full update
Finding noisy pairs for iteration 8...
2025-04-14 07:10:36,102 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 07:10:36,648 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:10:36,793 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:10:36,808 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 07:10:36,808 - __main__ - INFO - Loading dataset
2025-04-14 07:10:38,722 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 07:11:00,051 - __main__ - INFO - Processing dataset
2025-04-14 07:11:00,293 - __main__ - INFO - Searching for noisy pairs starting from index 1664
2025-04-14 07:11:08,761 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 07:11:17,526 - __main__ - INFO - Noisy pair found at index 1746: chosen_logps=-23.8750, rejected_logps=-24.2500
2025-04-14 07:11:17,527 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 07:11:17,594 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_8
2025-04-14 07:11:17,595 - __main__ - INFO - New dataset offset: 1747
Updated dataset offset to: 1747 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 8
Worker node 2 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 07:11:32,841 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]2025-04-14 07:11:33,157 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 07:11:33,286 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]2025-04-14 07:11:33,388 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 07:11:33,532 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:11:33,546 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 07:11:33,546 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.26it/s]2025-04-14 07:11:33,711 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
2025-04-14 07:11:33,842 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:11:33,864 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:11:33,879 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 07:11:33,879 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:11:33,982 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:11:33,994 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 07:11:33,994 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:11:50,152 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 07:11:50,437 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 07:11:50,519 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 07:12:00,268 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 07:12:00,591 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 07:12:00,621 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 07:12:04,087 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 07:12:04,328 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 07:12:04,409 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 07:12:07,531 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 07:12:07,592 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 07:12:07,800 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 07:12:08,014 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 07:12:08,224 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 07:12:08,320 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 07:12:11,456 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 07:12:11,627 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 07:12:11,991 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 07:12:12,012 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 07:12:12,183 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 07:12:12,237 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 07:12:15,315 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 07:12:15,478 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 07:12:15,711 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 07:12:15,991 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 07:12:16,016 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 07:12:16,161 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 07:12:19,275 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 07:12:19,580 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 07:12:19,882 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 07:12:19,976 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 07:12:20,128 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 07:12:20,334 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 07:12:23,122 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 07:12:23,171 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 07:12:23,879 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 07:12:24,004 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 07:12:24,117 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 07:12:24,702 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 07:12:27,553 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 07:12:27,555 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 07:12:27,800 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 07:12:28,175 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 07:12:28,625 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 07:12:29,022 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 07:12:31,252 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 07:12:31,361 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 07:12:32,216 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 07:12:32,464 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 07:12:32,467 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 07:12:32,815 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 07:12:32,821 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 07:12:33,541 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 07:12:33,547 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 07:12:35,782 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 07:12:35,835 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 07:12:36,490 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 07:12:40,012 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 07:12:40,222 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 07:12:40,460 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 07:12:40,918 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 07:12:40,932 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 07:12:41,062 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 07:14:04,320 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 07:14:08,546 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 07:14:08,763 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 07:14:09,168 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 07:14:10,289 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 07:14:11,400 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 07:14:12,374 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 07:14:13,286 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 07:14:15,491 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 07:14:15,994 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 07:14:16,494 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 07:14:17,631 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 07:14:19,843 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 07:14:20,903 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 07:14:20,965 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 07:14:26,620 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 07:14:26,901 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 07:14:28,651 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 07:14:28,946 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 07:14:29,299 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 07:14:29,327 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 07:14:31,930 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 07:14:33,770 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 07:14:36,819 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 07:14:37,279 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 07:14:37,520 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 07:14:38,356 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 07:14:38,807 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 07:14:41,463 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 07:14:42,022 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 07:16:30.716498934 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7af0b1ee8446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7af0671cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7af0671cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7af0671cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7af0671d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7af0671d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7af0b204f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7af0b2a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7af0b2b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:16:30.724301200 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7574701a9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7574255cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7574255cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7574255cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7574255d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7574255d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7574703105c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x757470c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x757470d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:16:31,008 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_0.npy
2025-04-14 07:16:31,009 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 90 negative comparisons
2025-04-14 07:16:33,949 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 07:16:33,970 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_1.npy
2025-04-14 07:16:33,970 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 100 negative comparisons
[E414 07:16:34.863820677 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x789f69210446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x789f1e5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x789f1e5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x789f1e5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x789f1e5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x789f1e5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x789f693775c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x789f69c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x789f69d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:16:34.866539870 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x784348847446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7842fdbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7842fdbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7842fdbcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7842fdbd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7842fdbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7843489ae5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x784349494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x784349526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:16:37,200 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 07:16:40,344 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_2.npy
2025-04-14 07:16:40,345 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 109 negative comparisons
Waiting for worker nodes to complete gradient computation...
2025-04-14 07:16:43,338 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 8
Worker node 2 successfully completed gradient computation for iteration 8
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 07:17:00,202 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 07:17:00,750 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:17:00,891 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:17:00,903 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 07:17:17,378 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 07:17:20,244 - __main__ - INFO - Loaded gradients from node 0: 90 negative comparisons
2025-04-14 07:17:20,474 - __main__ - INFO - Loaded gradients from node 1: 100 negative comparisons
2025-04-14 07:17:20,705 - __main__ - INFO - Loaded gradients from node 2: 109 negative comparisons
2025-04-14 07:17:20,863 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 07:17:21,050 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 07:17:21,237 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119137269
2025-04-14 07:17:21,424 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107354433
2025-04-14 07:17:21,611 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84833433
2025-04-14 07:17:21,798 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47192532
2025-04-14 07:17:21,986 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22249921
2025-04-14 07:17:22,173 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802381
2025-04-14 07:17:22,360 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551815
2025-04-14 07:17:22,547 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554857
2025-04-14 07:17:22,888 - __main__ - INFO - Non-zero entries after thresholding: 47192532
2025-04-14 07:17:22,888 - __main__ - INFO - Using relaxed comparison (count_negative: 299)
2025-04-14 07:17:22,888 - __main__ - INFO - Testing stepsize: 0
2025-04-14 07:17:23,633 - __main__ - INFO -   chosen_logps: -23.92811, rejected_logps: -23.81993
2025-04-14 07:17:23,633 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 07:17:24,017 - __main__ - INFO -   chosen_logps: -23.91472, rejected_logps: -23.80077
2025-04-14 07:17:24,017 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 07:17:24,401 - __main__ - INFO -   chosen_logps: -23.52732, rejected_logps: -23.91472
2025-04-14 07:17:24,401 - __main__ - INFO - Testing stepsize: 1
2025-04-14 07:17:24,787 - __main__ - INFO -   chosen_logps: -23.03574, rejected_logps: -24.36967
2025-04-14 07:17:24,787 - __main__ - INFO - Testing stepsize: 2
2025-04-14 07:17:25,178 - __main__ - INFO -   chosen_logps: -22.06879, rejected_logps: -25.09712
2025-04-14 07:17:25,178 - __main__ - INFO - Testing stepsize: 5
2025-04-14 07:17:25,571 - __main__ - INFO -   chosen_logps: -19.50418, rejected_logps: -26.65549
2025-04-14 07:17:25,571 - __main__ - INFO - Testing stepsize: 10
2025-04-14 07:17:25,964 - __main__ - INFO -   chosen_logps: -15.58253, rejected_logps: -29.69354
2025-04-14 07:17:25,964 - __main__ - INFO - Testing stepsize: 15
2025-04-14 07:17:26,360 - __main__ - INFO -   chosen_logps: -12.55601, rejected_logps: -32.76307
2025-04-14 07:17:27,121 - __main__ - INFO - Update scale: 0.005813888888888889
2025-04-14 07:17:27,122 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 07:17:27,202 - __main__ - INFO - Model weights updated successfully
2025-04-14 07:17:28,045 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:17:28,081 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:17:28,981 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 07:17:29,016 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 07:17:29,016 - __main__ - INFO - Skipping full model checkpoint for iteration 8
2025-04-14 07:17:29,016 - __main__ - INFO - Iteration 8 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 8
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 8
Iteration 8 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 9
===========================================
Iteration 9 is in valid list - performing full update
Finding noisy pairs for iteration 9...
2025-04-14 07:17:50,047 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.40it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.74it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
2025-04-14 07:17:50,589 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:17:50,733 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:17:50,747 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 07:17:50,747 - __main__ - INFO - Loading dataset
2025-04-14 07:17:51,491 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 07:18:12,881 - __main__ - INFO - Processing dataset
2025-04-14 07:18:13,124 - __main__ - INFO - Searching for noisy pairs starting from index 1747
2025-04-14 07:18:23,400 - __main__ - INFO - Processed 53 samples without finding a noisy pair
2025-04-14 07:18:41,592 - __main__ - INFO - Noisy pair found at index 1892: chosen_logps=-58.0000, rejected_logps=-59.7500
2025-04-14 07:18:41,593 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 07:18:41,660 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_9
2025-04-14 07:18:41,661 - __main__ - INFO - New dataset offset: 1893
Updated dataset offset to: 1893 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 9
Worker node 1 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 07:18:57,368 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 07:18:57,372 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 07:18:57,548 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  6.91it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.31it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.19it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.27it/s]2025-04-14 07:18:57,922 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:18:57,927 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
2025-04-14 07:18:58,071 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:18:58,076 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:18:58,086 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 07:18:58,086 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:18:58,091 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 07:18:58,091 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:18:58,103 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:18:58,247 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:18:58,260 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 07:18:58,260 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:19:14,643 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 07:19:14,782 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 07:19:14,818 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 07:19:24,718 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 07:19:24,832 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 07:19:24,911 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 07:19:28,550 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 07:19:28,640 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 07:19:29,080 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 07:19:31,640 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 07:19:31,991 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 07:19:32,081 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 07:19:32,448 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 07:19:32,966 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 07:19:33,460 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 07:19:35,933 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 07:19:36,130 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 07:19:36,187 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 07:19:36,459 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 07:19:36,869 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 07:19:37,318 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 07:19:39,549 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 07:19:40,468 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 07:19:40,581 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 07:19:40,736 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 07:19:40,800 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 07:19:41,262 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 07:19:43,581 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 07:19:44,422 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 07:19:44,425 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 07:19:44,864 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 07:19:44,945 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 07:19:45,228 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 07:19:47,688 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 07:19:48,165 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 07:19:48,339 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 07:19:48,685 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 07:19:49,111 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 07:19:49,181 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 07:19:52,162 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 07:19:52,308 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 07:19:52,705 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 07:19:52,907 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 07:19:53,196 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 07:19:53,239 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 07:19:55,561 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 07:19:56,566 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 07:19:56,849 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 07:19:57,279 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 07:19:57,283 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 07:19:57,375 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 07:19:57,379 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 07:19:58,074 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 07:19:58,079 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 07:20:00,198 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 07:20:00,755 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 07:20:00,929 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 07:20:04,515 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 07:20:04,714 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 07:20:04,902 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 07:20:05,027 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 07:20:06,055 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 07:20:06,391 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 07:21:29,133 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 07:21:29,908 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 07:21:32,520 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 07:21:33,526 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 07:21:34,598 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 07:21:35,054 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 07:21:35,400 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 07:21:37,450 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 07:21:38,715 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 07:21:39,839 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 07:21:41,900 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 07:21:45,164 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 07:21:45,524 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 07:21:46,227 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 07:21:46,400 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 07:21:47,274 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 07:21:49,590 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 07:21:50,546 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 07:21:51,828 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 07:21:53,338 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 07:21:55,703 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 07:21:56,858 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 07:21:57,788 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 07:21:58,457 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 07:22:00,379 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 07:22:00,733 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 07:22:01,634 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 07:22:02,438 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 07:22:06,164 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 07:22:06,298 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
[rank7]:[E414 07:23:55.190589269 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bc4fdace446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7bc4b2dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7bc4b2dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7bc4b2dcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7bc4b2dd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bc4b2dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7bc4fdc355c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7bc4fe694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7bc4fe726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:23:55,504 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_0.npy
2025-04-14 07:23:55,504 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 112 negative comparisons
[E414 07:23:57.822647430 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74b556638446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74b50b9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74b50b9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x74b50b9cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x74b50b9d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74b50b9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74b55679f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74b557094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74b557126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:23:57.844224727 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c1d2d9e3446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c1ce2dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c1ce2dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7c1ce2dcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7c1ce2dd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c1ce2dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c1d2db4a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c1d2e494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c1d2e526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:23:57,244 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_1.npy
2025-04-14 07:23:57,245 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 111 negative comparisons
2025-04-14 07:23:58,520 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 07:24:00,077 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 07:24:02,480 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_2.npy
2025-04-14 07:24:02,480 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 104 negative comparisons
Waiting for worker nodes to complete gradient computation...
2025-04-14 07:24:05,421 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 9
Worker node 2 successfully completed gradient computation for iteration 9
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 07:24:22,017 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 07:24:22,562 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:24:22,704 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:24:22,716 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 07:24:39,247 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 07:24:42,175 - __main__ - INFO - Loaded gradients from node 0: 112 negative comparisons
2025-04-14 07:24:42,409 - __main__ - INFO - Loaded gradients from node 1: 111 negative comparisons
2025-04-14 07:24:42,644 - __main__ - INFO - Loaded gradients from node 2: 104 negative comparisons
2025-04-14 07:24:42,804 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 07:24:42,995 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 07:24:43,185 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119126609
2025-04-14 07:24:43,374 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107348179
2025-04-14 07:24:43,563 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84833222
2025-04-14 07:24:43,752 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47188465
2025-04-14 07:24:43,940 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252596
2025-04-14 07:24:44,129 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8807264
2025-04-14 07:24:44,318 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551589
2025-04-14 07:24:44,507 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555938
2025-04-14 07:24:44,850 - __main__ - INFO - Non-zero entries after thresholding: 47188465
2025-04-14 07:24:44,851 - __main__ - INFO - Using relaxed comparison (count_negative: 327)
2025-04-14 07:24:44,851 - __main__ - INFO - Testing stepsize: 0
2025-04-14 07:24:45,598 - __main__ - INFO -   chosen_logps: -57.56463, rejected_logps: -59.94021
2025-04-14 07:24:45,598 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 07:24:45,977 - __main__ - INFO -   chosen_logps: -57.35117, rejected_logps: -60.03181
2025-04-14 07:24:45,977 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 07:24:46,356 - __main__ - INFO -   chosen_logps: -56.65231, rejected_logps: -60.46807
2025-04-14 07:24:46,356 - __main__ - INFO - Testing stepsize: 1
2025-04-14 07:24:46,738 - __main__ - INFO -   chosen_logps: -55.80112, rejected_logps: -60.93034
2025-04-14 07:24:46,738 - __main__ - INFO - Testing stepsize: 2
2025-04-14 07:24:47,126 - __main__ - INFO -   chosen_logps: -53.75995, rejected_logps: -61.55788
2025-04-14 07:24:47,126 - __main__ - INFO - Testing stepsize: 5
2025-04-14 07:24:47,513 - __main__ - INFO -   chosen_logps: -49.38896, rejected_logps: -64.01889
2025-04-14 07:24:47,514 - __main__ - INFO - Testing stepsize: 10
2025-04-14 07:24:47,903 - __main__ - INFO -   chosen_logps: -41.91108, rejected_logps: -68.49386
2025-04-14 07:24:47,903 - __main__ - INFO - Testing stepsize: 15
2025-04-14 07:24:48,293 - __main__ - INFO -   chosen_logps: -36.84736, rejected_logps: -72.75006
2025-04-14 07:24:49,056 - __main__ - INFO - Update scale: 0.006358333333333334
2025-04-14 07:24:49,057 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 07:24:49,137 - __main__ - INFO - Model weights updated successfully
2025-04-14 07:24:49,982 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:24:50,019 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:24:50,906 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 07:24:50,938 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 07:24:50,938 - __main__ - INFO - Skipping full model checkpoint for iteration 9
2025-04-14 07:24:50,939 - __main__ - INFO - Iteration 9 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 9
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 9
Iteration 9 completed successfully
Cleaning up iteration files...
Multi-node training completed successfully!
Copying scripts to worker nodes...
Starting with dataset offset: 0
===========================================
Processing iteration 0
===========================================
Iteration 0 is in valid list - performing full update
Finding noisy pairs for iteration 0...
2025-04-14 07:25:15,286 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 07:25:15,835 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:25:15,980 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:25:15,995 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 07:25:15,995 - __main__ - INFO - Loading dataset
2025-04-14 07:25:17,030 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 07:25:38,538 - __main__ - INFO - Processing dataset
2025-04-14 07:25:38,780 - __main__ - INFO - Searching for noisy pairs starting from index 0
2025-04-14 07:25:39,409 - __main__ - INFO - Processed 0 samples without finding a noisy pair
2025-04-14 07:25:59,816 - __main__ - INFO - Processed 100 samples without finding a noisy pair
2025-04-14 07:26:17,950 - __main__ - INFO - Processed 200 samples without finding a noisy pair
2025-04-14 07:26:36,980 - __main__ - INFO - Processed 300 samples without finding a noisy pair
2025-04-14 07:26:43,703 - __main__ - INFO - Noisy pair found at index 337: chosen_logps=-90.0000, rejected_logps=-92.0000
2025-04-14 07:26:43,704 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 07:26:43,771 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_0
2025-04-14 07:26:43,772 - __main__ - INFO - New dataset offset: 338
Updated dataset offset to: 338 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 0
Worker node 2 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 07:26:59,236 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 07:26:59,416 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 07:26:59,441 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.39it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.65it/s]2025-04-14 07:26:59,795 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 07:26:59,939 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:26:59,954 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 07:26:59,954 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:26:59,963 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:26:59,986 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:27:00,104 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:27:00,117 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 07:27:00,117 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:27:00,139 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:27:00,154 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 07:27:00,154 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:27:16,563 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 07:27:16,692 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 07:27:16,698 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 07:27:26,739 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 07:27:26,738 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 07:27:27,125 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 07:27:30,497 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 07:27:30,497 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 07:27:31,400 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 07:27:33,753 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 07:27:33,927 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 07:27:33,955 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 07:27:34,338 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 07:27:34,386 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 07:27:35,444 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 07:27:37,556 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 07:27:37,717 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 07:27:38,236 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 07:27:38,401 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 07:27:38,463 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 07:27:39,268 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 07:27:41,323 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 07:27:41,558 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 07:27:42,141 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 07:27:42,253 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 07:27:42,670 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 07:27:43,111 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 07:27:45,435 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 07:27:45,457 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 07:27:46,162 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 07:27:46,392 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 07:27:46,740 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 07:27:47,548 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 07:27:49,288 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 07:27:49,472 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 07:27:50,127 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 07:27:50,186 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 07:27:50,282 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 07:27:51,677 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 07:27:53,215 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 07:27:53,575 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 07:27:54,357 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 07:27:54,675 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 07:27:54,992 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 07:27:55,979 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 07:27:57,471 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 07:27:57,555 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 07:27:58,644 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 07:27:58,649 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 07:27:59,211 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 07:27:59,313 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 07:27:59,322 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 07:28:00,645 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 07:28:00,648 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 07:28:01,653 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 07:28:01,703 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 07:28:03,365 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 07:28:05,880 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 07:28:05,889 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 07:28:06,438 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 07:28:06,466 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 07:28:08,088 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 07:28:08,540 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 07:29:24,574 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 07:29:24,577 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 07:29:26,979 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 07:29:27,068 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 07:29:29,527 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 07:29:31,761 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 07:29:32,201 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 07:29:32,938 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 07:29:34,997 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 07:29:37,434 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 07:29:37,825 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 07:29:40,125 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 07:29:40,818 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 07:29:41,058 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 07:29:41,431 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 07:29:43,597 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 07:29:45,070 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 07:29:46,646 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 07:29:48,744 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 07:29:49,052 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 07:29:49,108 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 07:29:51,137 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 07:29:52,785 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 07:29:53,229 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 07:29:55,029 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 07:29:55,951 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 07:29:56,235 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 07:29:56,763 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 07:29:57,427 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 07:30:01,006 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 07:31:43,772 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_0.npy
2025-04-14 07:31:43,772 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 120 negative comparisons
2025-04-14 07:31:45,278 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_1.npy
2025-04-14 07:31:45,278 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 118 negative comparisons
[E414 07:31:45.163295143 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75eb80fdc446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75eb363cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75eb363cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x75eb363cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x75eb363d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75eb363d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x75eb811435c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x75eb81a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x75eb81b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:31:46,985 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 07:31:48,515 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 07:31:52.304839641 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7dd2073fd446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7dd1bc7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7dd1bc7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7dd1bc7cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7dd1bc7d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7dd1bc7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7dd2075645c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7dd208094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7dd208126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:31:52,604 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_2.npy
2025-04-14 07:31:52,604 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 119 negative comparisons
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 0
2025-04-14 07:31:55,401 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 0
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 07:32:11,972 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 07:32:12,521 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:32:12,662 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:32:12,675 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 07:32:29,140 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 07:32:32,030 - __main__ - INFO - Loaded gradients from node 0: 120 negative comparisons
2025-04-14 07:32:32,245 - __main__ - INFO - Loaded gradients from node 1: 118 negative comparisons
2025-04-14 07:32:32,479 - __main__ - INFO - Loaded gradients from node 2: 119 negative comparisons
2025-04-14 07:32:32,639 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 07:32:32,829 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 07:32:33,019 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119132098
2025-04-14 07:32:33,209 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107350541
2025-04-14 07:32:33,399 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84827941
2025-04-14 07:32:33,589 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47187485
2025-04-14 07:32:33,779 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22253771
2025-04-14 07:32:33,969 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802722
2025-04-14 07:32:34,159 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551854
2025-04-14 07:32:34,348 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 556428
2025-04-14 07:32:34,694 - __main__ - INFO - Non-zero entries after thresholding: 47187485
2025-04-14 07:32:34,694 - __main__ - INFO - Using relaxed comparison (count_negative: 357)
2025-04-14 07:32:34,694 - __main__ - INFO - Testing stepsize: 0
2025-04-14 07:32:35,329 - __main__ - INFO -   chosen_logps: -89.95925, rejected_logps: -91.92722
2025-04-14 07:32:35,329 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 07:32:35,596 - __main__ - INFO -   chosen_logps: -89.75139, rejected_logps: -92.04920
2025-04-14 07:32:35,596 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 07:32:35,862 - __main__ - INFO -   chosen_logps: -88.99260, rejected_logps: -92.94411
2025-04-14 07:32:35,862 - __main__ - INFO - Testing stepsize: 1
2025-04-14 07:32:36,135 - __main__ - INFO -   chosen_logps: -88.43790, rejected_logps: -93.56354
2025-04-14 07:32:36,135 - __main__ - INFO - Testing stepsize: 2
2025-04-14 07:32:36,410 - __main__ - INFO -   chosen_logps: -86.61703, rejected_logps: -94.92857
2025-04-14 07:32:36,410 - __main__ - INFO - Testing stepsize: 5
2025-04-14 07:32:36,685 - __main__ - INFO -   chosen_logps: -81.90250, rejected_logps: -99.11179
2025-04-14 07:32:36,685 - __main__ - INFO - Testing stepsize: 10
2025-04-14 07:32:36,962 - __main__ - INFO -   chosen_logps: -74.24609, rejected_logps: -106.58284
2025-04-14 07:32:36,962 - __main__ - INFO - Testing stepsize: 15
2025-04-14 07:32:37,242 - __main__ - INFO -   chosen_logps: -67.59596, rejected_logps: -114.05195
2025-04-14 07:32:37,843 - __main__ - INFO - Update scale: 0.006941666666666667
2025-04-14 07:32:37,844 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 07:32:37,925 - __main__ - INFO - Model weights updated successfully
2025-04-14 07:32:38,768 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:32:38,802 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:32:39,691 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 07:32:39,723 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 07:32:39,723 - __main__ - INFO - Skipping full model checkpoint for iteration 0
2025-04-14 07:32:39,723 - __main__ - INFO - Iteration 0 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 0
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 0
Iteration 0 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 1
===========================================
Iteration 1 is in valid list - performing full update
Finding noisy pairs for iteration 1...
2025-04-14 07:33:00,589 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 07:33:01,135 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:33:01,281 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:33:01,295 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 07:33:01,295 - __main__ - INFO - Loading dataset
2025-04-14 07:33:02,149 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 07:33:23,498 - __main__ - INFO - Processing dataset
2025-04-14 07:33:23,744 - __main__ - INFO - Searching for noisy pairs starting from index 338
2025-04-14 07:33:30,474 - __main__ - INFO - Noisy pair found at index 369: chosen_logps=-153.0000, rejected_logps=-154.0000
2025-04-14 07:33:30,475 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 07:33:30,542 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_1
2025-04-14 07:33:30,542 - __main__ - INFO - New dataset offset: 370
Updated dataset offset to: 370 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 1
Worker node 1 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 07:33:45,016 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]2025-04-14 07:33:45,309 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 07:33:45,370 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]2025-04-14 07:33:45,574 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.34it/s]2025-04-14 07:33:45,718 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:33:45,733 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 07:33:45,733 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 07:33:45,853 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:33:45,926 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:33:46,006 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:33:46,021 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 07:33:46,021 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:33:46,067 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:33:46,079 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 07:33:46,079 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:34:02,263 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 07:34:02,595 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 07:34:02,621 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 07:34:12,608 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 07:34:12,606 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 07:34:12,994 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 07:34:16,301 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 07:34:16,401 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 07:34:16,855 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 07:34:19,509 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 07:34:19,730 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 07:34:19,789 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 07:34:20,272 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 07:34:20,459 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 07:34:20,791 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 07:34:23,444 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 07:34:23,672 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 07:34:23,925 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 07:34:24,441 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 07:34:24,512 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 07:34:24,631 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 07:34:27,221 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 07:34:27,735 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 07:34:28,069 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 07:34:28,527 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 07:34:28,579 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 07:34:28,810 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 07:34:31,584 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 07:34:32,209 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 07:34:32,226 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 07:34:32,736 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 07:34:32,769 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 07:34:32,961 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 07:34:35,549 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 07:34:36,009 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 07:34:36,553 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 07:34:36,771 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 07:34:36,834 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 07:34:36,940 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 07:34:39,656 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 07:34:39,941 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 07:34:40,139 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 07:34:40,922 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 07:34:41,055 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 07:34:41,154 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 07:34:43,766 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 07:34:43,981 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 07:34:44,232 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 07:34:44,977 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 07:34:44,978 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 07:34:45,139 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 07:34:45,143 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 07:34:45,388 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 07:34:45,389 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 07:34:48,063 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 07:34:48,397 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 07:34:48,631 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 07:34:52,194 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 07:34:52,274 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 07:34:52,557 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 07:34:52,628 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 07:34:52,810 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 07:34:52,866 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 07:36:09,589 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 07:36:10,078 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 07:36:12,912 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 07:36:13,027 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 07:36:13,725 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 07:36:15,009 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 07:36:17,466 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 07:36:17,988 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 07:36:22,089 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 07:36:22,604 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 07:36:24,750 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 07:36:25,598 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 07:36:26,520 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 07:36:28,466 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 07:36:28,900 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 07:36:29,267 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 07:36:29,340 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 07:36:31,090 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 07:36:32,678 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 07:36:32,994 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 07:36:33,247 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 07:36:37,687 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 07:36:37,718 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 07:36:38,150 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 07:36:40,881 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 07:36:41,100 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 07:36:41,830 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 07:36:41,863 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 07:36:46,476 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 07:36:46,792 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 07:38:27.261866043 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d81e9587446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7d819e9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7d819e9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7d819e9cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7d819e9d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d819e9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7d81e96ee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7d81ea094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7d81ea126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:38:27.281485696 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b8d0ccef446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b8cc1fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b8cc1fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7b8cc1fcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7b8cc1fd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b8cc1fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b8d0ce565c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b8d0d894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b8d0d926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:38:27.284928064 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a0c48510446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7a0bfd7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7a0bfd7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7a0bfd7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7a0bfd7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a0bfd7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7a0c486775c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7a0c49094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7a0c49126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:38:27,588 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_0.npy
2025-04-14 07:38:27,588 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 123 negative comparisons
2025-04-14 07:38:29,044 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_1.npy
2025-04-14 07:38:29,044 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 126 negative comparisons
2025-04-14 07:38:30,525 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 07:38:32,180 - __main__ - INFO - Node 1: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
[E414 07:38:37.268077325 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79a7b0e5c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79a7661cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79a7661cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x79a7661cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x79a7661d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79a7661d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79a7b0fc35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x79a7b1a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x79a7b1b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:38:37.278723256 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77490718f446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7748bc5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7748bc5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7748bc5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7748bc5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7748bc5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7749072f65c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x774907e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x774907f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:38:37,624 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_2.npy
2025-04-14 07:38:37,624 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 129 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 1
2025-04-14 07:38:40,326 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 1
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 07:38:56,382 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 07:38:56,927 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:38:57,068 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:38:57,081 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 07:39:13,533 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 07:39:16,473 - __main__ - INFO - Loaded gradients from node 0: 123 negative comparisons
2025-04-14 07:39:16,706 - __main__ - INFO - Loaded gradients from node 1: 126 negative comparisons
2025-04-14 07:39:16,939 - __main__ - INFO - Loaded gradients from node 2: 129 negative comparisons
2025-04-14 07:39:17,098 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 07:39:17,288 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 07:39:17,478 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119128106
2025-04-14 07:39:17,668 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107341867
2025-04-14 07:39:17,859 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84829414
2025-04-14 07:39:18,049 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47190454
2025-04-14 07:39:18,238 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255789
2025-04-14 07:39:18,429 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8805811
2025-04-14 07:39:18,619 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552182
2025-04-14 07:39:18,809 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553519
2025-04-14 07:39:19,154 - __main__ - INFO - Non-zero entries after thresholding: 47190454
2025-04-14 07:39:19,154 - __main__ - INFO - Testing stepsize: 0
2025-04-14 07:39:19,782 - __main__ - INFO -   chosen_logps: -152.85397, rejected_logps: -153.87032
2025-04-14 07:39:19,782 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 07:39:20,043 - __main__ - INFO -   chosen_logps: -152.78343, rejected_logps: -153.87819
2025-04-14 07:39:20,044 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 07:39:20,308 - __main__ - INFO -   chosen_logps: -152.13455, rejected_logps: -154.29376
2025-04-14 07:39:20,308 - __main__ - INFO - Testing stepsize: 1
2025-04-14 07:39:20,575 - __main__ - INFO -   chosen_logps: -151.33534, rejected_logps: -154.49554
2025-04-14 07:39:20,575 - __main__ - INFO - Testing stepsize: 2
2025-04-14 07:39:20,845 - __main__ - INFO -   chosen_logps: -150.48071, rejected_logps: -155.56013
2025-04-14 07:39:20,845 - __main__ - INFO - Testing stepsize: 5
2025-04-14 07:39:21,114 - __main__ - INFO -   chosen_logps: -147.12132, rejected_logps: -158.23526
2025-04-14 07:39:21,114 - __main__ - INFO - Testing stepsize: 10
2025-04-14 07:39:21,387 - __main__ - INFO -   chosen_logps: -142.45206, rejected_logps: -162.39095
2025-04-14 07:39:21,387 - __main__ - INFO - Testing stepsize: 15
2025-04-14 07:39:21,661 - __main__ - INFO -   chosen_logps: -138.50185, rejected_logps: -166.80692
2025-04-14 07:39:22,252 - __main__ - INFO - Update scale: 0.007350000000000001
2025-04-14 07:39:22,335 - __main__ - INFO - Model weights updated successfully
2025-04-14 07:39:23,166 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:39:23,200 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:39:24,078 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 07:39:24,114 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 07:39:24,114 - __main__ - INFO - Skipping full model checkpoint for iteration 1
2025-04-14 07:39:24,114 - __main__ - INFO - Iteration 1 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 1
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 1
Iteration 1 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 2
===========================================
Iteration 2 is in valid list - performing full update
Finding noisy pairs for iteration 2...
2025-04-14 07:39:44,826 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 07:39:45,375 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:39:45,519 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:39:45,533 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 07:39:45,533 - __main__ - INFO - Loading dataset
2025-04-14 07:39:46,272 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 07:40:07,689 - __main__ - INFO - Processing dataset
2025-04-14 07:40:07,930 - __main__ - INFO - Searching for noisy pairs starting from index 370
2025-04-14 07:40:14,103 - __main__ - INFO - Processed 30 samples without finding a noisy pair
2025-04-14 07:40:34,637 - __main__ - INFO - Processed 130 samples without finding a noisy pair
2025-04-14 07:40:56,124 - __main__ - INFO - Processed 230 samples without finding a noisy pair
2025-04-14 07:41:15,447 - __main__ - INFO - Processed 330 samples without finding a noisy pair
2025-04-14 07:41:35,688 - __main__ - INFO - Processed 430 samples without finding a noisy pair
2025-04-14 07:41:55,337 - __main__ - INFO - Processed 530 samples without finding a noisy pair
2025-04-14 07:42:13,977 - __main__ - INFO - Processed 630 samples without finding a noisy pair
2025-04-14 07:42:19,457 - __main__ - INFO - Noisy pair found at index 1030: chosen_logps=-280.0000, rejected_logps=-282.0000
2025-04-14 07:42:19,458 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 07:42:19,525 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_2
2025-04-14 07:42:19,525 - __main__ - INFO - New dataset offset: 1031
Updated dataset offset to: 1031 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 2
Worker node 1 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 07:42:34,991 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 07:42:35,154 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 07:42:35,236 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.73it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.67it/s]2025-04-14 07:42:35,533 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 07:42:35,675 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:42:35,689 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 07:42:35,689 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:42:35,698 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 07:42:35,792 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:42:35,851 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:42:35,866 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 07:42:35,866 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:42:35,932 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:42:35,944 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 07:42:35,944 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:42:52,215 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 07:42:52,452 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 07:42:52,466 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 07:43:02,509 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 07:43:02,549 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 07:43:02,871 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 07:43:06,317 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 07:43:06,401 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 07:43:06,756 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 07:43:09,462 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 07:43:09,493 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 07:43:09,792 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 07:43:10,376 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 07:43:10,497 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 07:43:10,713 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 07:43:13,544 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 07:43:13,695 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 07:43:13,900 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 07:43:14,153 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 07:43:14,536 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 07:43:14,598 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 07:43:17,427 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 07:43:17,714 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 07:43:17,851 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 07:43:18,229 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 07:43:18,374 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 07:43:18,432 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 07:43:21,237 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 07:43:21,818 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 07:43:21,826 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 07:43:22,225 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 07:43:22,462 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 07:43:22,735 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 07:43:25,219 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 07:43:25,332 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 07:43:25,831 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 07:43:26,218 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 07:43:26,794 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 07:43:26,981 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 07:43:29,344 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 07:43:29,695 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 07:43:30,008 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 07:43:30,656 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 07:43:30,948 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 07:43:31,648 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 07:43:33,883 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 07:43:34,119 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 07:43:34,162 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 07:43:34,919 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 07:43:34,925 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 07:43:35,139 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 07:43:35,145 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 07:43:36,104 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 07:43:36,110 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 07:43:38,283 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 07:43:38,377 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 07:43:38,584 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 07:43:42,015 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 07:43:42,416 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 07:43:42,517 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 07:43:43,022 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 07:43:43,217 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 07:43:43,269 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 07:44:58,011 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 07:44:59,049 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 07:45:01,780 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 07:45:03,378 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 07:45:04,047 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 07:45:06,462 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 07:45:06,489 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 07:45:06,865 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 07:45:08,264 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 07:45:11,022 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 07:45:11,811 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 07:45:13,263 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 07:45:14,794 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 07:45:15,327 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 07:45:16,108 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 07:45:17,879 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 07:45:19,440 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 07:45:21,932 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 07:45:21,968 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 07:45:22,863 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 07:45:23,534 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 07:45:26,253 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 07:45:27,760 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 07:45:29,809 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 07:45:30,068 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 07:45:31,152 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 07:45:32,483 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 07:45:33,928 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 07:45:34,057 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 07:45:34,334 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 07:47:18.322909930 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7abb4aefd446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7abb001cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7abb001cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7abb001cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7abb001d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7abb001d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7abb4b0645c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7abb4ba94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7abb4bb26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:47:18.320145670 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x729e137ea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x729dc8bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x729dc8bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x729dc8bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x729dc8bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x729dc8bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x729e139515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x729e14294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x729e14326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:47:18,561 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_0.npy
2025-04-14 07:47:18,561 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 128 negative comparisons
2025-04-14 07:47:21,371 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 07:47:21,684 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_1.npy
2025-04-14 07:47:21,684 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 102 negative comparisons
[E414 07:47:21.561141163 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7cde8294d446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7cde37dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7cde37dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7cde37dcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7cde37dd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7cde37dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7cde82ab45c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7cde83494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7cde83526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:47:21.569475030 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78c10476c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78c0b9fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78c0b9fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x78c0b9fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x78c0b9fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78c0b9fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78c104c9d5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78c105694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78c105726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:47:24.893335157 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7594b8857446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75946dbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75946dbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x75946dbcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x75946dbd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75946dbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7594b89be5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7594b9494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7594b9526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:47:24.900856895 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71094b417446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7109007cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7109007cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7109007cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7109007d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7109007d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x71094b57e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x71094c094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x71094c126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:47:24,224 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_2.npy
2025-04-14 07:47:24,225 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 118 negative comparisons
2025-04-14 07:47:24,852 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 07:47:27,282 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 2
Worker node 2 successfully completed gradient computation for iteration 2
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 07:47:45,351 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 07:47:45,896 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:47:46,035 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:47:46,048 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 07:48:02,552 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 07:48:05,489 - __main__ - INFO - Loaded gradients from node 0: 128 negative comparisons
2025-04-14 07:48:05,723 - __main__ - INFO - Loaded gradients from node 1: 102 negative comparisons
2025-04-14 07:48:05,958 - __main__ - INFO - Loaded gradients from node 2: 118 negative comparisons
2025-04-14 07:48:06,119 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 07:48:06,311 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 07:48:06,503 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133461
2025-04-14 07:48:06,695 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352928
2025-04-14 07:48:06,885 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84832657
2025-04-14 07:48:07,075 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47192480
2025-04-14 07:48:07,267 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22251494
2025-04-14 07:48:07,457 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8807400
2025-04-14 07:48:07,648 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1553215
2025-04-14 07:48:07,839 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554930
2025-04-14 07:48:08,187 - __main__ - INFO - Non-zero entries after thresholding: 47192480
2025-04-14 07:48:08,187 - __main__ - INFO - Using relaxed comparison (count_negative: 348)
2025-04-14 07:48:08,187 - __main__ - INFO - Testing stepsize: 0
2025-04-14 07:48:08,806 - __main__ - INFO -   chosen_logps: -279.80750, rejected_logps: -281.87396
2025-04-14 07:48:08,806 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 07:48:09,056 - __main__ - INFO -   chosen_logps: -279.28760, rejected_logps: -282.07214
2025-04-14 07:48:09,056 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 07:48:09,307 - __main__ - INFO -   chosen_logps: -277.61279, rejected_logps: -282.97418
2025-04-14 07:48:09,307 - __main__ - INFO - Testing stepsize: 1
2025-04-14 07:48:09,561 - __main__ - INFO -   chosen_logps: -275.63446, rejected_logps: -284.19138
2025-04-14 07:48:09,561 - __main__ - INFO - Testing stepsize: 2
2025-04-14 07:48:09,818 - __main__ - INFO -   chosen_logps: -271.62848, rejected_logps: -286.18427
2025-04-14 07:48:09,818 - __main__ - INFO - Testing stepsize: 5
2025-04-14 07:48:10,077 - __main__ - INFO -   chosen_logps: -260.99567, rejected_logps: -293.73703
2025-04-14 07:48:10,077 - __main__ - INFO - Testing stepsize: 10
2025-04-14 07:48:10,337 - __main__ - INFO -   chosen_logps: -244.08296, rejected_logps: -305.31744
2025-04-14 07:48:10,337 - __main__ - INFO - Testing stepsize: 15
2025-04-14 07:48:10,598 - __main__ - INFO -   chosen_logps: -229.27336, rejected_logps: -318.48273
2025-04-14 07:48:11,174 - __main__ - INFO - Update scale: 0.006766666666666667
2025-04-14 07:48:11,175 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 07:48:11,254 - __main__ - INFO - Model weights updated successfully
2025-04-14 07:48:12,366 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:48:12,403 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:48:13,577 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 07:48:13,613 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 07:48:13,613 - __main__ - INFO - Skipping full model checkpoint for iteration 2
2025-04-14 07:48:13,613 - __main__ - INFO - Iteration 2 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 2
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 2
Iteration 2 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 3
===========================================
Iteration 3 is in valid list - performing full update
Finding noisy pairs for iteration 3...
2025-04-14 07:48:34,793 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 07:48:35,342 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:48:35,485 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:48:35,500 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 07:48:35,500 - __main__ - INFO - Loading dataset
2025-04-14 07:48:37,047 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 07:48:58,538 - __main__ - INFO - Processing dataset
2025-04-14 07:48:58,785 - __main__ - INFO - Searching for noisy pairs starting from index 1031
2025-04-14 07:49:13,726 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 07:49:27,322 - __main__ - INFO - Noisy pair found at index 1163: chosen_logps=-147.0000, rejected_logps=-149.0000
2025-04-14 07:49:27,323 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 07:49:27,391 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_3
2025-04-14 07:49:27,392 - __main__ - INFO - New dataset offset: 1164
Updated dataset offset to: 1164 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 3
Worker node 2 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 07:49:42,918 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]2025-04-14 07:49:43,197 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 07:49:43,361 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 07:49:43,467 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 07:49:43,611 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:49:43,626 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 07:49:43,626 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.24it/s]2025-04-14 07:49:43,742 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
2025-04-14 07:49:43,895 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:49:43,909 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 07:49:43,909 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:49:43,916 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:49:44,060 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:49:44,073 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 07:49:44,073 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:50:00,230 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 07:50:00,473 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 07:50:00,607 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 07:50:10,293 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 07:50:10,504 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 07:50:10,545 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 07:50:14,066 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 07:50:14,382 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 07:50:14,389 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 07:50:17,414 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 07:50:17,489 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 07:50:17,513 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 07:50:18,010 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 07:50:18,238 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 07:50:18,297 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 07:50:21,617 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 07:50:21,623 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 07:50:21,970 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 07:50:22,170 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 07:50:22,239 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 07:50:22,530 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 07:50:25,592 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 07:50:25,661 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 07:50:25,693 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 07:50:25,839 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 07:50:26,047 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 07:50:26,452 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 07:50:29,304 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 07:50:29,608 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 07:50:29,689 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 07:50:30,176 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 07:50:30,334 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 07:50:30,700 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 07:50:33,448 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 07:50:33,641 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 07:50:33,643 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 07:50:33,653 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 07:50:34,160 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 07:50:34,749 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 07:50:37,080 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 07:50:37,350 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 07:50:37,734 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 07:50:38,124 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 07:50:38,289 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 07:50:38,719 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 07:50:41,165 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 07:50:41,754 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 07:50:41,763 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 07:50:42,035 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 07:50:42,411 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 07:50:42,574 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 07:50:42,580 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 07:50:42,987 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 07:50:42,995 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 07:50:45,151 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 07:50:45,839 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 07:50:46,318 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 07:50:49,199 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 07:50:49,429 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 07:50:50,031 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 07:50:50,059 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 07:50:50,340 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 07:50:50,352 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 07:52:13,263 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 07:52:15,461 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 07:52:19,320 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 07:52:20,649 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 07:52:21,847 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 07:52:21,954 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 07:52:22,571 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 07:52:22,999 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 07:52:26,001 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 07:52:27,200 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 07:52:30,590 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 07:52:31,579 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 07:52:31,625 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 07:52:31,777 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 07:52:32,615 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 07:52:34,902 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 07:52:35,399 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 07:52:35,578 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 07:52:37,148 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 07:52:39,326 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 07:52:41,835 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 07:52:43,245 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 07:52:43,781 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 07:52:43,981 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 07:52:45,731 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 07:52:46,080 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 07:52:47,438 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 07:52:47,683 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 07:52:49,696 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 07:52:51,265 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
[E414 07:54:40.373348194 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x772e577fd446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x772e0cbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x772e0cbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x772e0cbcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x772e0cbd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x772e0cbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x772e579645c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x772e58294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x772e58326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:54:40.374639004 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73b973b93446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x73b928fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x73b928fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x73b928fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x73b928fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73b928fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x73b973cee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x73b974694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x73b974726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:54:40,627 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_0.npy
2025-04-14 07:54:40,627 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 157 negative comparisons
2025-04-14 07:54:41,726 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_1.npy
2025-04-14 07:54:41,726 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 163 negative comparisons
[E414 07:54:41.613192826 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7dd1b96fb446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7dd16e9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7dd16e9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7dd16e9cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7dd16e9d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7dd16e9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7dd1b98625c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7dd1ba294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7dd1ba326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:54:41.622370009 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fa5f44ea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7fa5a97cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7fa5a97cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7fa5a97cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7fa5a97d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fa5a97d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7fa5f46515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7fa5f5094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7fa5f5126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:54:43,526 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 07:54:44,873 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 07:54:47.495626542 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x715477104446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71542c3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71542c3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x71542c3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x71542c3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71542c3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x71547726b5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x715477c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x715477d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 07:54:47.491298935 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x784a8c5fb446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x784a419cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x784a419cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x784a419cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x784a419d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x784a419d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x784a8c7625c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x784a8d294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x784a8d326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 07:54:47,795 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_2.npy
2025-04-14 07:54:47,795 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 150 negative comparisons
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 3
2025-04-14 07:54:50,730 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 3
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 07:55:09,239 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 07:55:09,787 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:55:09,929 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:55:09,941 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 07:55:26,587 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 07:55:29,552 - __main__ - INFO - Loaded gradients from node 0: 157 negative comparisons
2025-04-14 07:55:29,790 - __main__ - INFO - Loaded gradients from node 1: 163 negative comparisons
2025-04-14 07:55:30,029 - __main__ - INFO - Loaded gradients from node 2: 150 negative comparisons
2025-04-14 07:55:30,193 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 07:55:30,388 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 07:55:30,584 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119136006
2025-04-14 07:55:30,780 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107351404
2025-04-14 07:55:30,976 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84837264
2025-04-14 07:55:31,172 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47197846
2025-04-14 07:55:31,368 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255927
2025-04-14 07:55:31,564 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804005
2025-04-14 07:55:31,761 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551447
2025-04-14 07:55:31,957 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553943
2025-04-14 07:55:32,312 - __main__ - INFO - Non-zero entries after thresholding: 47197846
2025-04-14 07:55:32,312 - __main__ - INFO - Testing stepsize: 0
2025-04-14 07:55:33,066 - __main__ - INFO -   chosen_logps: -147.82407, rejected_logps: -147.74109
2025-04-14 07:55:33,066 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 07:55:33,453 - __main__ - INFO -   chosen_logps: -147.41325, rejected_logps: -147.88173
2025-04-14 07:55:33,453 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 07:55:33,840 - __main__ - INFO -   chosen_logps: -145.86815, rejected_logps: -148.56923
2025-04-14 07:55:33,840 - __main__ - INFO - Testing stepsize: 1
2025-04-14 07:55:34,230 - __main__ - INFO -   chosen_logps: -144.32404, rejected_logps: -149.02112
2025-04-14 07:55:34,230 - __main__ - INFO - Testing stepsize: 2
2025-04-14 07:55:34,623 - __main__ - INFO -   chosen_logps: -141.48497, rejected_logps: -150.15279
2025-04-14 07:55:34,623 - __main__ - INFO - Testing stepsize: 5
2025-04-14 07:55:35,016 - __main__ - INFO -   chosen_logps: -132.96924, rejected_logps: -153.75412
2025-04-14 07:55:35,016 - __main__ - INFO - Testing stepsize: 10
2025-04-14 07:55:35,413 - __main__ - INFO -   chosen_logps: -120.34290, rejected_logps: -159.70667
2025-04-14 07:55:35,413 - __main__ - INFO - Testing stepsize: 15
2025-04-14 07:55:35,810 - __main__ - INFO -   chosen_logps: -110.77524, rejected_logps: -165.77724
2025-04-14 07:55:36,581 - __main__ - INFO - Update scale: 0.00913888888888889
2025-04-14 07:55:36,664 - __main__ - INFO - Model weights updated successfully
2025-04-14 07:55:37,504 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:55:37,538 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:55:38,437 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 07:55:38,472 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 07:55:38,472 - __main__ - INFO - Skipping full model checkpoint for iteration 3
2025-04-14 07:55:38,472 - __main__ - INFO - Iteration 3 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 3
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 3
Iteration 3 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 4
===========================================
Iteration 4 is in valid list - performing full update
Finding noisy pairs for iteration 4...
2025-04-14 07:55:59,741 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 07:56:00,290 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:56:00,436 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 07:56:00,450 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 07:56:00,450 - __main__ - INFO - Loading dataset
2025-04-14 07:56:01,821 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 07:56:23,445 - __main__ - INFO - Processing dataset
2025-04-14 07:56:23,694 - __main__ - INFO - Searching for noisy pairs starting from index 1164
2025-04-14 07:56:30,616 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 07:56:52,020 - __main__ - INFO - Processed 136 samples without finding a noisy pair
2025-04-14 07:57:02,121 - __main__ - INFO - Noisy pair found at index 1344: chosen_logps=-268.0000, rejected_logps=-270.0000
2025-04-14 07:57:02,122 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 07:57:02,191 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_4
2025-04-14 07:57:02,192 - __main__ - INFO - New dataset offset: 1345
Updated dataset offset to: 1345 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 4
Worker node 2 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 07:57:17,722 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]2025-04-14 07:57:18,065 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]2025-04-14 07:57:18,271 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.32it/s]2025-04-14 07:57:18,414 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:57:18,429 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 07:57:18,429 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:57:18,452 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
2025-04-14 07:57:18,622 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  5.41it/s]2025-04-14 07:57:18,762 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:57:18,774 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 07:57:18,774 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  5.80it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  6.01it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.90it/s]
2025-04-14 07:57:19,127 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 07:57:19,278 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 07:57:19,293 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 07:57:19,293 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 07:57:34,958 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 07:57:35,337 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 07:57:35,894 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 07:57:45,191 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 07:57:45,303 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 07:57:45,920 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 07:57:49,080 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 07:57:49,135 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 07:57:49,640 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 07:57:52,314 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 07:57:52,494 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 07:57:53,000 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 07:57:53,052 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 07:57:53,096 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 07:57:53,477 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 07:57:56,356 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 07:57:56,370 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 07:57:56,892 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 07:57:56,925 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 07:57:56,995 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 07:57:57,486 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 07:58:00,264 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 07:58:00,531 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 07:58:00,739 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 07:58:00,752 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 07:58:00,898 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 07:58:01,756 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 07:58:04,247 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 07:58:04,444 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 07:58:04,580 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 07:58:04,715 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 07:58:04,810 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 07:58:05,862 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 07:58:07,991 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 07:58:08,157 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 07:58:08,725 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 07:58:08,870 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 07:58:09,132 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 07:58:09,704 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 07:58:12,225 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 07:58:12,393 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 07:58:12,992 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 07:58:13,281 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 07:58:13,321 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 07:58:14,330 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 07:58:16,335 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 07:58:16,337 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 07:58:16,999 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 07:58:17,473 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 07:58:17,482 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 07:58:18,024 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 07:58:18,028 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 07:58:18,693 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 07:58:18,697 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 07:58:20,666 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 07:58:20,805 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 07:58:22,018 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 07:58:25,393 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 07:58:25,504 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 07:58:25,549 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 07:58:25,655 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 07:58:25,780 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 07:58:25,883 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 07:59:46,899 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 07:59:46,992 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 07:59:47,490 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 07:59:47,655 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 07:59:48,757 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 07:59:49,292 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 07:59:51,503 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 07:59:51,552 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 07:59:55,170 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 07:59:55,523 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 07:59:55,557 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 07:59:56,474 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 07:59:59,687 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 07:59:59,910 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 08:00:03,378 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 08:00:03,875 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 08:00:06,360 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 08:00:06,358 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 08:00:07,874 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 08:00:08,133 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 08:00:09,443 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 08:00:11,304 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 08:00:11,687 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 08:00:13,500 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 08:00:15,376 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 08:00:16,201 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 08:00:16,610 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 08:00:16,916 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 08:00:20,015 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 08:00:20,978 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 08:02:06,001 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_0.npy
2025-04-14 08:02:06,001 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 203 negative comparisons
[E414 08:02:06.050057390 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76bc92dc9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76bc481cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76bc481cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x76bc481cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x76bc481d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76bc481d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x76bc92f305c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x76bc93894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x76bc93926850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank2]:[E414 08:02:06.061909121 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e3b87a64446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e3b3cdcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e3b3cdcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7e3b3cdcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7e3b3cdd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e3b3cdd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e3b87bcb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e3b88694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e3b88726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:02:06,365 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_1.npy
2025-04-14 08:02:06,365 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 201 negative comparisons
2025-04-14 08:02:08,984 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 08:02:09,154 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 08:02:13,059 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_2.npy
2025-04-14 08:02:13,059 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 185 negative comparisons
[E414 08:02:13.998481395 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f84982ea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f844d5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f844d5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f844d5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f844d5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f844d5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f84984515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f8498e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f8498f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 08:02:13.000385373 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x776e9b8b9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x776e50bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x776e50bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x776e50bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x776e50bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x776e50bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x776e9ba205c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x776e9c494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x776e9c526850 in /lib/x86_64-linux-gnu/libc.so.6)

Worker node 1 successfully completed gradient computation for iteration 4
Waiting for worker nodes to complete gradient computation...
2025-04-14 08:02:16,031 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 4
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 08:02:34,237 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 08:02:34,785 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:02:34,925 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:02:34,937 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 08:02:51,417 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 08:02:54,268 - __main__ - INFO - Loaded gradients from node 0: 203 negative comparisons
2025-04-14 08:02:54,498 - __main__ - INFO - Loaded gradients from node 1: 201 negative comparisons
2025-04-14 08:02:54,727 - __main__ - INFO - Loaded gradients from node 2: 185 negative comparisons
2025-04-14 08:02:54,885 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 08:02:55,072 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 08:02:55,258 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119135361
2025-04-14 08:02:55,444 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107353956
2025-04-14 08:02:55,631 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84838817
2025-04-14 08:02:55,817 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47203570
2025-04-14 08:02:56,004 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255409
2025-04-14 08:02:56,191 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802105
2025-04-14 08:02:56,377 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1549863
2025-04-14 08:02:56,563 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553184
2025-04-14 08:02:56,904 - __main__ - INFO - Non-zero entries after thresholding: 47203570
2025-04-14 08:02:56,904 - __main__ - INFO - Testing stepsize: 0
2025-04-14 08:02:57,550 - __main__ - INFO -   chosen_logps: -267.78021, rejected_logps: -269.54211
2025-04-14 08:02:57,550 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 08:02:57,832 - __main__ - INFO -   chosen_logps: -267.58276, rejected_logps: -269.83707
2025-04-14 08:02:57,832 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 08:02:58,115 - __main__ - INFO -   chosen_logps: -265.37537, rejected_logps: -271.36600
2025-04-14 08:02:58,115 - __main__ - INFO - Testing stepsize: 1
2025-04-14 08:02:58,401 - __main__ - INFO -   chosen_logps: -263.50339, rejected_logps: -272.61658
2025-04-14 08:02:58,401 - __main__ - INFO - Testing stepsize: 2
2025-04-14 08:02:58,691 - __main__ - INFO -   chosen_logps: -259.07141, rejected_logps: -276.26093
2025-04-14 08:02:58,691 - __main__ - INFO - Testing stepsize: 5
2025-04-14 08:02:58,981 - __main__ - INFO -   chosen_logps: -247.18585, rejected_logps: -284.81549
2025-04-14 08:02:58,981 - __main__ - INFO - Testing stepsize: 10
2025-04-14 08:02:59,275 - __main__ - INFO -   chosen_logps: -228.37910, rejected_logps: -304.22705
2025-04-14 08:02:59,275 - __main__ - INFO - Testing stepsize: 15
2025-04-14 08:02:59,573 - __main__ - INFO -   chosen_logps: -211.78101, rejected_logps: -325.00592
2025-04-14 08:03:00,335 - __main__ - INFO - Update scale: 0.01145277777777778
2025-04-14 08:03:00,431 - __main__ - INFO - Model weights updated successfully
2025-04-14 08:03:01,458 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:03:01,493 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:03:02,519 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 08:03:02,556 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 08:03:02,556 - __main__ - INFO - Skipping full model checkpoint for iteration 4
2025-04-14 08:03:02,556 - __main__ - INFO - Iteration 4 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 4
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 4
Iteration 4 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 5
===========================================
Iteration 5 is in valid list - performing full update
Finding noisy pairs for iteration 5...
2025-04-14 08:03:24,092 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 08:03:24,642 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:03:24,786 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:03:24,801 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 08:03:24,801 - __main__ - INFO - Loading dataset
2025-04-14 08:03:25,694 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 08:03:47,117 - __main__ - INFO - Processing dataset
2025-04-14 08:03:47,359 - __main__ - INFO - Searching for noisy pairs starting from index 1345
2025-04-14 08:03:58,557 - __main__ - INFO - Processed 55 samples without finding a noisy pair
2025-04-14 08:04:19,207 - __main__ - INFO - Processed 155 samples without finding a noisy pair
2025-04-14 08:04:24,651 - __main__ - INFO - Noisy pair found at index 1530: chosen_logps=-360.0000, rejected_logps=-362.0000
2025-04-14 08:04:24,652 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 08:04:24,719 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_5
2025-04-14 08:04:24,719 - __main__ - INFO - New dataset offset: 1531
Updated dataset offset to: 1531 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 5
Worker node 1 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 08:04:39,747 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 08:04:39,930 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.50it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.40it/s]2025-04-14 08:04:40,140 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]2025-04-14 08:04:40,304 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
2025-04-14 08:04:40,447 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:04:40,462 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 08:04:40,462 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:04:40,476 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.24it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 08:04:40,630 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:04:40,645 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 08:04:40,645 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:04:40,696 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:04:40,838 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:04:40,850 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 08:04:40,850 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:04:57,080 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 08:04:57,200 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 08:04:57,390 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 08:05:07,370 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 08:05:07,484 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 08:05:07,490 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 08:05:11,132 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 08:05:11,332 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 08:05:11,396 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 08:05:14,516 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 08:05:14,563 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 08:05:14,601 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 08:05:15,208 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 08:05:15,273 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 08:05:15,345 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 08:05:18,295 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 08:05:18,427 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 08:05:18,647 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 08:05:18,980 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 08:05:19,230 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 08:05:19,393 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 08:05:22,271 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 08:05:22,489 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 08:05:22,556 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 08:05:23,166 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 08:05:23,176 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 08:05:23,304 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 08:05:26,042 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 08:05:26,763 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 08:05:26,823 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 08:05:27,212 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 08:05:27,258 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 08:05:27,696 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 08:05:30,412 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 08:05:30,483 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 08:05:30,678 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 08:05:31,399 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 08:05:31,433 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 08:05:31,749 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 08:05:34,256 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 08:05:34,968 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 08:05:35,180 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 08:05:35,765 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 08:05:35,773 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 08:05:36,066 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 08:05:38,293 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 08:05:38,486 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 08:05:38,744 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 08:05:39,980 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 08:05:39,983 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 08:05:40,079 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 08:05:40,080 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 08:05:40,645 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 08:05:40,652 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 08:05:42,984 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 08:05:43,021 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 08:05:43,627 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 08:05:47,269 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 08:05:47,836 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 08:05:47,994 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 08:05:48,001 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 08:05:48,000 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 08:05:48,024 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 08:07:07,472 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 08:07:07,612 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 08:07:07,892 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 08:07:10,614 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 08:07:10,937 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 08:07:13,669 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 08:07:14,556 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 08:07:15,726 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 08:07:16,597 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 08:07:18,790 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 08:07:19,310 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 08:07:22,810 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 08:07:23,135 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 08:07:23,253 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 08:07:26,016 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 08:07:27,088 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 08:07:30,234 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 08:07:30,447 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 08:07:30,710 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 08:07:31,791 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 08:07:32,849 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 08:07:34,604 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 08:07:35,564 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 08:07:36,253 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 08:07:37,985 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 08:07:40,195 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 08:07:40,393 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 08:07:41,332 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 08:07:43,337 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 08:07:43,639 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
[E414 08:09:31.198469883 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x762cb7568446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x762c6c9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x762c6c9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x762c6c9cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x762c6c9d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x762c6c9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x762cb76c35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x762cb8094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x762cb8126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:09:31,549 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_0.npy
2025-04-14 08:09:31,549 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 101 negative comparisons
2025-04-14 08:09:32,007 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_1.npy
2025-04-14 08:09:32,008 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 105 negative comparisons
2025-04-14 08:09:34,464 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 08:09:35,035 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 08:09:37,383 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_2.npy
2025-04-14 08:09:37,383 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 85 negative comparisons
2025-04-14 08:09:40,140 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 5
Worker node 2 successfully completed gradient computation for iteration 5
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 08:09:57,577 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 08:09:58,126 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:09:58,273 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:09:58,286 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 08:10:14,774 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 08:10:17,673 - __main__ - INFO - Loaded gradients from node 0: 101 negative comparisons
2025-04-14 08:10:17,907 - __main__ - INFO - Loaded gradients from node 1: 105 negative comparisons
2025-04-14 08:10:18,140 - __main__ - INFO - Loaded gradients from node 2: 85 negative comparisons
2025-04-14 08:10:18,300 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 08:10:18,492 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 08:10:18,690 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119129373
2025-04-14 08:10:18,887 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107339746
2025-04-14 08:10:19,076 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84830488
2025-04-14 08:10:19,265 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47191739
2025-04-14 08:10:19,454 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22253873
2025-04-14 08:10:19,641 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803238
2025-04-14 08:10:19,829 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551514
2025-04-14 08:10:20,016 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554406
2025-04-14 08:10:20,359 - __main__ - INFO - Non-zero entries after thresholding: 47191739
2025-04-14 08:10:20,359 - __main__ - INFO - Using relaxed comparison (count_negative: 291)
2025-04-14 08:10:20,359 - __main__ - INFO - Testing stepsize: 0
2025-04-14 08:10:21,029 - __main__ - INFO -   chosen_logps: -360.31793, rejected_logps: -361.51300
2025-04-14 08:10:21,030 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 08:10:21,329 - __main__ - INFO -   chosen_logps: -359.98401, rejected_logps: -361.62729
2025-04-14 08:10:21,329 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 08:10:21,628 - __main__ - INFO -   chosen_logps: -357.63818, rejected_logps: -363.50186
2025-04-14 08:10:21,628 - __main__ - INFO - Testing stepsize: 1
2025-04-14 08:10:21,933 - __main__ - INFO -   chosen_logps: -354.82458, rejected_logps: -364.63214
2025-04-14 08:10:21,933 - __main__ - INFO - Testing stepsize: 2
2025-04-14 08:10:22,239 - __main__ - INFO -   chosen_logps: -350.27304, rejected_logps: -367.03857
2025-04-14 08:10:22,239 - __main__ - INFO - Testing stepsize: 5
2025-04-14 08:10:22,546 - __main__ - INFO -   chosen_logps: -334.90326, rejected_logps: -376.16199
2025-04-14 08:10:22,546 - __main__ - INFO - Testing stepsize: 10
2025-04-14 08:10:22,856 - __main__ - INFO -   chosen_logps: -314.08459, rejected_logps: -392.75629
2025-04-14 08:10:22,856 - __main__ - INFO - Testing stepsize: 15
2025-04-14 08:10:23,166 - __main__ - INFO -   chosen_logps: -297.12561, rejected_logps: -412.05600
2025-04-14 08:10:23,813 - __main__ - INFO - Update scale: 0.005658333333333334
2025-04-14 08:10:23,815 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 08:10:23,895 - __main__ - INFO - Model weights updated successfully
2025-04-14 08:10:24,891 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:10:24,928 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:10:26,198 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 08:10:26,236 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 08:10:26,236 - __main__ - INFO - Skipping full model checkpoint for iteration 5
2025-04-14 08:10:26,237 - __main__ - INFO - Iteration 5 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 5
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 5
Iteration 5 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 6
===========================================
Iteration 6 is in valid list - performing full update
Finding noisy pairs for iteration 6...
2025-04-14 08:10:47,359 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 08:10:47,908 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:10:48,053 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:10:48,067 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 08:10:48,068 - __main__ - INFO - Loading dataset
2025-04-14 08:10:48,729 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 08:11:10,357 - __main__ - INFO - Processing dataset
2025-04-14 08:11:10,603 - __main__ - INFO - Searching for noisy pairs starting from index 1531
2025-04-14 08:11:24,693 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 08:11:29,241 - __main__ - INFO - Noisy pair found at index 1624: chosen_logps=-127.5000, rejected_logps=-130.0000
2025-04-14 08:11:29,242 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 08:11:29,310 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_6
2025-04-14 08:11:29,311 - __main__ - INFO - New dataset offset: 1625
Updated dataset offset to: 1625 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 6
Worker node 2 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 08:11:45,117 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 08:11:45,322 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 08:11:45,360 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]2025-04-14 08:11:45,674 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 08:11:45,819 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:11:45,835 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 08:11:45,835 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:11:45,868 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:11:45,916 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:11:46,022 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:11:46,036 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 08:11:46,036 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:11:46,057 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:11:46,070 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 08:11:46,070 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:12:02,435 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 08:12:02,601 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 08:12:02,616 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 08:12:12,577 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 08:12:12,632 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 08:12:12,666 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 08:12:16,399 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 08:12:16,415 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 08:12:16,928 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 08:12:19,576 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 08:12:19,619 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 08:12:20,037 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 08:12:20,305 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 08:12:20,341 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 08:12:20,856 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 08:12:23,790 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 08:12:23,791 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 08:12:24,125 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 08:12:24,185 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 08:12:24,705 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 08:12:24,870 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 08:12:27,729 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 08:12:27,868 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 08:12:27,880 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 08:12:27,986 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 08:12:28,146 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 08:12:29,059 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 08:12:31,209 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 08:12:31,676 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 08:12:31,882 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 08:12:31,915 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 08:12:31,968 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 08:12:33,340 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 08:12:35,375 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 08:12:35,617 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 08:12:35,833 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 08:12:35,963 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 08:12:36,680 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 08:12:37,295 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 08:12:39,277 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 08:12:39,372 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 08:12:39,978 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 08:12:40,054 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 08:12:40,633 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 08:12:41,455 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 08:12:43,505 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 08:12:43,693 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 08:12:44,177 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 08:12:44,185 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 08:12:44,190 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 08:12:44,194 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 08:12:44,599 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 08:12:45,782 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 08:12:45,785 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 08:12:47,240 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 08:12:48,121 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 08:12:48,616 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 08:12:51,743 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 08:12:51,908 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 08:12:53,014 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 08:12:53,079 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 08:12:53,191 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 08:12:53,569 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 08:14:15,217 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 08:14:17,062 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 08:14:17,667 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 08:14:19,452 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 08:14:21,831 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 08:14:22,960 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 08:14:24,429 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 08:14:24,672 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 08:14:26,458 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 08:14:27,291 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 08:14:28,404 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 08:14:29,741 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 08:14:30,825 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 08:14:32,362 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 08:14:35,130 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 08:14:36,071 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 08:14:37,496 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 08:14:37,989 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 08:14:39,821 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 08:14:41,858 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 08:14:42,469 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 08:14:44,064 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 08:14:44,460 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 08:14:45,229 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 08:14:47,137 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 08:14:48,475 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 08:14:48,474 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 08:14:49,669 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 08:14:50,652 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 08:14:50,945 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 08:16:41.148217302 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x705db3c88446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x705d68fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x705d68fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x705d68fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x705d68fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x705d68fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x705db3def5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x705db4894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x705db4926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:16:41,371 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_0.npy
2025-04-14 08:16:41,371 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 162 negative comparisons
[E414 08:16:43.470713439 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7303ce91e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x730383bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x730383bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x730383bcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x730383bd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x730383bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7303cea855c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7303cf494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7303cf526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 08:16:43.488791658 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70eef9ac9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70eeaedcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70eeaedcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x70eeaedcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x70eeaedd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70eeaedd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70eef9c305c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70eefa694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70eefa726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 08:16:43.503915003 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e8c18ce3446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e8bcdfcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e8bcdfcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7e8bcdfcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7e8bcdfd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e8bcdfd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e8c18e4a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e8c19894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e8c19926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:16:43,737 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_1.npy
2025-04-14 08:16:43,737 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 158 negative comparisons
2025-04-14 08:16:44,231 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 08:16:46,492 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_2.npy
2025-04-14 08:16:46,492 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 155 negative comparisons
2025-04-14 08:16:46,545 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 08:16:46.443757550 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d10772c7446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7d102c5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7d102c5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7d102c5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7d102c5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d102c5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7d107742e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7d1077e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7d1077f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 08:16:46.451036089 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x779660887446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x779615bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x779615bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x779615bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x779615bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x779615bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7796609ee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x779661494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x779661526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:16:49,745 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 6
Worker node 2 successfully completed gradient computation for iteration 6
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 08:17:07,127 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.72it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
2025-04-14 08:17:07,670 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:17:07,812 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:17:07,825 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 08:17:24,291 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 08:17:27,250 - __main__ - INFO - Loaded gradients from node 0: 162 negative comparisons
2025-04-14 08:17:27,484 - __main__ - INFO - Loaded gradients from node 1: 158 negative comparisons
2025-04-14 08:17:27,717 - __main__ - INFO - Loaded gradients from node 2: 155 negative comparisons
2025-04-14 08:17:27,877 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 08:17:28,067 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 08:17:28,257 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119130993
2025-04-14 08:17:28,447 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107349521
2025-04-14 08:17:28,637 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84841506
2025-04-14 08:17:28,829 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47186816
2025-04-14 08:17:29,020 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255681
2025-04-14 08:17:29,211 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806062
2025-04-14 08:17:29,402 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552034
2025-04-14 08:17:29,593 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554843
2025-04-14 08:17:29,939 - __main__ - INFO - Non-zero entries after thresholding: 47186816
2025-04-14 08:17:29,939 - __main__ - INFO - Testing stepsize: 0
2025-04-14 08:17:30,676 - __main__ - INFO -   chosen_logps: -127.93110, rejected_logps: -129.48669
2025-04-14 08:17:30,676 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 08:17:31,042 - __main__ - INFO -   chosen_logps: -127.49780, rejected_logps: -129.48672
2025-04-14 08:17:31,042 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 08:17:31,409 - __main__ - INFO -   chosen_logps: -125.68799, rejected_logps: -129.96822
2025-04-14 08:17:31,409 - __main__ - INFO - Testing stepsize: 1
2025-04-14 08:17:31,779 - __main__ - INFO -   chosen_logps: -123.94316, rejected_logps: -130.28151
2025-04-14 08:17:31,780 - __main__ - INFO - Testing stepsize: 2
2025-04-14 08:17:32,153 - __main__ - INFO -   chosen_logps: -120.62180, rejected_logps: -131.08527
2025-04-14 08:17:32,153 - __main__ - INFO - Testing stepsize: 5
2025-04-14 08:17:32,528 - __main__ - INFO -   chosen_logps: -111.64455, rejected_logps: -134.30005
2025-04-14 08:17:32,528 - __main__ - INFO - Testing stepsize: 10
2025-04-14 08:17:32,906 - __main__ - INFO -   chosen_logps: -99.53680, rejected_logps: -138.98271
2025-04-14 08:17:32,906 - __main__ - INFO - Testing stepsize: 15
2025-04-14 08:17:33,284 - __main__ - INFO -   chosen_logps: -90.79144, rejected_logps: -143.90515
2025-04-14 08:17:34,023 - __main__ - INFO - Update scale: 0.009236111111111112
2025-04-14 08:17:34,104 - __main__ - INFO - Model weights updated successfully
2025-04-14 08:17:35,221 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:17:35,256 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:17:36,422 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 08:17:36,458 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 08:17:36,458 - __main__ - INFO - Skipping full model checkpoint for iteration 6
2025-04-14 08:17:36,458 - __main__ - INFO - Iteration 6 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 6
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 6
Iteration 6 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 7
===========================================
Iteration 7 is in valid list - performing full update
Finding noisy pairs for iteration 7...
2025-04-14 08:17:57,455 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 08:17:58,004 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:17:58,150 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:17:58,165 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 08:17:58,165 - __main__ - INFO - Loading dataset
2025-04-14 08:17:59,916 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 08:18:21,619 - __main__ - INFO - Processing dataset
2025-04-14 08:18:21,882 - __main__ - INFO - Searching for noisy pairs starting from index 1625
2025-04-14 08:18:29,570 - __main__ - INFO - Noisy pair found at index 1663: chosen_logps=-49.7500, rejected_logps=-50.5000
2025-04-14 08:18:29,571 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 08:18:29,639 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_7
2025-04-14 08:18:29,640 - __main__ - INFO - New dataset offset: 1664
Updated dataset offset to: 1664 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 7
Worker node 1 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 08:18:44,130 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 08:18:44,339 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 08:18:44,451 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]2025-04-14 08:18:44,688 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 08:18:44,833 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:18:44,848 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 08:18:44,848 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:18:44,884 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 08:18:45,007 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:18:45,037 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:18:45,052 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 08:18:45,052 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:18:45,149 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:18:45,162 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 08:18:45,162 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:19:01,350 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 08:19:01,607 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 08:19:01,698 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 08:19:11,517 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 08:19:11,763 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 08:19:12,259 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 08:19:15,355 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 08:19:15,563 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 08:19:16,071 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 08:19:18,302 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 08:19:18,923 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 08:19:19,114 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 08:19:19,497 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 08:19:20,000 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 08:19:20,082 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 08:19:22,194 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 08:19:22,859 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 08:19:23,330 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 08:19:23,858 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 08:19:23,894 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 08:19:24,210 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 08:19:26,682 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 08:19:27,264 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 08:19:27,326 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 08:19:27,335 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 08:19:27,820 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 08:19:28,134 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 08:19:30,374 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 08:19:31,289 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 08:19:31,316 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 08:19:31,700 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 08:19:31,881 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 08:19:32,048 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 08:19:34,482 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 08:19:35,239 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 08:19:35,251 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 08:19:35,522 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 08:19:36,001 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 08:19:36,137 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 08:19:38,527 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 08:19:39,268 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 08:19:39,431 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 08:19:39,442 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 08:19:40,110 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 08:19:40,214 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 08:19:42,439 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 08:19:43,395 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 08:19:43,398 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 08:19:43,512 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 08:19:43,813 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 08:19:44,228 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 08:19:44,231 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 08:19:44,467 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 08:19:44,475 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 08:19:46,496 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 08:19:47,419 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 08:19:48,155 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 08:19:50,566 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 08:19:50,936 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 08:19:52,011 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 08:19:52,096 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 08:19:52,934 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 08:19:53,023 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 08:21:13,742 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 08:21:16,613 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 08:21:17,403 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 08:21:19,979 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 08:21:20,019 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 08:21:21,251 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 08:21:21,848 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 08:21:23,141 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 08:21:27,010 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 08:21:27,460 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 08:21:27,541 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 08:21:29,454 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 08:21:29,841 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 08:21:30,503 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 08:21:31,609 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 08:21:35,699 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 08:21:35,737 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 08:21:38,071 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 08:21:39,989 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 08:21:40,358 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 08:21:40,674 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 08:21:44,123 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 08:21:44,828 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 08:21:44,884 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 08:21:46,191 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 08:21:47,559 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 08:21:48,521 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 08:21:48,948 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 08:21:52,025 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 08:21:52,510 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[rank3]:[E414 08:23:40.692413584 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ed2499b0446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ed1fedcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ed1fedcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7ed1fedcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7ed1fedd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ed1fedd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7ed249b175c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7ed24a494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7ed24a526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:23:40,967 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_0.npy
2025-04-14 08:23:40,967 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 90 negative comparisons
[E414 08:23:41.907956025 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72b2ad3ea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72b2627cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72b2627cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x72b2627cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x72b2627d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72b2627d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72b2ad5515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72b2ade94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72b2adf26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:23:41,049 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_1.npy
2025-04-14 08:23:41,049 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 86 negative comparisons
[E414 08:23:41.912020786 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78d969efd446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78d91f1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78d91f1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x78d91f1cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x78d91f1d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78d91f1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78d96a0645c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78d96aa94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78d96ab26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:23:43,765 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 08:23:44,022 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 08:23:48,378 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_2.npy
2025-04-14 08:23:48,378 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 80 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 7
Waiting for worker nodes to complete gradient computation...
2025-04-14 08:23:51,469 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 7
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 08:24:09,468 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 08:24:10,016 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:24:10,157 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:24:10,169 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 08:24:26,688 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 08:24:29,592 - __main__ - INFO - Loaded gradients from node 0: 90 negative comparisons
2025-04-14 08:24:29,825 - __main__ - INFO - Loaded gradients from node 1: 86 negative comparisons
2025-04-14 08:24:30,059 - __main__ - INFO - Loaded gradients from node 2: 80 negative comparisons
2025-04-14 08:24:30,218 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 08:24:30,409 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 08:24:30,599 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119130686
2025-04-14 08:24:30,789 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107349484
2025-04-14 08:24:30,980 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84836084
2025-04-14 08:24:31,170 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47196463
2025-04-14 08:24:31,361 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22254871
2025-04-14 08:24:31,551 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804105
2025-04-14 08:24:31,741 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551750
2025-04-14 08:24:31,932 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554586
2025-04-14 08:24:32,278 - __main__ - INFO - Non-zero entries after thresholding: 47196463
2025-04-14 08:24:32,278 - __main__ - INFO - Using relaxed comparison (count_negative: 256)
2025-04-14 08:24:32,278 - __main__ - INFO - Testing stepsize: 0
2025-04-14 08:24:33,001 - __main__ - INFO -   chosen_logps: -49.78997, rejected_logps: -50.12823
2025-04-14 08:24:33,001 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 08:24:33,364 - __main__ - INFO -   chosen_logps: -49.78347, rejected_logps: -50.06491
2025-04-14 08:24:33,364 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 08:24:33,731 - __main__ - INFO -   chosen_logps: -49.47014, rejected_logps: -50.27317
2025-04-14 08:24:33,731 - __main__ - INFO - Testing stepsize: 1
2025-04-14 08:24:34,099 - __main__ - INFO -   chosen_logps: -49.20634, rejected_logps: -50.71636
2025-04-14 08:24:34,099 - __main__ - INFO - Testing stepsize: 2
2025-04-14 08:24:34,470 - __main__ - INFO -   chosen_logps: -47.93710, rejected_logps: -51.02448
2025-04-14 08:24:34,470 - __main__ - INFO - Testing stepsize: 5
2025-04-14 08:24:34,841 - __main__ - INFO -   chosen_logps: -45.00603, rejected_logps: -52.33303
2025-04-14 08:24:34,842 - __main__ - INFO - Testing stepsize: 10
2025-04-14 08:24:35,214 - __main__ - INFO -   chosen_logps: -40.78362, rejected_logps: -55.19584
2025-04-14 08:24:35,215 - __main__ - INFO - Testing stepsize: 15
2025-04-14 08:24:35,587 - __main__ - INFO -   chosen_logps: -37.44324, rejected_logps: -58.01585
2025-04-14 08:24:36,316 - __main__ - INFO - Update scale: 0.004977777777777778
2025-04-14 08:24:36,317 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 08:24:36,397 - __main__ - INFO - Model weights updated successfully
2025-04-14 08:24:37,233 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:24:37,267 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:24:38,135 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 08:24:38,168 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 08:24:38,168 - __main__ - INFO - Skipping full model checkpoint for iteration 7
2025-04-14 08:24:38,169 - __main__ - INFO - Iteration 7 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 7
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 7
Iteration 7 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 8
===========================================
Iteration 8 is in valid list - performing full update
Finding noisy pairs for iteration 8...
2025-04-14 08:24:59,355 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 08:24:59,905 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:25:00,048 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:25:00,063 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 08:25:00,063 - __main__ - INFO - Loading dataset
2025-04-14 08:25:00,835 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 08:25:22,502 - __main__ - INFO - Processing dataset
2025-04-14 08:25:22,747 - __main__ - INFO - Searching for noisy pairs starting from index 1664
2025-04-14 08:25:31,224 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 08:25:40,015 - __main__ - INFO - Noisy pair found at index 1746: chosen_logps=-23.8750, rejected_logps=-24.2500
2025-04-14 08:25:40,016 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 08:25:40,083 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_8
2025-04-14 08:25:40,084 - __main__ - INFO - New dataset offset: 1747
Updated dataset offset to: 1747 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 8
Worker node 2 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 08:25:55,454 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 08:25:55,719 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 08:25:55,749 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.50it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 08:25:56,012 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.50it/s]2025-04-14 08:25:56,156 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:25:56,171 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 08:25:56,171 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
2025-04-14 08:25:56,264 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:25:56,305 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:25:56,417 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:25:56,432 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 08:25:56,432 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:25:56,445 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:25:56,458 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 08:25:56,458 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:26:12,700 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 08:26:13,011 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 08:26:13,042 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 08:26:22,878 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 08:26:23,061 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 08:26:23,292 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 08:26:26,634 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 08:26:26,924 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 08:26:27,093 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 08:26:30,182 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 08:26:30,511 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 08:26:30,575 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 08:26:30,616 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 08:26:30,803 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 08:26:31,077 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 08:26:34,179 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 08:26:34,387 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 08:26:34,424 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 08:26:34,540 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 08:26:34,772 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 08:26:34,814 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 08:26:37,982 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 08:26:38,238 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 08:26:38,336 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 08:26:38,435 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 08:26:38,685 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 08:26:38,754 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 08:26:41,929 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 08:26:42,142 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 08:26:42,178 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 08:26:42,537 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 08:26:42,780 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 08:26:43,195 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 08:26:45,874 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 08:26:45,940 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 08:26:46,215 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 08:26:46,735 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 08:26:47,051 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 08:26:47,495 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 08:26:49,898 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 08:26:50,276 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 08:26:50,513 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 08:26:51,208 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 08:26:51,593 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 08:26:51,843 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 08:26:53,992 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 08:26:54,467 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 08:26:54,834 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 08:26:55,910 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 08:26:55,921 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 08:26:56,075 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 08:26:56,075 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 08:26:56,205 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 08:26:56,211 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 08:26:58,510 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 08:26:59,161 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 08:27:00,071 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 08:27:03,184 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 08:27:03,499 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 08:27:03,508 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 08:27:03,699 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 08:27:04,267 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 08:27:04,356 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 08:28:27,833 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 08:28:29,394 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 08:28:30,353 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 08:28:30,610 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 08:28:32,262 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 08:28:35,231 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 08:28:35,354 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 08:28:36,825 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 08:28:38,032 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 08:28:38,538 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 08:28:38,916 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 08:28:41,500 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 08:28:42,621 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 08:28:42,867 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 08:28:42,899 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 08:28:45,236 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 08:28:47,242 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 08:28:50,923 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 08:28:51,284 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 08:28:52,535 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 08:28:54,188 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 08:28:55,752 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 08:28:56,523 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 08:28:59,609 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 08:29:00,258 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 08:29:00,343 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 08:29:01,433 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 08:29:03,872 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 08:29:04,297 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 08:29:04,598 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
[E414 08:30:54.486751507 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x760d97884446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x760d4cbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x760d4cbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x760d4cbcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x760d4cbd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x760d4cbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x760d979eb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x760d98494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x760d98526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:30:54,795 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_0.npy
2025-04-14 08:30:54,795 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 111 negative comparisons
2025-04-14 08:30:57,811 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 08:30:58,401 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_1.npy
2025-04-14 08:30:58,401 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 96 negative comparisons
[E414 08:31:00.165282372 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7407092fd446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7406be5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7406be5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7406be5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7406be5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7406be5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7407094645c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x740709e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x740709f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 08:31:00.161337774 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72e8e660e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72e89b9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72e89b9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x72e89b9cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x72e89b9d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72e89b9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72e8e67755c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72e8e7294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72e8e7326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 08:31:00.176503334 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x725a2755c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7259dc9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7259dc9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7259dc9cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7259dc9d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7259dc9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x725a276c35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x725a28294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x725a28326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:31:00,382 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_2.npy
2025-04-14 08:31:00,382 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 114 negative comparisons
2025-04-14 08:31:01,413 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 08:31:03,238 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 8
Worker node 2 successfully completed gradient computation for iteration 8
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 08:31:21,540 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
2025-04-14 08:31:22,090 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:31:22,231 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:31:22,244 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 08:31:38,765 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 08:31:41,648 - __main__ - INFO - Loaded gradients from node 0: 111 negative comparisons
2025-04-14 08:31:41,882 - __main__ - INFO - Loaded gradients from node 1: 96 negative comparisons
2025-04-14 08:31:42,118 - __main__ - INFO - Loaded gradients from node 2: 114 negative comparisons
2025-04-14 08:31:42,278 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 08:31:42,469 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 08:31:42,659 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119135865
2025-04-14 08:31:42,849 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107345702
2025-04-14 08:31:43,038 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84829096
2025-04-14 08:31:43,228 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47194718
2025-04-14 08:31:43,418 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22257734
2025-04-14 08:31:43,608 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806541
2025-04-14 08:31:43,797 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1549398
2025-04-14 08:31:43,987 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553575
2025-04-14 08:31:44,332 - __main__ - INFO - Non-zero entries after thresholding: 47194718
2025-04-14 08:31:44,332 - __main__ - INFO - Using relaxed comparison (count_negative: 321)
2025-04-14 08:31:44,332 - __main__ - INFO - Testing stepsize: 0
2025-04-14 08:31:45,079 - __main__ - INFO -   chosen_logps: -23.92811, rejected_logps: -23.81993
2025-04-14 08:31:45,079 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 08:31:45,461 - __main__ - INFO -   chosen_logps: -23.88326, rejected_logps: -23.80079
2025-04-14 08:31:45,461 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 08:31:45,843 - __main__ - INFO -   chosen_logps: -23.66836, rejected_logps: -24.04942
2025-04-14 08:31:45,843 - __main__ - INFO - Testing stepsize: 1
2025-04-14 08:31:46,230 - __main__ - INFO -   chosen_logps: -22.95412, rejected_logps: -24.46335
2025-04-14 08:31:46,230 - __main__ - INFO - Testing stepsize: 2
2025-04-14 08:31:46,618 - __main__ - INFO -   chosen_logps: -22.17219, rejected_logps: -25.06232
2025-04-14 08:31:46,619 - __main__ - INFO - Testing stepsize: 5
2025-04-14 08:31:47,009 - __main__ - INFO -   chosen_logps: -19.39636, rejected_logps: -27.13433
2025-04-14 08:31:47,009 - __main__ - INFO - Testing stepsize: 10
2025-04-14 08:31:47,401 - __main__ - INFO -   chosen_logps: -15.16653, rejected_logps: -30.22252
2025-04-14 08:31:47,401 - __main__ - INFO - Testing stepsize: 15
2025-04-14 08:31:47,795 - __main__ - INFO -   chosen_logps: -11.67980, rejected_logps: -33.55547
2025-04-14 08:31:48,558 - __main__ - INFO - Update scale: 0.006241666666666668
2025-04-14 08:31:48,559 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 08:31:48,640 - __main__ - INFO - Model weights updated successfully
2025-04-14 08:31:49,480 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:31:49,518 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:31:50,372 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 08:31:50,407 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 08:31:50,407 - __main__ - INFO - Skipping full model checkpoint for iteration 8
2025-04-14 08:31:50,407 - __main__ - INFO - Iteration 8 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 8
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 8
Iteration 8 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 9
===========================================
Iteration 9 is in valid list - performing full update
Finding noisy pairs for iteration 9...
2025-04-14 08:32:11,368 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 08:32:11,917 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:32:12,063 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:32:12,077 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 08:32:12,077 - __main__ - INFO - Loading dataset
2025-04-14 08:32:13,122 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 08:32:34,579 - __main__ - INFO - Processing dataset
2025-04-14 08:32:34,823 - __main__ - INFO - Searching for noisy pairs starting from index 1747
2025-04-14 08:32:45,102 - __main__ - INFO - Processed 53 samples without finding a noisy pair
2025-04-14 08:33:03,313 - __main__ - INFO - Noisy pair found at index 1892: chosen_logps=-58.0000, rejected_logps=-59.7500
2025-04-14 08:33:03,314 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 08:33:03,381 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_9
2025-04-14 08:33:03,382 - __main__ - INFO - New dataset offset: 1893
Updated dataset offset to: 1893 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 9
Worker node 1 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 08:33:18,841 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]2025-04-14 08:33:19,053 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]2025-04-14 08:33:19,225 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.68it/s]2025-04-14 08:33:19,390 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 08:33:19,533 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:33:19,554 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 08:33:19,554 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.25it/s]2025-04-14 08:33:19,597 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 08:33:19,749 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:33:19,764 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 08:33:19,765 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:33:19,781 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:33:19,921 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:33:19,933 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 08:33:19,933 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:33:36,293 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 08:33:36,311 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 08:33:36,510 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 08:33:46,549 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 08:33:46,546 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 08:33:46,554 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 08:33:50,320 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 08:33:50,334 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 08:33:50,346 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 08:33:53,540 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 08:33:53,710 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 08:33:53,731 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 08:33:54,375 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 08:33:54,660 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 08:33:54,761 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 08:33:57,744 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 08:33:57,814 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 08:33:57,834 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 08:33:58,228 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 08:33:58,410 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 08:33:58,897 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 08:34:01,760 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 08:34:01,964 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 08:34:02,144 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 08:34:02,171 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 08:34:02,707 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 08:34:03,205 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 08:34:05,571 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 08:34:06,056 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 08:34:06,278 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 08:34:06,408 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 08:34:06,612 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 08:34:07,518 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 08:34:09,525 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 08:34:09,896 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 08:34:10,269 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 08:34:10,542 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 08:34:10,788 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 08:34:11,841 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 08:34:13,548 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 08:34:13,671 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 08:34:14,367 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 08:34:14,988 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 08:34:15,361 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 08:34:16,084 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 08:34:17,738 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 08:34:17,907 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 08:34:19,054 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 08:34:19,097 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 08:34:19,105 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 08:34:19,749 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 08:34:19,751 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 08:34:20,558 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 08:34:20,568 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 08:34:22,296 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 08:34:22,664 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 08:34:23,621 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 08:34:26,959 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 08:34:27,178 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 08:34:27,381 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 08:34:27,384 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 08:34:28,211 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 08:34:28,369 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 08:35:50,071 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 08:35:51,355 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 08:35:55,143 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 08:35:55,364 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 08:35:55,676 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 08:35:55,949 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 08:35:58,079 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 08:35:58,496 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 08:36:00,987 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 08:36:02,031 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 08:36:03,582 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 08:36:07,278 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 08:36:07,577 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 08:36:07,783 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 08:36:08,757 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 08:36:09,806 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 08:36:10,560 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 08:36:11,615 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 08:36:14,444 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 08:36:14,954 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 08:36:15,970 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 08:36:17,677 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 08:36:20,448 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 08:36:22,152 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 08:36:23,756 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 08:36:23,977 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 08:36:24,352 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 08:36:24,506 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 08:36:25,934 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 08:36:28,140 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
[E414 08:38:19.803468183 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76dbf6157446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76dbab5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76dbab5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x76dbab5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x76dbab5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76dbab5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x76dbf62be5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x76dbf6c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x76dbf6d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:38:19,226 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_1.npy
2025-04-14 08:38:19,227 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 116 negative comparisons
[E414 08:38:20.249080702 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70eac26ef446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70ea779cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70ea779cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x70ea779cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x70ea779d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70ea779d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70eac28565c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70eac3294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70eac3326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:38:20,491 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_0.npy
2025-04-14 08:38:20,491 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 122 negative comparisons
2025-04-14 08:38:22,296 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 08:38:23,317 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 08:38:25,596 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_2.npy
2025-04-14 08:38:25,596 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 110 negative comparisons
2025-04-14 08:38:28,279 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 9
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 9
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 08:38:45,711 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 08:38:46,260 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:38:46,401 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:38:46,413 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 08:39:02,905 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 08:39:05,802 - __main__ - INFO - Loaded gradients from node 0: 122 negative comparisons
2025-04-14 08:39:06,035 - __main__ - INFO - Loaded gradients from node 1: 116 negative comparisons
2025-04-14 08:39:06,267 - __main__ - INFO - Loaded gradients from node 2: 110 negative comparisons
2025-04-14 08:39:06,426 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 08:39:06,617 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 08:39:06,807 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119134362
2025-04-14 08:39:06,998 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107357653
2025-04-14 08:39:07,188 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84837104
2025-04-14 08:39:07,379 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47190839
2025-04-14 08:39:07,568 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22248927
2025-04-14 08:39:07,757 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8805714
2025-04-14 08:39:07,946 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552516
2025-04-14 08:39:08,135 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555087
2025-04-14 08:39:08,478 - __main__ - INFO - Non-zero entries after thresholding: 47190839
2025-04-14 08:39:08,478 - __main__ - INFO - Using relaxed comparison (count_negative: 348)
2025-04-14 08:39:08,479 - __main__ - INFO - Testing stepsize: 0
2025-04-14 08:39:09,227 - __main__ - INFO -   chosen_logps: -57.56392, rejected_logps: -59.94020
2025-04-14 08:39:09,227 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 08:39:09,605 - __main__ - INFO -   chosen_logps: -57.33023, rejected_logps: -60.03181
2025-04-14 08:39:09,605 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 08:39:09,985 - __main__ - INFO -   chosen_logps: -56.70031, rejected_logps: -60.46785
2025-04-14 08:39:09,985 - __main__ - INFO - Testing stepsize: 1
2025-04-14 08:39:10,369 - __main__ - INFO -   chosen_logps: -55.51836, rejected_logps: -60.71523
2025-04-14 08:39:10,369 - __main__ - INFO - Testing stepsize: 2
2025-04-14 08:39:10,756 - __main__ - INFO -   chosen_logps: -53.65226, rejected_logps: -61.40021
2025-04-14 08:39:10,756 - __main__ - INFO - Testing stepsize: 5
2025-04-14 08:39:11,144 - __main__ - INFO -   chosen_logps: -48.53706, rejected_logps: -63.64615
2025-04-14 08:39:11,144 - __main__ - INFO - Testing stepsize: 10
2025-04-14 08:39:11,533 - __main__ - INFO -   chosen_logps: -40.89184, rejected_logps: -67.28647
2025-04-14 08:39:11,533 - __main__ - INFO - Testing stepsize: 15
2025-04-14 08:39:11,924 - __main__ - INFO -   chosen_logps: -35.08379, rejected_logps: -71.04443
2025-04-14 08:39:12,682 - __main__ - INFO - Update scale: 0.006766666666666667
2025-04-14 08:39:12,684 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 08:39:12,764 - __main__ - INFO - Model weights updated successfully
2025-04-14 08:39:14,008 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:39:14,044 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:39:14,944 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 08:39:14,978 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 08:39:14,978 - __main__ - INFO - Skipping full model checkpoint for iteration 9
2025-04-14 08:39:14,978 - __main__ - INFO - Iteration 9 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 9
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 9
Iteration 9 completed successfully
Cleaning up iteration files...
Multi-node training completed successfully!
Copying scripts to worker nodes...
Starting with dataset offset: 0
===========================================
Processing iteration 0
===========================================
Iteration 0 is in valid list - performing full update
Finding noisy pairs for iteration 0...
2025-04-14 08:39:39,283 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.15it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 08:39:39,834 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:39:39,979 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:39:39,993 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 08:39:39,993 - __main__ - INFO - Loading dataset
2025-04-14 08:39:40,760 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 08:40:02,124 - __main__ - INFO - Processing dataset
2025-04-14 08:40:02,369 - __main__ - INFO - Searching for noisy pairs starting from index 0
2025-04-14 08:40:03,013 - __main__ - INFO - Processed 0 samples without finding a noisy pair
2025-04-14 08:40:23,400 - __main__ - INFO - Processed 100 samples without finding a noisy pair
2025-04-14 08:40:41,550 - __main__ - INFO - Processed 200 samples without finding a noisy pair
2025-04-14 08:41:00,589 - __main__ - INFO - Processed 300 samples without finding a noisy pair
2025-04-14 08:41:07,313 - __main__ - INFO - Noisy pair found at index 337: chosen_logps=-90.0000, rejected_logps=-92.0000
2025-04-14 08:41:07,314 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 08:41:07,381 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_0
2025-04-14 08:41:07,381 - __main__ - INFO - New dataset offset: 338
Updated dataset offset to: 338 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 0
Worker node 2 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 08:41:23,123 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 08:41:23,161 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]2025-04-14 08:41:23,516 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
2025-04-14 08:41:23,668 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.31it/s]2025-04-14 08:41:23,716 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:41:23,821 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:41:23,836 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 08:41:23,836 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]2025-04-14 08:41:23,863 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:41:23,875 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 08:41:23,875 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 08:41:24,061 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:41:24,204 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:41:24,219 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 08:41:24,219 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:41:40,446 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 08:41:40,500 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 08:41:40,833 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 08:41:50,713 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 08:41:50,958 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 08:41:50,994 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 08:41:54,538 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 08:41:54,776 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 08:41:54,780 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 08:41:57,668 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 08:41:57,816 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 08:41:58,002 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 08:41:58,583 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 08:41:58,671 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 08:41:58,713 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 08:42:01,414 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 08:42:01,931 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 08:42:01,963 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 08:42:02,413 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 08:42:02,658 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 08:42:02,746 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 08:42:05,733 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 08:42:05,803 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 08:42:06,090 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 08:42:06,268 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 08:42:06,568 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 08:42:06,723 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 08:42:09,719 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 08:42:09,802 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 08:42:09,916 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 08:42:10,220 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 08:42:10,713 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 08:42:10,896 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 08:42:13,487 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 08:42:13,612 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 08:42:13,823 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 08:42:14,392 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 08:42:14,970 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 08:42:15,134 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 08:42:17,330 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 08:42:18,016 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 08:42:18,185 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 08:42:18,830 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 08:42:19,286 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 08:42:19,572 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 08:42:21,840 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 08:42:22,134 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 08:42:22,218 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 08:42:23,254 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 08:42:23,264 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 08:42:23,449 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 08:42:23,460 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 08:42:24,031 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 08:42:24,042 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 08:42:26,312 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 08:42:26,684 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 08:42:26,789 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 08:42:30,478 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 08:42:30,945 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 08:42:31,059 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 08:42:31,082 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 08:42:31,221 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 08:42:31,786 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 08:43:47,241 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 08:43:48,279 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 08:43:49,802 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 08:43:51,545 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 08:43:52,522 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 08:43:55,350 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 08:43:57,113 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 08:43:59,880 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 08:44:00,549 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 08:44:00,778 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 08:44:01,591 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 08:44:03,129 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 08:44:03,978 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 08:44:05,468 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 08:44:06,516 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 08:44:07,484 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 08:44:08,071 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 08:44:09,547 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 08:44:12,399 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 08:44:13,425 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 08:44:15,691 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 08:44:16,085 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 08:44:16,484 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 08:44:16,700 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 08:44:19,777 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 08:44:20,933 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 08:44:21,039 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 08:44:22,685 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 08:44:23,581 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 08:44:24,591 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 08:46:09,647 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_1.npy
2025-04-14 08:46:09,647 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 89 negative comparisons
[E414 08:46:09.547103509 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x755631015446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7555e63cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7555e63cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7555e63cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7555e63d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7555e63d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x75563117c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x755631a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x755631b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 08:46:09.543067345 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x725603a80446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7255b8dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7255b8dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7255b8dcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7255b8dd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7255b8dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x725603be75c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x725604694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x725604726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:46:12,799 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 08:46:14.948396505 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7491082fb446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7490bd5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7490bd5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7490bd5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7490bd5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7490bd5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7491084625c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x749108e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x749108f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:46:14,245 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_2.npy
2025-04-14 08:46:14,245 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 132 negative comparisons
[E414 08:46:14.588942096 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f558b51e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f55407cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f55407cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f55407cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f55407d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f55407d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f558b6855c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f558c094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f558c126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 08:46:14.589322155 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7dcb0bc6f446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7dcac0fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7dcac0fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7dcac0fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7dcac0fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7dcac0fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7dcb0bdd65c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7dcb0c894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7dcb0c926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 08:46:14.605110442 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74e27d4c9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74e2327cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74e2327cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x74e2327cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x74e2327d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74e2327d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74e27d6305c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74e27e094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74e27e126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 08:46:14.608511237 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7edd60c84446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7edd15fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7edd15fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7edd15fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7edd15fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7edd15fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7edd60deb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7edd61894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7edd61926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:46:14,790 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_0.npy
2025-04-14 08:46:14,790 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 134 negative comparisons
2025-04-14 08:46:17,106 - __main__ - INFO - Node 2: Gradient computation completed successfully
2025-04-14 08:46:17,618 - __main__ - INFO - Node 0: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 0
Worker node 2 successfully completed gradient computation for iteration 0
Waiting for worker nodes to complete gradient computation...
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 08:46:35,171 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 08:46:35,721 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:46:35,861 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:46:35,873 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 08:46:52,385 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 08:46:55,295 - __main__ - INFO - Loaded gradients from node 0: 134 negative comparisons
2025-04-14 08:46:55,531 - __main__ - INFO - Loaded gradients from node 1: 89 negative comparisons
2025-04-14 08:46:55,768 - __main__ - INFO - Loaded gradients from node 2: 132 negative comparisons
2025-04-14 08:46:55,929 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 08:46:56,122 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 08:46:56,315 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119138431
2025-04-14 08:46:56,508 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107358019
2025-04-14 08:46:56,701 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84836424
2025-04-14 08:46:56,894 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47199459
2025-04-14 08:46:57,088 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22253452
2025-04-14 08:46:57,282 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806970
2025-04-14 08:46:57,476 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550041
2025-04-14 08:46:57,671 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553547
2025-04-14 08:46:58,024 - __main__ - INFO - Non-zero entries after thresholding: 47199459
2025-04-14 08:46:58,024 - __main__ - INFO - Using relaxed comparison (count_negative: 355)
2025-04-14 08:46:58,024 - __main__ - INFO - Testing stepsize: 0
2025-04-14 08:46:58,662 - __main__ - INFO -   chosen_logps: -89.95923, rejected_logps: -91.92723
2025-04-14 08:46:58,662 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 08:46:58,932 - __main__ - INFO -   chosen_logps: -89.57205, rejected_logps: -92.16581
2025-04-14 08:46:58,932 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 08:46:59,200 - __main__ - INFO -   chosen_logps: -88.81917, rejected_logps: -92.94940
2025-04-14 08:46:59,200 - __main__ - INFO - Testing stepsize: 1
2025-04-14 08:46:59,473 - __main__ - INFO -   chosen_logps: -87.58289, rejected_logps: -93.25357
2025-04-14 08:46:59,473 - __main__ - INFO - Testing stepsize: 2
2025-04-14 08:46:59,748 - __main__ - INFO -   chosen_logps: -85.45917, rejected_logps: -94.67184
2025-04-14 08:46:59,748 - __main__ - INFO - Testing stepsize: 5
2025-04-14 08:47:00,025 - __main__ - INFO -   chosen_logps: -79.25677, rejected_logps: -98.30971
2025-04-14 08:47:00,025 - __main__ - INFO - Testing stepsize: 10
2025-04-14 08:47:00,304 - __main__ - INFO -   chosen_logps: -69.13422, rejected_logps: -104.68018
2025-04-14 08:47:00,304 - __main__ - INFO - Testing stepsize: 15
2025-04-14 08:47:00,583 - __main__ - INFO -   chosen_logps: -60.51645, rejected_logps: -111.09663
2025-04-14 08:47:01,186 - __main__ - INFO - Update scale: 0.0069027777777777785
2025-04-14 08:47:01,188 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 08:47:01,269 - __main__ - INFO - Model weights updated successfully
2025-04-14 08:47:02,110 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:47:02,148 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:47:03,041 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 08:47:03,075 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 08:47:03,075 - __main__ - INFO - Skipping full model checkpoint for iteration 0
2025-04-14 08:47:03,075 - __main__ - INFO - Iteration 0 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 0
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 0
Iteration 0 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 1
===========================================
Iteration 1 is in valid list - performing full update
Finding noisy pairs for iteration 1...
2025-04-14 08:47:24,269 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
2025-04-14 08:47:24,820 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:47:24,964 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:47:24,979 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 08:47:24,979 - __main__ - INFO - Loading dataset
2025-04-14 08:47:26,260 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 08:47:47,693 - __main__ - INFO - Processing dataset
2025-04-14 08:47:47,938 - __main__ - INFO - Searching for noisy pairs starting from index 338
2025-04-14 08:47:54,684 - __main__ - INFO - Noisy pair found at index 369: chosen_logps=-153.0000, rejected_logps=-154.0000
2025-04-14 08:47:54,685 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 08:47:54,752 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_1
2025-04-14 08:47:54,752 - __main__ - INFO - New dataset offset: 370
Updated dataset offset to: 370 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 1
Worker node 2 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 08:48:09,423 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 08:48:09,461 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 08:48:09,967 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:48:10,016 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:48:10,025 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 08:48:10,121 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:48:10,135 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 08:48:10,135 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:48:10,157 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:48:10,170 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 08:48:10,170 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.23it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.54it/s]
2025-04-14 08:48:10,577 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:48:10,722 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:48:10,737 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 08:48:10,737 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:48:26,740 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 08:48:26,754 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 08:48:27,278 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 08:48:36,812 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 08:48:36,861 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 08:48:37,587 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 08:48:40,646 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 08:48:40,763 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 08:48:41,323 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 08:48:43,815 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 08:48:44,126 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 08:48:44,244 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 08:48:44,642 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 08:48:44,734 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 08:48:45,553 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 08:48:47,465 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 08:48:47,584 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 08:48:48,162 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 08:48:48,427 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 08:48:48,788 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 08:48:49,483 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 08:48:51,581 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 08:48:51,657 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 08:48:52,618 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 08:48:52,706 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 08:48:52,863 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 08:48:53,648 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 08:48:55,444 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 08:48:56,466 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 08:48:56,792 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 08:48:56,831 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 08:48:56,935 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 08:48:57,888 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 08:48:59,892 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 08:48:59,923 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 08:49:00,788 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 08:49:00,793 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 08:49:01,081 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 08:49:02,222 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 08:49:03,767 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 08:49:04,610 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 08:49:05,228 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 08:49:05,363 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 08:49:05,534 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 08:49:06,617 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 08:49:07,763 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 08:49:08,235 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 08:49:09,475 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 08:49:09,695 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 08:49:09,704 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 08:49:09,757 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 08:49:09,767 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 08:49:10,647 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 08:49:10,649 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 08:49:11,829 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 08:49:13,327 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 08:49:14,075 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 08:49:16,983 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 08:49:17,028 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 08:49:17,222 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 08:49:17,545 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 08:49:17,927 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 08:49:17,939 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 08:50:34,073 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 08:50:35,289 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 08:50:37,792 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 08:50:40,363 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 08:50:40,521 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 08:50:41,513 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 08:50:41,696 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 08:50:42,395 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 08:50:46,118 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 08:50:47,005 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 08:50:47,643 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 08:50:49,622 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 08:50:50,120 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 08:50:50,221 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 08:50:52,020 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 08:50:53,063 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 08:50:55,770 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 08:50:55,865 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 08:50:57,626 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 08:51:00,393 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 08:51:00,627 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 08:51:02,105 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 08:51:03,199 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 08:51:06,165 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 08:51:07,141 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 08:51:07,786 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 08:51:07,882 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 08:51:07,976 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 08:51:10,749 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 08:51:14,360 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
[E414 08:52:55.220737970 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x727ed7a23446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x727e8cdcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x727e8cdcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x727e8cdcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x727e8cdd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x727e8cdd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x727ed7b8a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x727ed8494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x727ed8526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 08:52:55.243898742 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77408d787446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x774042bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x774042bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x774042bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x774042bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x774042bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x77408d8ee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x77408e294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x77408e326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 08:52:55,528 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_0.npy
2025-04-14 08:52:55,528 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 107 negative comparisons
2025-04-14 08:52:58,638 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 08:53:01,207 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_2.npy
2025-04-14 08:53:01,207 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 128 negative comparisons
2025-04-14 08:53:04,233 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
2025-04-14 08:53:10,082 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_1.npy
2025-04-14 08:53:10,082 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 115 negative comparisons
Worker node 2 successfully completed gradient computation for iteration 1
2025-04-14 08:53:12,833 - __main__ - INFO - Node 1: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 1
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 08:53:30,881 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 08:53:31,429 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:53:31,570 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:53:31,583 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 08:53:48,038 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 08:53:50,967 - __main__ - INFO - Loaded gradients from node 0: 107 negative comparisons
2025-04-14 08:53:51,206 - __main__ - INFO - Loaded gradients from node 1: 115 negative comparisons
2025-04-14 08:53:51,447 - __main__ - INFO - Loaded gradients from node 2: 128 negative comparisons
2025-04-14 08:53:51,609 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 08:53:51,803 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 08:53:51,997 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119131489
2025-04-14 08:53:52,191 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352613
2025-04-14 08:53:52,385 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84845224
2025-04-14 08:53:52,579 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47203606
2025-04-14 08:53:52,773 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22259882
2025-04-14 08:53:52,969 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8800520
2025-04-14 08:53:53,164 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1549819
2025-04-14 08:53:53,361 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553969
2025-04-14 08:53:53,715 - __main__ - INFO - Non-zero entries after thresholding: 47203606
2025-04-14 08:53:53,715 - __main__ - INFO - Using relaxed comparison (count_negative: 350)
2025-04-14 08:53:53,715 - __main__ - INFO - Testing stepsize: 0
2025-04-14 08:53:54,351 - __main__ - INFO -   chosen_logps: -152.85396, rejected_logps: -153.87022
2025-04-14 08:53:54,351 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 08:53:54,616 - __main__ - INFO -   chosen_logps: -152.60252, rejected_logps: -153.82237
2025-04-14 08:53:54,616 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 08:53:54,881 - __main__ - INFO -   chosen_logps: -152.21449, rejected_logps: -154.19562
2025-04-14 08:53:54,881 - __main__ - INFO - Testing stepsize: 1
2025-04-14 08:53:55,148 - __main__ - INFO -   chosen_logps: -151.39810, rejected_logps: -154.42511
2025-04-14 08:53:55,148 - __main__ - INFO - Testing stepsize: 2
2025-04-14 08:53:55,419 - __main__ - INFO -   chosen_logps: -149.99898, rejected_logps: -155.05972
2025-04-14 08:53:55,419 - __main__ - INFO - Testing stepsize: 5
2025-04-14 08:53:55,690 - __main__ - INFO -   chosen_logps: -146.53058, rejected_logps: -157.16837
2025-04-14 08:53:55,690 - __main__ - INFO - Testing stepsize: 10
2025-04-14 08:53:55,964 - __main__ - INFO -   chosen_logps: -140.86395, rejected_logps: -160.86180
2025-04-14 08:53:55,964 - __main__ - INFO - Testing stepsize: 15
2025-04-14 08:53:56,239 - __main__ - INFO -   chosen_logps: -136.01993, rejected_logps: -164.19418
2025-04-14 08:53:56,837 - __main__ - INFO - Update scale: 0.006805555555555556
2025-04-14 08:53:56,838 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 08:53:56,919 - __main__ - INFO - Model weights updated successfully
2025-04-14 08:53:57,755 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:53:57,791 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:53:58,674 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 08:53:58,706 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 08:53:58,706 - __main__ - INFO - Skipping full model checkpoint for iteration 1
2025-04-14 08:53:58,706 - __main__ - INFO - Iteration 1 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 1
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 1
Iteration 1 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 2
===========================================
Iteration 2 is in valid list - performing full update
Finding noisy pairs for iteration 2...
2025-04-14 08:54:20,327 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.38it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.73it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
2025-04-14 08:54:20,871 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:54:21,018 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 08:54:21,032 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 08:54:21,032 - __main__ - INFO - Loading dataset
2025-04-14 08:54:22,792 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 08:54:44,114 - __main__ - INFO - Processing dataset
2025-04-14 08:54:44,356 - __main__ - INFO - Searching for noisy pairs starting from index 370
2025-04-14 08:54:50,514 - __main__ - INFO - Processed 30 samples without finding a noisy pair
2025-04-14 08:55:11,067 - __main__ - INFO - Processed 130 samples without finding a noisy pair
2025-04-14 08:55:32,580 - __main__ - INFO - Processed 230 samples without finding a noisy pair
2025-04-14 08:55:51,934 - __main__ - INFO - Processed 330 samples without finding a noisy pair
2025-04-14 08:56:12,212 - __main__ - INFO - Processed 430 samples without finding a noisy pair
2025-04-14 08:56:31,879 - __main__ - INFO - Processed 530 samples without finding a noisy pair
2025-04-14 08:56:50,548 - __main__ - INFO - Processed 630 samples without finding a noisy pair
2025-04-14 08:56:56,031 - __main__ - INFO - Noisy pair found at index 1030: chosen_logps=-280.0000, rejected_logps=-282.0000
2025-04-14 08:56:56,032 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 08:56:56,098 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_2
2025-04-14 08:56:56,099 - __main__ - INFO - New dataset offset: 1031
Updated dataset offset to: 1031 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 2
Worker node 2 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 08:57:11,815 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 08:57:11,971 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.24it/s]2025-04-14 08:57:12,360 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:57:12,379 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 08:57:12,512 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:57:12,527 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 08:57:12,527 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:57:12,526 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]2025-04-14 08:57:12,669 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:57:12,681 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 08:57:12,681 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.54it/s]
2025-04-14 08:57:12,929 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 08:57:13,073 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 08:57:13,088 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 08:57:13,088 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 08:57:29,103 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 08:57:29,199 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 08:57:29,693 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 08:57:39,212 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 08:57:39,345 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 08:57:40,159 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 08:57:43,025 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 08:57:43,156 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 08:57:43,964 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 08:57:45,923 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 08:57:46,191 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 08:57:47,084 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 08:57:47,131 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 08:57:47,394 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 08:57:47,921 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 08:57:50,136 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 08:57:50,722 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 08:57:50,857 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 08:57:51,122 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 08:57:51,651 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 08:57:51,833 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 08:57:54,163 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 08:57:54,447 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 08:57:54,626 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 08:57:55,213 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 08:57:55,754 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 08:57:55,916 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 08:57:57,720 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 08:57:58,446 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 08:57:59,012 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 08:57:59,031 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 08:57:59,948 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 08:57:59,957 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 08:58:01,534 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 08:58:02,375 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 08:58:02,968 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 08:58:03,087 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 08:58:04,041 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 08:58:04,334 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 08:58:05,563 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 08:58:06,383 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 08:58:07,082 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 08:58:07,155 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 08:58:08,416 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 08:58:08,831 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 08:58:09,330 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 08:58:10,540 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 08:58:10,542 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 08:58:11,236 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 08:58:11,694 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 08:58:12,426 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 08:58:12,432 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 08:58:13,383 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 08:58:13,391 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 08:58:13,470 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 08:58:15,711 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 08:58:16,297 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 08:58:17,894 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 08:58:18,029 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 08:58:19,975 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 08:58:20,018 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 08:58:20,764 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 08:58:20,920 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 08:59:35,680 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 08:59:36,071 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 08:59:38,461 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 08:59:38,986 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 08:59:41,948 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 08:59:42,939 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 08:59:43,288 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 08:59:43,481 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 08:59:46,062 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 08:59:47,661 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 08:59:48,772 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 08:59:49,831 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 08:59:51,316 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 08:59:52,584 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 08:59:56,069 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 08:59:56,113 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 08:59:56,898 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 08:59:58,751 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 08:59:58,993 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 08:59:59,994 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 09:00:00,356 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 09:00:04,253 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 09:00:05,469 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 09:00:07,371 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 09:00:08,272 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 09:00:08,349 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 09:00:08,970 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 09:00:09,239 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 09:00:09,960 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 09:00:13,017 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 09:01:55,945 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_1.npy
2025-04-14 09:01:55,945 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 116 negative comparisons
2025-04-14 09:01:56,105 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_0.npy
2025-04-14 09:01:56,105 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 120 negative comparisons
2025-04-14 09:01:58,806 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 09:01:59,254 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 09:02:03.897788544 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x767ce88fb446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x767c9dbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x767c9dbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x767c9dbcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x767c9dbd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x767c9dbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x767ce8a625c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x767ce9494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x767ce9526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:02:03,223 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_2.npy
2025-04-14 09:02:03,224 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 136 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 2
Waiting for worker nodes to complete gradient computation...
2025-04-14 09:02:06,077 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 2
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 09:02:23,226 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 09:02:23,775 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:02:23,915 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:02:23,927 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 09:02:40,512 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 09:02:43,444 - __main__ - INFO - Loaded gradients from node 0: 120 negative comparisons
2025-04-14 09:02:43,684 - __main__ - INFO - Loaded gradients from node 1: 116 negative comparisons
2025-04-14 09:02:43,923 - __main__ - INFO - Loaded gradients from node 2: 136 negative comparisons
2025-04-14 09:02:44,088 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 09:02:44,282 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 09:02:44,475 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119134060
2025-04-14 09:02:44,669 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107351672
2025-04-14 09:02:44,863 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84836038
2025-04-14 09:02:45,057 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47193315
2025-04-14 09:02:45,251 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22253864
2025-04-14 09:02:45,445 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803090
2025-04-14 09:02:45,639 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551270
2025-04-14 09:02:45,834 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555631
2025-04-14 09:02:46,185 - __main__ - INFO - Non-zero entries after thresholding: 47193315
2025-04-14 09:02:46,185 - __main__ - INFO - Testing stepsize: 0
2025-04-14 09:02:46,808 - __main__ - INFO -   chosen_logps: -279.81625, rejected_logps: -281.87357
2025-04-14 09:02:46,808 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 09:02:47,061 - __main__ - INFO -   chosen_logps: -279.57111, rejected_logps: -281.89569
2025-04-14 09:02:47,061 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 09:02:47,314 - __main__ - INFO -   chosen_logps: -277.77649, rejected_logps: -282.95740
2025-04-14 09:02:47,315 - __main__ - INFO - Testing stepsize: 1
2025-04-14 09:02:47,570 - __main__ - INFO -   chosen_logps: -275.76730, rejected_logps: -283.87775
2025-04-14 09:02:47,571 - __main__ - INFO - Testing stepsize: 2
2025-04-14 09:02:47,831 - __main__ - INFO -   chosen_logps: -272.42468, rejected_logps: -286.21561
2025-04-14 09:02:47,831 - __main__ - INFO - Testing stepsize: 5
2025-04-14 09:02:48,093 - __main__ - INFO -   chosen_logps: -260.77185, rejected_logps: -292.39127
2025-04-14 09:02:48,093 - __main__ - INFO - Testing stepsize: 10
2025-04-14 09:02:48,356 - __main__ - INFO -   chosen_logps: -244.28488, rejected_logps: -304.71204
2025-04-14 09:02:48,356 - __main__ - INFO - Testing stepsize: 15
2025-04-14 09:02:48,620 - __main__ - INFO -   chosen_logps: -229.38541, rejected_logps: -315.98956
2025-04-14 09:02:49,202 - __main__ - INFO - Update scale: 0.007233333333333334
2025-04-14 09:02:49,284 - __main__ - INFO - Model weights updated successfully
2025-04-14 09:02:50,562 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:02:50,598 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:02:51,484 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 09:02:51,517 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 09:02:51,517 - __main__ - INFO - Skipping full model checkpoint for iteration 2
2025-04-14 09:02:51,517 - __main__ - INFO - Iteration 2 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 2
Worker node 2 successfully applied weights for iteration 2
Iteration 2 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 3
===========================================
Iteration 3 is in valid list - performing full update
Finding noisy pairs for iteration 3...
2025-04-14 09:03:12,499 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
2025-04-14 09:03:13,045 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:03:13,186 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:03:13,199 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 09:03:13,199 - __main__ - INFO - Loading dataset
2025-04-14 09:03:14,406 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 09:03:35,977 - __main__ - INFO - Processing dataset
2025-04-14 09:03:36,223 - __main__ - INFO - Searching for noisy pairs starting from index 1031
2025-04-14 09:03:51,167 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 09:04:04,786 - __main__ - INFO - Noisy pair found at index 1163: chosen_logps=-147.0000, rejected_logps=-149.0000
2025-04-14 09:04:04,788 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 09:04:04,855 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_3
2025-04-14 09:04:04,856 - __main__ - INFO - New dataset offset: 1164
Updated dataset offset to: 1164 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 3
Worker node 2 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 09:04:20,556 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 09:04:20,585 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 09:04:21,048 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 09:04:21,113 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:04:21,129 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.38it/s]2025-04-14 09:04:21,253 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:04:21,266 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 09:04:21,266 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:04:21,282 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:04:21,297 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 09:04:21,297 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
2025-04-14 09:04:21,594 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:04:21,738 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:04:21,752 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 09:04:21,752 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:04:37,811 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 09:04:37,984 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 09:04:38,370 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 09:04:47,798 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 09:04:48,019 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 09:04:48,662 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 09:04:51,543 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 09:04:51,853 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 09:04:52,478 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 09:04:55,002 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 09:04:55,046 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 09:04:55,390 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 09:04:55,760 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 09:04:56,034 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 09:04:56,436 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 09:04:59,167 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 09:04:59,261 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 09:04:59,590 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 09:04:59,632 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 09:04:59,992 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 09:05:00,471 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 09:05:02,709 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 09:05:03,174 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 09:05:03,210 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 09:05:03,506 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 09:05:03,863 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 09:05:04,507 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 09:05:06,790 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 09:05:07,030 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 09:05:07,086 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 09:05:07,467 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 09:05:07,923 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 09:05:08,418 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 09:05:10,468 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 09:05:11,022 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 09:05:11,289 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 09:05:11,344 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 09:05:11,925 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 09:05:12,899 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 09:05:14,238 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 09:05:14,911 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 09:05:15,523 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 09:05:15,621 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 09:05:15,992 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 09:05:17,085 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 09:05:18,621 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 09:05:18,822 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 09:05:19,818 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 09:05:19,822 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 09:05:20,325 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 09:05:20,329 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 09:05:20,448 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 09:05:21,411 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 09:05:21,421 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 09:05:22,919 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 09:05:23,253 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 09:05:24,873 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 09:05:27,629 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 09:05:27,773 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 09:05:27,804 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 09:05:27,880 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 09:05:28,747 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 09:05:29,146 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 09:06:51,957 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 09:06:53,816 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 09:06:55,964 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 09:06:57,832 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 09:06:59,083 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 09:06:59,629 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 09:07:00,845 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 09:07:03,677 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 09:07:04,101 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 09:07:04,264 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 09:07:05,439 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 09:07:05,588 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 09:07:07,126 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 09:07:08,939 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 09:07:09,513 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 09:07:10,507 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 09:07:11,996 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 09:07:14,326 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 09:07:14,512 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 09:07:17,396 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 09:07:18,258 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 09:07:19,738 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 09:07:23,260 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 09:07:24,062 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 09:07:24,269 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 09:07:24,354 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 09:07:25,332 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 09:07:25,505 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 09:07:29,345 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 09:07:29,780 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
[E414 09:09:18.389128089 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ccd8ebce446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ccd43fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ccd43fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7ccd43fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7ccd43fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ccd43fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7ccd8ed355c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7ccd8f694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7ccd8f726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:09:18,647 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_1.npy
2025-04-14 09:09:18,647 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 147 negative comparisons
2025-04-14 09:09:21,565 - __main__ - INFO - Node 1: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 3
[E414 09:09:27.740853470 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c131c4ea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c12d17cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c12d17cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7c12d17cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7c12d17d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c12d17d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c131c6515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c131d094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c131d126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 09:09:27.746499394 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a72b1fe8446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7a72673cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7a72673cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7a72673cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7a72673d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a72673d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7a72b214f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7a72b2c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7a72b2d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:09:28,039 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_2.npy
2025-04-14 09:09:28,040 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 140 negative comparisons
2025-04-14 09:09:31,232 - __main__ - INFO - Node 2: Gradient computation completed successfully
[E414 09:09:31.539469900 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x716ea95b2446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x716e5e9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x716e5e9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x716e5e9cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x716e5e9d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x716e5e9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x716ea97195c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x716eaa094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x716eaa126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:09:31,862 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_0.npy
2025-04-14 09:09:31,863 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 149 negative comparisons
2025-04-14 09:09:34,857 - __main__ - INFO - Node 0: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 3
Waiting for worker nodes to complete gradient computation...
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 09:09:53,344 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.31it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.73it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
2025-04-14 09:09:53,889 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:09:54,029 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:09:54,041 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 09:10:10,521 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 09:10:13,434 - __main__ - INFO - Loaded gradients from node 0: 149 negative comparisons
2025-04-14 09:10:13,665 - __main__ - INFO - Loaded gradients from node 1: 147 negative comparisons
2025-04-14 09:10:13,894 - __main__ - INFO - Loaded gradients from node 2: 140 negative comparisons
2025-04-14 09:10:14,052 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 09:10:14,238 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 09:10:14,424 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119135292
2025-04-14 09:10:14,611 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107354319
2025-04-14 09:10:14,797 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84842569
2025-04-14 09:10:14,984 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47190818
2025-04-14 09:10:15,170 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22254995
2025-04-14 09:10:15,358 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802471
2025-04-14 09:10:15,544 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551566
2025-04-14 09:10:15,731 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555115
2025-04-14 09:10:16,072 - __main__ - INFO - Non-zero entries after thresholding: 47190818
2025-04-14 09:10:16,073 - __main__ - INFO - Testing stepsize: 0
2025-04-14 09:10:16,813 - __main__ - INFO -   chosen_logps: -147.82397, rejected_logps: -147.74109
2025-04-14 09:10:16,813 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 09:10:17,192 - __main__ - INFO -   chosen_logps: -147.51846, rejected_logps: -147.88126
2025-04-14 09:10:17,192 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 09:10:17,572 - __main__ - INFO -   chosen_logps: -146.05266, rejected_logps: -148.43796
2025-04-14 09:10:17,572 - __main__ - INFO - Testing stepsize: 1
2025-04-14 09:10:17,955 - __main__ - INFO -   chosen_logps: -144.45135, rejected_logps: -148.90665
2025-04-14 09:10:17,955 - __main__ - INFO - Testing stepsize: 2
2025-04-14 09:10:18,341 - __main__ - INFO -   chosen_logps: -141.49242, rejected_logps: -150.02417
2025-04-14 09:10:18,341 - __main__ - INFO - Testing stepsize: 5
2025-04-14 09:10:18,727 - __main__ - INFO -   chosen_logps: -132.99139, rejected_logps: -153.50296
2025-04-14 09:10:18,727 - __main__ - INFO - Testing stepsize: 10
2025-04-14 09:10:19,115 - __main__ - INFO -   chosen_logps: -121.12872, rejected_logps: -159.75354
2025-04-14 09:10:19,115 - __main__ - INFO - Testing stepsize: 15
2025-04-14 09:10:19,505 - __main__ - INFO -   chosen_logps: -112.03943, rejected_logps: -165.77153
2025-04-14 09:10:20,256 - __main__ - INFO - Update scale: 0.008477777777777779
2025-04-14 09:10:20,337 - __main__ - INFO - Model weights updated successfully
2025-04-14 09:10:21,182 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:10:21,218 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:10:22,110 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 09:10:22,145 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 09:10:22,146 - __main__ - INFO - Skipping full model checkpoint for iteration 3
2025-04-14 09:10:22,146 - __main__ - INFO - Iteration 3 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 3
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 3
Iteration 3 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 4
===========================================
Iteration 4 is in valid list - performing full update
Finding noisy pairs for iteration 4...
2025-04-14 09:10:43,817 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 09:10:44,362 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:10:44,507 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:10:44,522 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 09:10:44,522 - __main__ - INFO - Loading dataset
2025-04-14 09:10:45,405 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 09:11:07,133 - __main__ - INFO - Processing dataset
2025-04-14 09:11:07,376 - __main__ - INFO - Searching for noisy pairs starting from index 1164
2025-04-14 09:11:14,317 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 09:11:35,718 - __main__ - INFO - Processed 136 samples without finding a noisy pair
2025-04-14 09:11:45,869 - __main__ - INFO - Noisy pair found at index 1344: chosen_logps=-268.0000, rejected_logps=-270.0000
2025-04-14 09:11:45,870 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 09:11:45,937 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_4
2025-04-14 09:11:45,938 - __main__ - INFO - New dataset offset: 1345
Updated dataset offset to: 1345 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 4
Worker node 1 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 09:12:01,761 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 09:12:01,789 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
2025-04-14 09:12:02,307 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:12:02,345 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:12:02,460 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 09:12:02,468 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:12:02,475 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 09:12:02,475 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:12:02,489 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:12:02,502 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 09:12:02,502 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 09:12:03,014 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:12:03,158 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:12:03,173 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 09:12:03,173 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:12:19,036 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 09:12:19,151 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 09:12:19,708 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 09:12:29,186 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 09:12:29,196 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 09:12:30,262 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 09:12:32,963 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 09:12:32,999 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 09:12:34,081 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 09:12:36,396 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 09:12:36,500 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 09:12:36,864 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 09:12:36,911 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 09:12:37,046 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 09:12:38,021 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 09:12:40,073 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 09:12:40,353 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 09:12:40,792 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 09:12:41,132 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 09:12:41,215 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 09:12:41,888 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 09:12:43,965 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 09:12:44,699 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 09:12:44,718 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 09:12:45,312 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 09:12:45,524 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 09:12:46,029 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 09:12:48,100 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 09:12:48,120 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 09:12:48,672 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 09:12:49,198 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 09:12:49,408 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 09:12:50,223 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 09:12:51,988 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 09:12:52,019 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 09:12:52,658 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 09:12:53,247 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 09:12:53,405 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 09:12:54,179 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 09:12:56,354 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 09:12:56,404 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 09:12:57,212 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 09:12:57,508 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 09:12:57,737 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 09:12:58,412 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 09:13:00,380 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 09:13:00,597 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 09:13:01,403 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 09:13:01,526 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 09:13:01,527 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 09:13:02,074 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 09:13:02,081 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 09:13:02,866 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 09:13:02,867 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 09:13:04,479 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 09:13:04,832 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 09:13:05,424 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 09:13:09,168 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 09:13:09,290 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 09:13:09,318 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 09:13:09,363 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 09:13:10,223 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 09:13:10,249 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 09:14:28,379 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 09:14:28,414 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 09:14:30,758 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 09:14:31,806 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 09:14:32,182 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 09:14:34,688 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 09:14:36,325 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 09:14:37,068 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 09:14:38,682 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 09:14:39,084 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 09:14:40,228 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 09:14:40,421 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 09:14:42,994 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 09:14:44,010 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 09:14:46,633 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 09:14:47,559 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 09:14:47,685 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 09:14:51,587 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 09:14:51,961 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 09:14:52,963 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 09:14:54,210 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 09:14:56,844 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 09:14:57,830 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 09:14:58,329 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 09:14:59,994 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 09:15:00,286 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 09:15:00,291 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 09:15:01,187 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 09:15:01,217 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 09:15:03,882 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 09:16:48.718632604 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74c3aa75c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74c35fbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74c35fbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x74c35fbcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x74c35fbd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74c35fbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74c3aa8c35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74c3ab294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74c3ab326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:16:49,029 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_0.npy
2025-04-14 09:16:49,029 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 196 negative comparisons
2025-04-14 09:16:50,169 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_1.npy
2025-04-14 09:16:50,169 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 208 negative comparisons
[E414 09:16:50.970652407 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70bce3268446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70bc985cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70bc985cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x70bc985cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x70bc985d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70bc985d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70bce33cf5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70bce3e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70bce3f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:16:51,848 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 09:16:53,145 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 09:16:54,846 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_2.npy
2025-04-14 09:16:54,846 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 192 negative comparisons
2025-04-14 09:16:57,681 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 4
Worker node 2 successfully completed gradient computation for iteration 4
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 09:17:14,665 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 09:17:15,210 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:17:15,352 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:17:15,365 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 09:17:31,871 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 09:17:34,857 - __main__ - INFO - Loaded gradients from node 0: 196 negative comparisons
2025-04-14 09:17:35,096 - __main__ - INFO - Loaded gradients from node 1: 208 negative comparisons
2025-04-14 09:17:35,334 - __main__ - INFO - Loaded gradients from node 2: 192 negative comparisons
2025-04-14 09:17:35,499 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 09:17:35,694 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 09:17:35,889 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133524
2025-04-14 09:17:36,084 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107350169
2025-04-14 09:17:36,282 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84837014
2025-04-14 09:17:36,479 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47194871
2025-04-14 09:17:36,676 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255963
2025-04-14 09:17:36,872 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804262
2025-04-14 09:17:37,068 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551412
2025-04-14 09:17:37,264 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554170
2025-04-14 09:17:37,617 - __main__ - INFO - Non-zero entries after thresholding: 47194871
2025-04-14 09:17:37,617 - __main__ - INFO - Testing stepsize: 0
2025-04-14 09:17:38,281 - __main__ - INFO -   chosen_logps: -267.78009, rejected_logps: -269.53983
2025-04-14 09:17:38,281 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 09:17:38,571 - __main__ - INFO -   chosen_logps: -267.45529, rejected_logps: -269.94855
2025-04-14 09:17:38,571 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 09:17:38,860 - __main__ - INFO -   chosen_logps: -265.50061, rejected_logps: -271.37970
2025-04-14 09:17:38,860 - __main__ - INFO - Testing stepsize: 1
2025-04-14 09:17:39,154 - __main__ - INFO -   chosen_logps: -263.22314, rejected_logps: -273.72885
2025-04-14 09:17:39,154 - __main__ - INFO - Testing stepsize: 2
2025-04-14 09:17:39,450 - __main__ - INFO -   chosen_logps: -258.87308, rejected_logps: -276.53214
2025-04-14 09:17:39,450 - __main__ - INFO - Testing stepsize: 5
2025-04-14 09:17:39,749 - __main__ - INFO -   chosen_logps: -246.93225, rejected_logps: -287.54614
2025-04-14 09:17:39,749 - __main__ - INFO - Testing stepsize: 10
2025-04-14 09:17:40,049 - __main__ - INFO -   chosen_logps: -228.34360, rejected_logps: -308.78131
2025-04-14 09:17:40,049 - __main__ - INFO - Testing stepsize: 15
2025-04-14 09:17:40,349 - __main__ - INFO -   chosen_logps: -211.83455, rejected_logps: -333.27927
2025-04-14 09:17:40,990 - __main__ - INFO - Update scale: 0.011588888888888892
2025-04-14 09:17:41,074 - __main__ - INFO - Model weights updated successfully
2025-04-14 09:17:41,918 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:17:41,952 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:17:43,207 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 09:17:43,243 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 09:17:43,243 - __main__ - INFO - Skipping full model checkpoint for iteration 4
2025-04-14 09:17:43,243 - __main__ - INFO - Iteration 4 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 4
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 4
Iteration 4 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 5
===========================================
Iteration 5 is in valid list - performing full update
Finding noisy pairs for iteration 5...
2025-04-14 09:18:05,085 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 09:18:05,633 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:18:05,778 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:18:05,793 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 09:18:05,793 - __main__ - INFO - Loading dataset
2025-04-14 09:18:07,074 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 09:18:28,688 - __main__ - INFO - Processing dataset
2025-04-14 09:18:28,938 - __main__ - INFO - Searching for noisy pairs starting from index 1345
2025-04-14 09:18:40,157 - __main__ - INFO - Processed 55 samples without finding a noisy pair
2025-04-14 09:19:00,836 - __main__ - INFO - Processed 155 samples without finding a noisy pair
2025-04-14 09:19:06,290 - __main__ - INFO - Noisy pair found at index 1530: chosen_logps=-360.0000, rejected_logps=-362.0000
2025-04-14 09:19:06,291 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 09:19:06,359 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_5
2025-04-14 09:19:06,359 - __main__ - INFO - New dataset offset: 1531
Updated dataset offset to: 1531 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 5
Worker node 2 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 09:19:22,076 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 09:19:22,232 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
2025-04-14 09:19:22,537 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 09:19:22,620 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.37it/s]2025-04-14 09:19:22,775 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:19:22,791 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 09:19:22,791 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:19:22,790 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]2025-04-14 09:19:22,930 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:19:22,943 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 09:19:22,943 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
2025-04-14 09:19:23,083 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:19:23,227 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:19:23,242 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 09:19:23,242 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:19:39,370 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 09:19:39,528 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 09:19:39,853 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 09:19:49,667 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 09:19:49,687 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 09:19:50,496 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 09:19:53,459 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 09:19:53,499 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 09:19:54,304 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 09:19:56,547 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 09:19:56,600 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 09:19:57,203 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 09:19:57,396 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 09:19:57,428 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 09:19:58,768 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 09:20:00,480 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 09:20:01,231 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 09:20:01,270 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 09:20:01,414 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 09:20:01,608 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 09:20:02,705 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 09:20:04,346 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 09:20:04,816 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 09:20:05,085 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 09:20:05,388 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 09:20:05,935 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 09:20:06,996 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 09:20:08,621 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 09:20:08,799 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 09:20:09,005 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 09:20:09,323 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 09:20:09,978 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 09:20:10,999 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 09:20:12,123 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 09:20:12,715 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 09:20:12,975 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 09:20:13,388 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 09:20:14,179 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 09:20:14,948 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 09:20:16,007 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 09:20:16,517 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 09:20:17,067 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 09:20:18,098 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 09:20:18,194 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 09:20:19,413 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 09:20:20,454 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 09:20:21,200 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 09:20:21,204 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 09:20:21,315 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 09:20:22,198 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 09:20:22,333 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 09:20:22,340 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 09:20:23,782 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 09:20:23,791 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 09:20:24,172 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 09:20:25,793 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 09:20:26,623 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 09:20:28,311 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 09:20:28,415 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 09:20:30,040 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 09:20:30,519 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 09:20:31,213 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 09:20:31,230 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 09:21:49,928 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 09:21:50,199 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 09:21:51,426 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 09:21:51,461 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 09:21:52,693 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 09:21:57,751 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 09:21:58,606 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 09:21:58,786 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 09:21:59,057 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 09:22:02,140 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 09:22:02,715 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 09:22:03,944 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 09:22:04,889 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 09:22:06,127 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 09:22:06,305 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 09:22:08,337 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 09:22:10,677 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 09:22:13,548 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 09:22:13,845 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 09:22:14,689 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 09:22:16,767 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 09:22:17,669 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 09:22:17,760 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 09:22:18,877 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 09:22:20,697 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 09:22:20,804 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 09:22:22,436 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 09:22:22,749 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 09:22:23,699 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 09:22:25,888 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 09:24:11.707120672 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7aaa088a1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7aa9bdbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7aa9bdbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7aa9bdbcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7aa9bdbd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7aa9bdbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7aaa08a085c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7aaa09494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7aaa09526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 09:24:11.713575513 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74aabf609446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74aa749cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74aa749cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x74aa749cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x74aa749d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74aa749d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74aabf7705c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74aac0094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74aac0126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:24:12,040 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_1.npy
2025-04-14 09:24:12,040 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 85 negative comparisons
2025-04-14 09:24:13,071 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_0.npy
2025-04-14 09:24:13,071 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 104 negative comparisons
[E414 09:24:13.021361419 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75c4caac9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75c47fdcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75c47fdcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x75c47fdcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x75c47fdd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75c47fdd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x75c4cac305c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x75c4cb694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x75c4cb726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:24:14,869 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 09:24:16,257 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 09:24:18,525 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_2.npy
2025-04-14 09:24:18,525 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 101 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 5
2025-04-14 09:24:21,338 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 5
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 09:24:37,166 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 09:24:37,714 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:24:37,855 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:24:37,867 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 09:24:54,401 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 09:24:57,309 - __main__ - INFO - Loaded gradients from node 0: 104 negative comparisons
2025-04-14 09:24:57,544 - __main__ - INFO - Loaded gradients from node 1: 85 negative comparisons
2025-04-14 09:24:57,779 - __main__ - INFO - Loaded gradients from node 2: 101 negative comparisons
2025-04-14 09:24:57,939 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 09:24:58,130 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 09:24:58,321 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119128047
2025-04-14 09:24:58,512 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107342984
2025-04-14 09:24:58,702 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84832126
2025-04-14 09:24:58,893 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47196940
2025-04-14 09:24:59,085 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22254514
2025-04-14 09:24:59,277 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8807469
2025-04-14 09:24:59,470 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551265
2025-04-14 09:24:59,663 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554690
2025-04-14 09:25:00,013 - __main__ - INFO - Non-zero entries after thresholding: 47196940
2025-04-14 09:25:00,013 - __main__ - INFO - Using relaxed comparison (count_negative: 290)
2025-04-14 09:25:00,013 - __main__ - INFO - Testing stepsize: 0
2025-04-14 09:25:00,687 - __main__ - INFO -   chosen_logps: -360.31842, rejected_logps: -361.51324
2025-04-14 09:25:00,687 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 09:25:00,989 - __main__ - INFO -   chosen_logps: -359.79980, rejected_logps: -361.53949
2025-04-14 09:25:00,989 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 09:25:01,291 - __main__ - INFO -   chosen_logps: -358.19931, rejected_logps: -362.86523
2025-04-14 09:25:01,291 - __main__ - INFO - Testing stepsize: 1
2025-04-14 09:25:01,596 - __main__ - INFO -   chosen_logps: -355.53674, rejected_logps: -364.07104
2025-04-14 09:25:01,596 - __main__ - INFO - Testing stepsize: 2
2025-04-14 09:25:01,905 - __main__ - INFO -   chosen_logps: -350.28638, rejected_logps: -366.47183
2025-04-14 09:25:01,906 - __main__ - INFO - Testing stepsize: 5
2025-04-14 09:25:02,216 - __main__ - INFO -   chosen_logps: -335.74277, rejected_logps: -374.44165
2025-04-14 09:25:02,216 - __main__ - INFO - Testing stepsize: 10
2025-04-14 09:25:02,528 - __main__ - INFO -   chosen_logps: -314.56326, rejected_logps: -390.11224
2025-04-14 09:25:02,528 - __main__ - INFO - Testing stepsize: 15
2025-04-14 09:25:02,841 - __main__ - INFO -   chosen_logps: -297.17737, rejected_logps: -407.84534
2025-04-14 09:25:03,494 - __main__ - INFO - Update scale: 0.0056388888888888895
2025-04-14 09:25:03,495 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 09:25:03,575 - __main__ - INFO - Model weights updated successfully
2025-04-14 09:25:04,422 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:25:04,458 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:25:05,584 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 09:25:05,619 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 09:25:05,619 - __main__ - INFO - Skipping full model checkpoint for iteration 5
2025-04-14 09:25:05,619 - __main__ - INFO - Iteration 5 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 5
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 5
Iteration 5 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 6
===========================================
Iteration 6 is in valid list - performing full update
Finding noisy pairs for iteration 6...
2025-04-14 09:25:27,288 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
2025-04-14 09:25:27,836 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:25:27,981 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:25:27,996 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 09:25:27,996 - __main__ - INFO - Loading dataset
2025-04-14 09:25:28,793 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 09:25:50,584 - __main__ - INFO - Processing dataset
2025-04-14 09:25:50,832 - __main__ - INFO - Searching for noisy pairs starting from index 1531
2025-04-14 09:26:04,920 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 09:26:09,477 - __main__ - INFO - Noisy pair found at index 1624: chosen_logps=-127.5000, rejected_logps=-130.0000
2025-04-14 09:26:09,478 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 09:26:09,545 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_6
2025-04-14 09:26:09,546 - __main__ - INFO - New dataset offset: 1625
Updated dataset offset to: 1625 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 6
Worker node 2 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 09:26:25,156 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 09:26:25,194 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
2025-04-14 09:26:25,701 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:26:25,738 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:26:25,841 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:26:25,854 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 09:26:25,854 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:26:25,892 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:26:25,907 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 09:26:25,907 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:26:25,974 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.17it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]
2025-04-14 09:26:26,533 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:26:26,677 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:26:26,692 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 09:26:26,692 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:26:42,442 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 09:26:42,451 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 09:26:43,305 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 09:26:52,548 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 09:26:52,636 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 09:26:53,648 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 09:26:56,331 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 09:26:56,456 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 09:26:57,845 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 09:26:59,872 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 09:27:00,012 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 09:27:00,251 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 09:27:00,320 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 09:27:00,872 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 09:27:01,826 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 09:27:03,719 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 09:27:03,770 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 09:27:04,038 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 09:27:04,165 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 09:27:05,182 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 09:27:06,073 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 09:27:07,490 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 09:27:07,661 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 09:27:07,988 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 09:27:08,145 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 09:27:09,191 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 09:27:09,971 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 09:27:11,336 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 09:27:11,667 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 09:27:12,044 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 09:27:12,205 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 09:27:13,434 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 09:27:13,903 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 09:27:15,454 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 09:27:15,770 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 09:27:16,147 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 09:27:16,286 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 09:27:17,725 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 09:27:17,981 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 09:27:19,537 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 09:27:19,565 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 09:27:20,374 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 09:27:20,437 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 09:27:21,282 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 09:27:22,032 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 09:27:23,336 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 09:27:23,722 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 09:27:24,457 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 09:27:24,462 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 09:27:24,832 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 09:27:24,834 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 09:27:25,494 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 09:27:26,083 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 09:27:26,092 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 09:27:27,610 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 09:27:27,927 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 09:27:29,522 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 09:27:31,782 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 09:27:31,824 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 09:27:32,343 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 09:27:32,584 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 09:27:33,621 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 09:27:33,643 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 09:28:56,366 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 09:28:56,593 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 09:29:00,186 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 09:29:00,629 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 09:29:02,848 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 09:29:03,437 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 09:29:03,671 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 09:29:04,339 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 09:29:06,451 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 09:29:08,626 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 09:29:09,443 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 09:29:11,425 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 09:29:13,037 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 09:29:14,127 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 09:29:14,912 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 09:29:15,934 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 09:29:17,306 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 09:29:20,556 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 09:29:21,291 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 09:29:22,696 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 09:29:22,808 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 09:29:22,917 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 09:29:24,038 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 09:29:27,145 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 09:29:27,174 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 09:29:27,651 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 09:29:28,783 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 09:29:28,982 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 09:29:30,655 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 09:29:31,528 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 09:31:23.353741168 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a61bf1dc446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7a61745cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7a61745cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7a61745cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7a61745d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a61745d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7a61bf3435c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7a61bfc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7a61bfd26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 09:31:23.359474264 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c574b6d7446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c57009cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c57009cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7c57009cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7c57009d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c57009d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c574b83e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c574c294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c574c326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:31:23,601 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_0.npy
2025-04-14 09:31:23,601 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 158 negative comparisons
[E414 09:31:25.465909562 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7009d55fb446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70098a9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70098a9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x70098a9cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x70098a9d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70098a9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7009d57625c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7009d6094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7009d6126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 09:31:25.475722066 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x791f08109446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x791ebd3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x791ebd3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x791ebd3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x791ebd3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x791ebd3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x791f082705c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x791f08c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x791f08d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 09:31:25.474700718 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70c37c676446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70c3319cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70c3319cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x70c3319cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x70c3319d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70c3319d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70c37c7dd5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70c37d294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70c37d326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:31:25,691 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_1.npy
2025-04-14 09:31:25,692 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 159 negative comparisons
2025-04-14 09:31:26,383 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 09:31:26.384335525 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x764418c57446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7643cdfcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7643cdfcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7643cdfcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7643cdfd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7643cdfd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x764418dbe5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x764419894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x764419926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 09:31:26.381179580 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7baa93b96446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7baa48fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7baa48fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7baa48fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7baa48fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7baa48fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7baa93cfd5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7baa94894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7baa94926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:31:26,739 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_2.npy
2025-04-14 09:31:26,739 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 153 negative comparisons
2025-04-14 09:31:28,540 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 09:31:29,584 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 6
Worker node 2 successfully completed gradient computation for iteration 6
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 09:31:48,328 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  4.83it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  5.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.64it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.51it/s]
2025-04-14 09:31:49,062 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:31:49,207 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:31:49,222 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 09:32:05,651 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 09:32:08,533 - __main__ - INFO - Loaded gradients from node 0: 158 negative comparisons
2025-04-14 09:32:08,764 - __main__ - INFO - Loaded gradients from node 1: 159 negative comparisons
2025-04-14 09:32:08,995 - __main__ - INFO - Loaded gradients from node 2: 153 negative comparisons
2025-04-14 09:32:09,153 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 09:32:09,341 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 09:32:09,528 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119132292
2025-04-14 09:32:09,715 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107349575
2025-04-14 09:32:09,903 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84839396
2025-04-14 09:32:10,090 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47195333
2025-04-14 09:32:10,277 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22258292
2025-04-14 09:32:10,465 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803907
2025-04-14 09:32:10,653 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550664
2025-04-14 09:32:10,842 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553995
2025-04-14 09:32:11,185 - __main__ - INFO - Non-zero entries after thresholding: 47195333
2025-04-14 09:32:11,186 - __main__ - INFO - Testing stepsize: 0
2025-04-14 09:32:11,918 - __main__ - INFO -   chosen_logps: -127.93187, rejected_logps: -129.48669
2025-04-14 09:32:11,918 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 09:32:12,284 - __main__ - INFO -   chosen_logps: -127.50946, rejected_logps: -129.48637
2025-04-14 09:32:12,284 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 09:32:12,651 - __main__ - INFO -   chosen_logps: -125.63794, rejected_logps: -129.88438
2025-04-14 09:32:12,651 - __main__ - INFO - Testing stepsize: 1
2025-04-14 09:32:13,018 - __main__ - INFO -   chosen_logps: -123.78987, rejected_logps: -130.27557
2025-04-14 09:32:13,018 - __main__ - INFO - Testing stepsize: 2
2025-04-14 09:32:13,390 - __main__ - INFO -   chosen_logps: -119.91084, rejected_logps: -131.32550
2025-04-14 09:32:13,390 - __main__ - INFO - Testing stepsize: 5
2025-04-14 09:32:13,763 - __main__ - INFO -   chosen_logps: -109.49083, rejected_logps: -133.83932
2025-04-14 09:32:13,763 - __main__ - INFO - Testing stepsize: 10
2025-04-14 09:32:14,139 - __main__ - INFO -   chosen_logps: -94.88492, rejected_logps: -138.24355
2025-04-14 09:32:14,139 - __main__ - INFO - Testing stepsize: 15
2025-04-14 09:32:14,515 - __main__ - INFO -   chosen_logps: -83.63375, rejected_logps: -143.10132
2025-04-14 09:32:15,250 - __main__ - INFO - Update scale: 0.00913888888888889
2025-04-14 09:32:15,331 - __main__ - INFO - Model weights updated successfully
2025-04-14 09:32:16,165 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:32:16,202 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:32:17,090 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 09:32:17,124 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 09:32:17,124 - __main__ - INFO - Skipping full model checkpoint for iteration 6
2025-04-14 09:32:17,124 - __main__ - INFO - Iteration 6 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 6
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 6
Iteration 6 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 7
===========================================
Iteration 7 is in valid list - performing full update
Finding noisy pairs for iteration 7...
2025-04-14 09:32:38,439 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 09:32:38,984 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:32:39,128 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:32:39,142 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 09:32:39,142 - __main__ - INFO - Loading dataset
2025-04-14 09:32:39,845 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 09:33:02,096 - __main__ - INFO - Processing dataset
2025-04-14 09:33:02,362 - __main__ - INFO - Searching for noisy pairs starting from index 1625
2025-04-14 09:33:10,059 - __main__ - INFO - Noisy pair found at index 1663: chosen_logps=-49.7500, rejected_logps=-50.5000
2025-04-14 09:33:10,060 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 09:33:10,128 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_7
2025-04-14 09:33:10,129 - __main__ - INFO - New dataset offset: 1664
Updated dataset offset to: 1664 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 7
Worker node 1 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 09:33:24,776 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 09:33:24,803 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 09:33:25,266 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 09:33:25,321 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 09:33:25,358 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]2025-04-14 09:33:25,473 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:33:25,488 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 09:33:25,488 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:33:25,500 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:33:25,512 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 09:33:25,512 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.54it/s]
2025-04-14 09:33:25,816 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:33:25,961 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:33:25,982 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 09:33:25,982 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:33:42,020 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 09:33:42,045 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 09:33:42,636 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 09:33:52,048 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 09:33:52,519 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 09:33:52,955 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 09:33:55,789 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 09:33:56,348 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 09:33:57,085 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 09:33:59,217 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 09:33:59,449 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 09:33:59,722 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 09:34:00,243 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 09:34:00,438 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 09:34:01,042 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 09:34:03,121 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 09:34:03,932 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 09:34:03,969 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 09:34:04,628 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 09:34:04,859 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 09:34:05,066 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 09:34:07,136 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 09:34:07,760 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 09:34:07,888 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 09:34:08,423 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 09:34:08,852 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 09:34:09,153 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 09:34:11,797 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 09:34:11,817 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 09:34:11,841 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 09:34:12,205 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 09:34:12,211 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 09:34:13,152 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 09:34:15,466 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 09:34:15,731 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 09:34:15,845 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 09:34:16,369 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 09:34:16,547 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 09:34:17,230 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 09:34:19,590 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 09:34:19,634 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 09:34:19,901 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 09:34:20,461 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 09:34:20,615 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 09:34:21,516 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 09:34:23,214 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 09:34:24,081 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 09:34:24,086 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 09:34:24,207 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 09:34:24,551 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 09:34:24,613 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 09:34:24,614 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 09:34:26,233 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 09:34:26,242 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 09:34:27,435 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 09:34:28,205 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 09:34:29,172 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 09:34:31,965 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 09:34:32,079 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 09:34:32,158 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 09:34:32,489 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 09:34:33,856 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 09:34:33,945 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 09:35:55,199 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 09:35:55,393 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 09:35:56,001 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 09:35:59,536 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 09:36:01,180 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 09:36:01,605 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 09:36:03,975 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 09:36:05,465 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 09:36:06,719 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 09:36:07,047 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 09:36:07,466 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 09:36:11,108 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 09:36:11,143 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 09:36:11,767 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 09:36:11,843 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 09:36:14,399 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 09:36:16,202 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 09:36:18,653 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 09:36:18,668 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 09:36:19,302 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 09:36:20,584 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 09:36:23,541 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 09:36:23,568 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 09:36:26,826 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 09:36:28,016 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 09:36:28,219 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 09:36:28,494 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 09:36:30,324 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 09:36:31,280 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 09:36:33,250 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 09:38:22,126 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_1.npy
2025-04-14 09:38:22,126 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 85 negative comparisons
[E414 09:38:23.326603748 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7701f91a5446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7701ae5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7701ae5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7701ae5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7701ae5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7701ae5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7701f930c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7701f9c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7701f9d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:38:23,510 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_0.npy
2025-04-14 09:38:23,510 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 87 negative comparisons
2025-04-14 09:38:24,901 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 09:38:26,489 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 09:38:30,721 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_2.npy
2025-04-14 09:38:30,721 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 54 negative comparisons
[E414 09:38:30.654659948 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x6fffb2cde446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x6fff67fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x6fff67fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x6fff67fcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x6fff67fd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x6fff67fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x6fffb2e455c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x6fffb3894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x6fffb3926850 in /lib/x86_64-linux-gnu/libc.so.6)

Worker node 1 successfully completed gradient computation for iteration 7
Waiting for worker nodes to complete gradient computation...
2025-04-14 09:38:33,681 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 7
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 09:38:51,938 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 09:38:52,495 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:38:52,639 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:38:52,652 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 09:39:09,147 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 09:39:12,070 - __main__ - INFO - Loaded gradients from node 0: 87 negative comparisons
2025-04-14 09:39:12,338 - __main__ - INFO - Loaded gradients from node 1: 85 negative comparisons
2025-04-14 09:39:12,576 - __main__ - INFO - Loaded gradients from node 2: 54 negative comparisons
2025-04-14 09:39:12,739 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 09:39:12,935 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 09:39:13,130 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133576
2025-04-14 09:39:13,325 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107347313
2025-04-14 09:39:13,522 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84830031
2025-04-14 09:39:13,717 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47189169
2025-04-14 09:39:13,912 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252570
2025-04-14 09:39:14,108 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8805586
2025-04-14 09:39:14,303 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1553380
2025-04-14 09:39:14,500 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555061
2025-04-14 09:39:14,855 - __main__ - INFO - Non-zero entries after thresholding: 47189169
2025-04-14 09:39:14,856 - __main__ - INFO - Using relaxed comparison (count_negative: 226)
2025-04-14 09:39:14,856 - __main__ - INFO - Testing stepsize: 0
2025-04-14 09:39:15,594 - __main__ - INFO -   chosen_logps: -49.78995, rejected_logps: -50.12741
2025-04-14 09:39:15,594 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 09:39:15,960 - __main__ - INFO -   chosen_logps: -49.80775, rejected_logps: -50.12747
2025-04-14 09:39:15,960 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 09:39:16,328 - __main__ - INFO -   chosen_logps: -49.52692, rejected_logps: -50.18206
2025-04-14 09:39:16,328 - __main__ - INFO - Testing stepsize: 1
2025-04-14 09:39:16,698 - __main__ - INFO -   chosen_logps: -48.96986, rejected_logps: -50.59930
2025-04-14 09:39:16,698 - __main__ - INFO - Testing stepsize: 2
2025-04-14 09:39:17,073 - __main__ - INFO -   chosen_logps: -47.86412, rejected_logps: -51.11036
2025-04-14 09:39:17,073 - __main__ - INFO - Testing stepsize: 5
2025-04-14 09:39:17,451 - __main__ - INFO -   chosen_logps: -45.10682, rejected_logps: -52.91551
2025-04-14 09:39:17,451 - __main__ - INFO - Testing stepsize: 10
2025-04-14 09:39:17,830 - __main__ - INFO -   chosen_logps: -41.14353, rejected_logps: -55.95200
2025-04-14 09:39:17,830 - __main__ - INFO - Testing stepsize: 15
2025-04-14 09:39:18,210 - __main__ - INFO -   chosen_logps: -37.86196, rejected_logps: -59.25658
2025-04-14 09:39:18,953 - __main__ - INFO - Update scale: 0.004394444444444445
2025-04-14 09:39:18,954 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 09:39:19,038 - __main__ - INFO - Model weights updated successfully
2025-04-14 09:39:19,882 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:39:19,918 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:39:20,767 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 09:39:20,803 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 09:39:20,803 - __main__ - INFO - Skipping full model checkpoint for iteration 7
2025-04-14 09:39:20,803 - __main__ - INFO - Iteration 7 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 7
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 7
Iteration 7 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 8
===========================================
Iteration 8 is in valid list - performing full update
Finding noisy pairs for iteration 8...
2025-04-14 09:39:42,010 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.18it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 09:39:42,558 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:39:42,701 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:39:42,714 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 09:39:42,715 - __main__ - INFO - Loading dataset
2025-04-14 09:39:43,455 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 09:40:05,121 - __main__ - INFO - Processing dataset
2025-04-14 09:40:05,370 - __main__ - INFO - Searching for noisy pairs starting from index 1664
2025-04-14 09:40:13,841 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 09:40:22,625 - __main__ - INFO - Noisy pair found at index 1746: chosen_logps=-23.8750, rejected_logps=-24.2500
2025-04-14 09:40:22,626 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 09:40:22,694 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_8
2025-04-14 09:40:22,695 - __main__ - INFO - New dataset offset: 1747
Updated dataset offset to: 1747 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 8
Worker node 2 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 09:40:37,904 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.40it/s]2025-04-14 09:40:38,199 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.62it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]2025-04-14 09:40:38,449 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:40:38,499 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 09:40:38,603 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:40:38,618 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 09:40:38,618 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.72it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]2025-04-14 09:40:38,744 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]2025-04-14 09:40:38,884 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:40:38,897 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 09:40:38,897 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 09:40:39,048 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:40:39,192 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:40:39,207 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 09:40:39,207 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:40:55,187 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 09:40:55,414 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 09:40:55,689 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 09:41:05,416 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 09:41:05,432 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 09:41:05,941 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 09:41:09,176 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 09:41:09,204 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 09:41:09,836 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 09:41:12,586 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 09:41:12,925 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 09:41:13,049 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 09:41:13,096 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 09:41:13,125 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 09:41:13,788 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 09:41:16,445 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 09:41:16,572 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 09:41:16,913 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 09:41:16,950 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 09:41:17,447 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 09:41:17,664 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 09:41:20,424 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 09:41:20,437 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 09:41:20,870 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 09:41:20,937 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 09:41:21,172 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 09:41:21,534 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 09:41:24,064 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 09:41:24,805 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 09:41:25,061 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 09:41:25,094 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 09:41:25,170 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 09:41:25,426 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 09:41:28,351 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 09:41:28,737 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 09:41:28,842 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 09:41:28,879 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 09:41:29,256 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 09:41:29,898 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 09:41:32,275 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 09:41:32,486 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 09:41:33,192 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 09:41:33,336 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 09:41:33,441 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 09:41:34,166 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 09:41:36,457 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 09:41:36,463 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 09:41:37,088 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 09:41:37,421 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 09:41:37,424 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 09:41:37,529 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 09:41:37,533 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 09:41:38,268 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 09:41:38,274 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 09:41:40,539 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 09:41:40,589 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 09:41:41,647 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 09:41:44,901 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 09:41:44,973 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 09:41:45,233 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 09:41:45,234 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 09:41:45,719 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 09:41:45,946 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 09:43:10,737 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 09:43:11,213 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 09:43:11,330 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 09:43:14,814 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 09:43:14,850 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 09:43:14,883 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 09:43:18,064 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 09:43:19,883 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 09:43:21,526 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 09:43:22,314 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 09:43:23,140 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 09:43:25,061 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 09:43:26,036 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 09:43:26,776 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 09:43:26,927 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 09:43:30,448 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 09:43:30,789 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 09:43:32,086 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 09:43:33,929 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 09:43:34,180 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 09:43:36,561 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 09:43:37,517 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 09:43:38,130 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 09:43:39,315 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 09:43:41,130 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 09:43:42,321 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 09:43:42,833 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 09:43:45,206 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 09:43:46,613 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 09:43:46,829 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
[E414 09:45:36.508396820 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c8b66cce446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c8b1bfcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c8b1bfcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7c8b1bfcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7c8b1bfd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c8b1bfd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c8b66e355c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c8b67894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c8b67926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 09:45:36.519696459 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7aa89954d446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7aa84e9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7aa84e9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7aa84e9cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7aa84e9d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7aa84e9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7aa8996b45c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7aa89a094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7aa89a126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 09:45:36.528107451 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7163e51a1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71639a5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71639a5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x71639a5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x71639a5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71639a5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7163e53085c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7163e5c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7163e5d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:45:36,727 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_0.npy
2025-04-14 09:45:36,727 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 118 negative comparisons
2025-04-14 09:45:39,612 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 09:45:42,576 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_1.npy
2025-04-14 09:45:42,576 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 98 negative comparisons
2025-04-14 09:45:45,253 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_2.npy
2025-04-14 09:45:45,253 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 103 negative comparisons
[E414 09:45:45.194493698 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a1531638446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7a14e69cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7a14e69cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7a14e69cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7a14e69d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a14e69d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7a153179f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7a1532294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7a1532326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 09:45:45.202984501 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75e094798446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75e049bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75e049bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x75e049bcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x75e049bd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75e049bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x75e0948ff5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x75e095494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x75e095526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 09:45:45.209159366 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72b1b32dc446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72b1685cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72b1685cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x72b1685cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x72b1685d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72b1685d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72b1b34435c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72b1b3e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72b1b3f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:45:45,364 - __main__ - INFO - Node 1: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
2025-04-14 09:45:48,272 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 8
Worker node 2 successfully completed gradient computation for iteration 8
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 09:46:06,859 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  4.69it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  5.08it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.82it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.55it/s]
2025-04-14 09:46:07,572 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:46:07,716 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:46:07,732 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 09:46:24,228 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 09:46:27,206 - __main__ - INFO - Loaded gradients from node 0: 118 negative comparisons
2025-04-14 09:46:27,441 - __main__ - INFO - Loaded gradients from node 1: 98 negative comparisons
2025-04-14 09:46:27,679 - __main__ - INFO - Loaded gradients from node 2: 103 negative comparisons
2025-04-14 09:46:27,841 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 09:46:28,035 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 09:46:28,229 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119130071
2025-04-14 09:46:28,423 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107342815
2025-04-14 09:46:28,617 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84830820
2025-04-14 09:46:28,812 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47187330
2025-04-14 09:46:29,008 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22256638
2025-04-14 09:46:29,203 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806978
2025-04-14 09:46:29,398 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551045
2025-04-14 09:46:29,595 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555399
2025-04-14 09:46:29,949 - __main__ - INFO - Non-zero entries after thresholding: 47187330
2025-04-14 09:46:29,949 - __main__ - INFO - Using relaxed comparison (count_negative: 319)
2025-04-14 09:46:29,949 - __main__ - INFO - Testing stepsize: 0
2025-04-14 09:46:30,704 - __main__ - INFO -   chosen_logps: -23.92811, rejected_logps: -23.81993
2025-04-14 09:46:30,705 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 09:46:31,090 - __main__ - INFO -   chosen_logps: -23.88323, rejected_logps: -23.80716
2025-04-14 09:46:31,090 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 09:46:31,476 - __main__ - INFO -   chosen_logps: -23.57538, rejected_logps: -23.97799
2025-04-14 09:46:31,476 - __main__ - INFO - Testing stepsize: 1
2025-04-14 09:46:31,865 - __main__ - INFO -   chosen_logps: -22.93386, rejected_logps: -24.50862
2025-04-14 09:46:31,865 - __main__ - INFO - Testing stepsize: 2
2025-04-14 09:46:32,259 - __main__ - INFO -   chosen_logps: -22.04452, rejected_logps: -24.95119
2025-04-14 09:46:32,259 - __main__ - INFO - Testing stepsize: 5
2025-04-14 09:46:32,653 - __main__ - INFO -   chosen_logps: -19.08101, rejected_logps: -26.85170
2025-04-14 09:46:32,653 - __main__ - INFO - Testing stepsize: 10
2025-04-14 09:46:33,049 - __main__ - INFO -   chosen_logps: -14.91016, rejected_logps: -29.75811
2025-04-14 09:46:33,049 - __main__ - INFO - Testing stepsize: 15
2025-04-14 09:46:33,446 - __main__ - INFO -   chosen_logps: -11.56431, rejected_logps: -33.10548
2025-04-14 09:46:34,217 - __main__ - INFO - Update scale: 0.006202777777777778
2025-04-14 09:46:34,219 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 09:46:34,299 - __main__ - INFO - Model weights updated successfully
2025-04-14 09:46:35,146 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:46:35,183 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:46:36,085 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 09:46:36,120 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 09:46:36,120 - __main__ - INFO - Skipping full model checkpoint for iteration 8
2025-04-14 09:46:36,120 - __main__ - INFO - Iteration 8 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 8
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 8
Iteration 8 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 9
===========================================
Iteration 9 is in valid list - performing full update
Finding noisy pairs for iteration 9...
2025-04-14 09:46:57,692 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 09:46:58,242 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:46:58,387 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:46:58,402 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 09:46:58,402 - __main__ - INFO - Loading dataset
2025-04-14 09:46:59,208 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 09:47:21,012 - __main__ - INFO - Processing dataset
2025-04-14 09:47:21,262 - __main__ - INFO - Searching for noisy pairs starting from index 1747
2025-04-14 09:47:31,560 - __main__ - INFO - Processed 53 samples without finding a noisy pair
2025-04-14 09:47:49,763 - __main__ - INFO - Noisy pair found at index 1892: chosen_logps=-58.0000, rejected_logps=-59.7500
2025-04-14 09:47:49,764 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 09:47:49,832 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_9
2025-04-14 09:47:49,833 - __main__ - INFO - New dataset offset: 1893
Updated dataset offset to: 1893 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 9
Worker node 1 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 09:48:05,608 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 09:48:05,666 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 09:48:06,151 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:48:06,220 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:48:06,243 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 09:48:06,303 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:48:06,318 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 09:48:06,318 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:48:06,364 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:48:06,376 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 09:48:06,376 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.37it/s]
2025-04-14 09:48:06,803 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:48:06,948 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:48:06,963 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 09:48:06,963 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:48:22,848 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 09:48:22,910 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 09:48:23,517 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 09:48:32,939 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 09:48:33,066 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 09:48:33,741 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 09:48:36,761 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 09:48:36,858 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 09:48:37,981 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 09:48:39,825 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 09:48:40,109 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 09:48:40,737 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 09:48:41,013 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 09:48:41,118 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 09:48:42,298 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 09:48:44,219 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 09:48:44,223 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 09:48:44,780 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 09:48:44,975 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 09:48:45,848 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 09:48:46,500 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 09:48:48,166 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 09:48:48,404 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 09:48:48,858 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 09:48:48,992 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 09:48:49,434 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 09:48:50,558 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 09:48:52,457 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 09:48:52,802 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 09:48:52,865 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 09:48:52,974 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 09:48:54,488 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 09:48:54,510 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 09:48:56,213 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 09:48:56,651 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 09:48:57,138 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 09:48:57,263 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 09:48:57,714 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 09:48:58,704 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 09:49:00,122 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 09:49:00,690 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 09:49:01,120 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 09:49:01,824 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 09:49:01,996 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 09:49:02,927 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 09:49:04,498 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 09:49:04,576 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 09:49:05,290 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 09:49:05,292 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 09:49:06,023 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 09:49:06,037 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 09:49:06,046 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 09:49:07,216 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 09:49:07,219 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 09:49:08,613 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 09:49:10,063 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 09:49:10,517 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 09:49:12,298 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 09:49:12,728 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 09:49:14,036 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 09:49:14,232 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 09:49:14,868 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 09:49:15,022 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 09:50:36,503 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 09:50:37,319 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 09:50:40,584 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 09:50:41,719 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 09:50:41,935 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 09:50:42,653 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 09:50:45,991 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 09:50:46,729 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 09:50:49,885 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 09:50:50,137 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 09:50:50,481 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 09:50:50,518 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 09:50:52,774 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 09:50:53,939 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 09:50:58,005 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 09:50:58,412 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 09:50:58,421 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 09:51:02,341 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 09:51:02,811 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 09:51:03,239 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 09:51:05,129 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 09:51:06,508 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 09:51:07,532 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 09:51:08,634 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 09:51:08,782 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 09:51:08,904 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 09:51:09,326 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 09:51:10,633 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 09:51:11,654 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 09:51:14,002 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 09:53:05,656 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_0.npy
2025-04-14 09:53:05,656 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 113 negative comparisons
2025-04-14 09:53:06,145 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_1.npy
2025-04-14 09:53:06,145 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 109 negative comparisons
[rank4]:[E414 09:53:06.025282534 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x768452f5c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7684083cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7684083cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7684083cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7684083d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7684083d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7684530c35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x768453a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x768453b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:53:08,374 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 09:53:09,180 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 09:53:11,160 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_2.npy
2025-04-14 09:53:11,160 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 120 negative comparisons
[E414 09:53:11.088622730 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76621b878446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7661d0bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7661d0bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7661d0bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7661d0bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7661d0bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x76621b9df5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x76621c494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x76621c526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 09:53:11.122314822 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c88811de446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c88365cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c88365cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7c88365cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7c88365d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c88365d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c88813455c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c8881e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c8881f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 09:53:14,164 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 9
Worker node 2 successfully completed gradient computation for iteration 9
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 09:53:31,490 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.54it/s]
2025-04-14 09:53:32,041 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:53:32,181 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:53:32,193 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 09:53:48,626 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 09:53:51,518 - __main__ - INFO - Loaded gradients from node 0: 113 negative comparisons
2025-04-14 09:53:51,752 - __main__ - INFO - Loaded gradients from node 1: 109 negative comparisons
2025-04-14 09:53:51,985 - __main__ - INFO - Loaded gradients from node 2: 120 negative comparisons
2025-04-14 09:53:52,146 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 09:53:52,337 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 09:53:52,527 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133020
2025-04-14 09:53:52,718 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107350081
2025-04-14 09:53:52,910 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84823681
2025-04-14 09:53:53,103 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47186085
2025-04-14 09:53:53,295 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22257039
2025-04-14 09:53:53,488 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8808452
2025-04-14 09:53:53,678 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552621
2025-04-14 09:53:53,868 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554896
2025-04-14 09:53:54,215 - __main__ - INFO - Non-zero entries after thresholding: 47186085
2025-04-14 09:53:54,215 - __main__ - INFO - Using relaxed comparison (count_negative: 342)
2025-04-14 09:53:54,215 - __main__ - INFO - Testing stepsize: 0
2025-04-14 09:53:54,959 - __main__ - INFO -   chosen_logps: -57.56502, rejected_logps: -59.94020
2025-04-14 09:53:54,959 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 09:53:55,339 - __main__ - INFO -   chosen_logps: -57.43454, rejected_logps: -60.03182
2025-04-14 09:53:55,339 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 09:53:55,718 - __main__ - INFO -   chosen_logps: -56.40023, rejected_logps: -60.34315
2025-04-14 09:53:55,718 - __main__ - INFO - Testing stepsize: 1
2025-04-14 09:53:56,100 - __main__ - INFO -   chosen_logps: -55.31060, rejected_logps: -60.59404
2025-04-14 09:53:56,100 - __main__ - INFO - Testing stepsize: 2
2025-04-14 09:53:56,487 - __main__ - INFO -   chosen_logps: -53.31710, rejected_logps: -61.14934
2025-04-14 09:53:56,487 - __main__ - INFO - Testing stepsize: 5
2025-04-14 09:53:56,875 - __main__ - INFO -   chosen_logps: -48.34158, rejected_logps: -63.07657
2025-04-14 09:53:56,875 - __main__ - INFO - Testing stepsize: 10
2025-04-14 09:53:57,265 - __main__ - INFO -   chosen_logps: -41.82376, rejected_logps: -66.05006
2025-04-14 09:53:57,265 - __main__ - INFO - Testing stepsize: 15
2025-04-14 09:53:57,656 - __main__ - INFO -   chosen_logps: -37.06126, rejected_logps: -69.19344
2025-04-14 09:53:58,414 - __main__ - INFO - Update scale: 0.0066500000000000005
2025-04-14 09:53:58,415 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 09:53:58,495 - __main__ - INFO - Model weights updated successfully
2025-04-14 09:53:59,785 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:53:59,827 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:54:00,890 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 09:54:00,926 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 09:54:00,926 - __main__ - INFO - Skipping full model checkpoint for iteration 9
2025-04-14 09:54:00,926 - __main__ - INFO - Iteration 9 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 9
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 9
Iteration 9 completed successfully
Cleaning up iteration files...
Multi-node training completed successfully!
Copying scripts to worker nodes...
Starting with dataset offset: 0
===========================================
Processing iteration 0
===========================================
Iteration 0 is in valid list - performing full update
Finding noisy pairs for iteration 0...
2025-04-14 09:54:25,642 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.15it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 09:54:26,193 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:54:26,337 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 09:54:26,352 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 09:54:26,352 - __main__ - INFO - Loading dataset
2025-04-14 09:54:27,151 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 09:54:48,745 - __main__ - INFO - Processing dataset
2025-04-14 09:54:48,990 - __main__ - INFO - Searching for noisy pairs starting from index 0
2025-04-14 09:54:49,623 - __main__ - INFO - Processed 0 samples without finding a noisy pair
2025-04-14 09:55:09,941 - __main__ - INFO - Processed 100 samples without finding a noisy pair
2025-04-14 09:55:28,039 - __main__ - INFO - Processed 200 samples without finding a noisy pair
2025-04-14 09:55:47,068 - __main__ - INFO - Processed 300 samples without finding a noisy pair
2025-04-14 09:55:53,795 - __main__ - INFO - Noisy pair found at index 337: chosen_logps=-90.0000, rejected_logps=-92.0000
2025-04-14 09:55:53,800 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 09:55:53,871 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_0
2025-04-14 09:55:53,872 - __main__ - INFO - New dataset offset: 338
Updated dataset offset to: 338 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 0
Worker node 1 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 09:56:09,292 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]2025-04-14 09:56:09,552 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 09:56:09,616 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]2025-04-14 09:56:09,843 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]2025-04-14 09:56:09,986 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:56:10,001 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 09:56:10,001 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 09:56:10,096 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:56:10,172 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 09:56:10,250 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:56:10,265 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 09:56:10,265 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:56:10,315 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 09:56:10,327 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 09:56:10,328 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 09:56:26,628 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 09:56:26,845 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 09:56:26,881 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 09:56:36,701 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 09:56:36,748 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 09:56:37,184 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 09:56:40,534 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 09:56:40,548 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 09:56:40,975 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 09:56:43,313 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 09:56:43,931 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 09:56:43,998 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 09:56:44,457 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 09:56:44,875 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 09:56:45,092 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 09:56:47,258 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 09:56:47,778 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 09:56:47,917 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 09:56:48,220 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 09:56:48,675 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 09:56:49,255 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 09:56:51,607 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 09:56:51,916 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 09:56:52,093 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 09:56:52,398 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 09:56:52,586 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 09:56:53,228 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 09:56:55,331 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 09:56:56,116 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 09:56:56,179 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 09:56:56,520 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 09:56:56,631 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 09:56:57,439 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 09:56:59,235 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 09:56:59,592 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 09:57:00,064 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 09:57:00,171 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 09:57:00,521 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 09:57:01,602 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 09:57:03,529 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 09:57:03,697 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 09:57:04,191 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 09:57:04,625 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 09:57:04,659 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 09:57:05,874 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 09:57:07,631 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 09:57:07,680 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 09:57:08,600 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 09:57:08,602 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 09:57:08,785 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 09:57:08,840 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 09:57:08,852 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 09:57:09,904 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 09:57:09,913 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 09:57:11,829 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 09:57:11,936 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 09:57:13,458 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 09:57:15,874 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 09:57:16,261 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 09:57:16,689 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 09:57:16,759 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 09:57:17,064 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 09:57:17,246 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 09:58:34,335 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 09:58:36,412 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 09:58:39,013 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 09:58:39,532 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 09:58:40,508 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 09:58:40,856 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 09:58:41,039 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 09:58:44,241 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 09:58:45,443 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 09:58:45,545 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 09:58:47,261 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 09:58:47,566 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 09:58:48,835 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 09:58:50,960 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 09:58:52,527 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 09:58:54,023 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 09:58:54,789 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 09:58:55,560 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 09:58:58,200 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 09:59:00,685 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 09:59:02,508 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 09:59:04,005 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 09:59:04,774 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 09:59:06,160 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 09:59:06,553 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 09:59:06,756 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 09:59:07,436 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 09:59:07,545 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 09:59:11,567 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 09:59:12,299 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
[E414 10:00:54.945312347 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x766ae2346446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x766a977cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x766a977cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x766a977cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x766a977d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x766a977d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x766ae24ad5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x766ae2e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x766ae2f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:00:54,169 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_0.npy
2025-04-14 10:00:54,169 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 119 negative comparisons
[E414 10:00:54.433800804 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f8982787446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f8937bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f8937bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f8937bcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f8937bd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f8937bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f89828ee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f8983294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f8983326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 10:00:54.433533890 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x762e1b146446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x762dd05cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x762dd05cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x762dd05cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x762dd05d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x762dd05d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x762e1b2ad5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x762e1bc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x762e1bd26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:00:54,799 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_1.npy
2025-04-14 10:00:54,799 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 130 negative comparisons
2025-04-14 10:00:56,892 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 10:00:57,708 - __main__ - INFO - Node 1: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 0
2025-04-14 10:01:03,283 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_2.npy
2025-04-14 10:01:03,283 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 145 negative comparisons
[E414 10:01:03.230493129 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c1d1b4de446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c1cd07cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c1cd07cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7c1cd07cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7c1cd07d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c1cd07d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c1d1b6455c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c1d1c094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c1d1c126850 in /lib/x86_64-linux-gnu/libc.so.6)

Waiting for worker nodes to complete gradient computation...
2025-04-14 10:01:06,368 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 0
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 10:01:24,013 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 10:01:24,559 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:01:24,701 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:01:24,714 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 10:01:41,164 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 10:01:44,102 - __main__ - INFO - Loaded gradients from node 0: 119 negative comparisons
2025-04-14 10:01:44,337 - __main__ - INFO - Loaded gradients from node 1: 130 negative comparisons
2025-04-14 10:01:44,572 - __main__ - INFO - Loaded gradients from node 2: 145 negative comparisons
2025-04-14 10:01:44,732 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 10:01:44,922 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 10:01:45,113 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119134996
2025-04-14 10:01:45,304 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107348517
2025-04-14 10:01:45,494 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84826081
2025-04-14 10:01:45,685 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47189607
2025-04-14 10:01:45,875 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22258452
2025-04-14 10:01:46,067 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804533
2025-04-14 10:01:46,259 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551529
2025-04-14 10:01:46,453 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555079
2025-04-14 10:01:46,805 - __main__ - INFO - Non-zero entries after thresholding: 47189607
2025-04-14 10:01:46,805 - __main__ - INFO - Testing stepsize: 0
2025-04-14 10:01:47,446 - __main__ - INFO -   chosen_logps: -89.94107, rejected_logps: -91.92723
2025-04-14 10:01:47,446 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 10:01:47,714 - __main__ - INFO -   chosen_logps: -89.66228, rejected_logps: -92.17711
2025-04-14 10:01:47,714 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 10:01:47,985 - __main__ - INFO -   chosen_logps: -89.02004, rejected_logps: -92.69740
2025-04-14 10:01:47,985 - __main__ - INFO - Testing stepsize: 1
2025-04-14 10:01:48,254 - __main__ - INFO -   chosen_logps: -87.43575, rejected_logps: -93.27243
2025-04-14 10:01:48,254 - __main__ - INFO - Testing stepsize: 2
2025-04-14 10:01:48,529 - __main__ - INFO -   chosen_logps: -85.25107, rejected_logps: -94.68098
2025-04-14 10:01:48,529 - __main__ - INFO - Testing stepsize: 5
2025-04-14 10:01:48,805 - __main__ - INFO -   chosen_logps: -78.48628, rejected_logps: -98.95224
2025-04-14 10:01:48,806 - __main__ - INFO - Testing stepsize: 10
2025-04-14 10:01:49,083 - __main__ - INFO -   chosen_logps: -68.51540, rejected_logps: -105.24008
2025-04-14 10:01:49,083 - __main__ - INFO - Testing stepsize: 15
2025-04-14 10:01:49,361 - __main__ - INFO -   chosen_logps: -60.39312, rejected_logps: -112.30363
2025-04-14 10:01:49,962 - __main__ - INFO - Update scale: 0.007661111111111112
2025-04-14 10:01:50,045 - __main__ - INFO - Model weights updated successfully
2025-04-14 10:01:50,888 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:01:50,924 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:01:51,808 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 10:01:51,841 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 10:01:51,841 - __main__ - INFO - Skipping full model checkpoint for iteration 0
2025-04-14 10:01:51,841 - __main__ - INFO - Iteration 0 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 0
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 0
Iteration 0 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 1
===========================================
Iteration 1 is in valid list - performing full update
Finding noisy pairs for iteration 1...
2025-04-14 10:02:12,934 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]
2025-04-14 10:02:13,486 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:02:13,631 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:02:13,645 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 10:02:13,646 - __main__ - INFO - Loading dataset
2025-04-14 10:02:14,828 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 10:02:36,832 - __main__ - INFO - Processing dataset
2025-04-14 10:02:37,084 - __main__ - INFO - Searching for noisy pairs starting from index 338
2025-04-14 10:02:43,817 - __main__ - INFO - Noisy pair found at index 369: chosen_logps=-153.0000, rejected_logps=-154.0000
2025-04-14 10:02:43,818 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 10:02:43,886 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_1
2025-04-14 10:02:43,887 - __main__ - INFO - New dataset offset: 370
Updated dataset offset to: 370 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 1
Worker node 1 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 10:02:58,199 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 10:02:58,472 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
2025-04-14 10:02:58,725 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 10:02:58,756 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]2025-04-14 10:02:58,900 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:02:58,915 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 10:02:58,915 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  6.86it/s]2025-04-14 10:02:59,015 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.36it/s]2025-04-14 10:02:59,168 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:02:59,183 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 10:02:59,183 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.46it/s]
2025-04-14 10:02:59,279 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:02:59,419 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:02:59,432 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 10:02:59,432 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:03:15,502 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 10:03:15,759 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 10:03:15,959 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 10:03:25,658 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 10:03:25,794 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 10:03:26,258 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 10:03:29,606 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 10:03:29,849 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 10:03:30,014 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 10:03:32,685 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 10:03:32,760 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 10:03:33,283 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 10:03:33,822 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 10:03:33,912 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 10:03:34,007 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 10:03:36,728 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 10:03:36,753 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 10:03:37,163 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 10:03:37,675 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 10:03:37,921 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 10:03:37,949 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 10:03:40,929 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 10:03:40,974 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 10:03:41,290 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 10:03:41,488 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 10:03:41,863 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 10:03:41,865 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 10:03:44,871 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 10:03:45,099 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 10:03:45,120 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 10:03:45,485 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 10:03:45,967 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 10:03:46,262 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 10:03:48,605 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 10:03:49,269 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 10:03:49,335 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 10:03:49,881 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 10:03:49,895 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 10:03:50,242 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 10:03:52,235 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 10:03:53,183 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 10:03:53,543 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 10:03:53,918 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 10:03:54,248 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 10:03:54,391 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 10:03:57,099 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 10:03:57,534 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 10:03:57,712 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 10:03:58,140 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 10:03:58,146 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 10:03:58,737 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 10:03:58,748 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 10:03:59,151 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 10:03:59,153 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 10:04:01,190 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 10:04:01,652 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 10:04:01,689 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 10:04:05,306 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 10:04:05,553 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 10:04:05,791 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 10:04:06,152 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 10:04:06,859 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 10:04:06,935 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 10:05:22,647 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 10:05:23,180 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 10:05:25,243 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 10:05:25,638 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 10:05:26,613 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 10:05:29,271 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 10:05:29,374 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 10:05:31,346 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 10:05:33,369 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 10:05:35,054 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 10:05:36,292 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 10:05:38,287 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 10:05:39,510 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 10:05:40,234 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 10:05:41,939 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 10:05:43,301 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 10:05:43,755 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 10:05:46,335 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 10:05:46,693 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 10:05:47,292 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 10:05:49,576 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 10:05:50,979 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 10:05:51,695 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 10:05:53,550 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 10:05:54,375 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 10:05:55,672 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 10:05:56,219 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 10:05:58,153 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 10:06:00,229 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 10:06:00,458 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 10:07:41,805 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_0.npy
2025-04-14 10:07:41,805 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 128 negative comparisons
[E414 10:07:41.755829835 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d9d735fb446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7d9d289cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7d9d289cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7d9d289cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7d9d289d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d9d289d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7d9d737625c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7d9d74094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7d9d74126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 10:07:41.759264364 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bc79423a446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7bc7495cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7bc7495cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7bc7495cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7bc7495d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bc7495d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7bc7943a15c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7bc794e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7bc794f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:07:44,964 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 10:07:47,870 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_1.npy
2025-04-14 10:07:47,870 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 143 negative comparisons
2025-04-14 10:07:50,850 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 10:07:51.319387210 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7469fd557446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7469b29cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7469b29cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7469b29cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7469b29d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7469b29d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7469fd6be5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7469fe294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7469fe326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 10:07:51.328124521 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b0ffad57446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b0fb01cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b0fb01cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7b0fb01cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7b0fb01d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b0fb01d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b0ffaebe5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b0ffba94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b0ffbb26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 10:07:51.343595562 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7579e8b64446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75799dfcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75799dfcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x75799dfcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x75799dfd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75799dfd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7579e8ccb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7579e9894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7579e9926850 in /lib/x86_64-linux-gnu/libc.so.6)

Waiting for worker nodes to complete gradient computation...
2025-04-14 10:07:51,609 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_2.npy
2025-04-14 10:07:51,609 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 109 negative comparisons
2025-04-14 10:07:54,381 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 1
Worker node 2 successfully completed gradient computation for iteration 1
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 10:08:12,274 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.19it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
2025-04-14 10:08:12,826 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:08:12,968 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:08:12,981 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 10:08:29,476 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 10:08:32,390 - __main__ - INFO - Loaded gradients from node 0: 128 negative comparisons
2025-04-14 10:08:32,622 - __main__ - INFO - Loaded gradients from node 1: 143 negative comparisons
2025-04-14 10:08:32,853 - __main__ - INFO - Loaded gradients from node 2: 109 negative comparisons
2025-04-14 10:08:33,011 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 10:08:33,197 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 10:08:33,384 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119137335
2025-04-14 10:08:33,571 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107351839
2025-04-14 10:08:33,758 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84838846
2025-04-14 10:08:33,946 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47196490
2025-04-14 10:08:34,134 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22254492
2025-04-14 10:08:34,323 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8799796
2025-04-14 10:08:34,511 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552704
2025-04-14 10:08:34,699 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554742
2025-04-14 10:08:35,042 - __main__ - INFO - Non-zero entries after thresholding: 47196490
2025-04-14 10:08:35,042 - __main__ - INFO - Testing stepsize: 0
2025-04-14 10:08:35,672 - __main__ - INFO -   chosen_logps: -152.85396, rejected_logps: -153.87024
2025-04-14 10:08:35,672 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 10:08:35,931 - __main__ - INFO -   chosen_logps: -152.76852, rejected_logps: -153.90605
2025-04-14 10:08:35,931 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 10:08:36,190 - __main__ - INFO -   chosen_logps: -152.22664, rejected_logps: -154.34650
2025-04-14 10:08:36,190 - __main__ - INFO - Testing stepsize: 1
2025-04-14 10:08:36,452 - __main__ - INFO -   chosen_logps: -151.46887, rejected_logps: -154.68260
2025-04-14 10:08:36,452 - __main__ - INFO - Testing stepsize: 2
2025-04-14 10:08:36,718 - __main__ - INFO -   chosen_logps: -150.34981, rejected_logps: -155.72353
2025-04-14 10:08:36,718 - __main__ - INFO - Testing stepsize: 5
2025-04-14 10:08:36,988 - __main__ - INFO -   chosen_logps: -146.96150, rejected_logps: -158.48338
2025-04-14 10:08:36,988 - __main__ - INFO - Testing stepsize: 10
2025-04-14 10:08:37,257 - __main__ - INFO -   chosen_logps: -141.76999, rejected_logps: -163.58029
2025-04-14 10:08:37,257 - __main__ - INFO - Testing stepsize: 15
2025-04-14 10:08:37,527 - __main__ - INFO -   chosen_logps: -137.64754, rejected_logps: -168.72963
2025-04-14 10:08:38,111 - __main__ - INFO - Update scale: 0.007388888888888889
2025-04-14 10:08:38,189 - __main__ - INFO - Model weights updated successfully
2025-04-14 10:08:39,031 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:08:39,064 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:08:40,037 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 10:08:40,070 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 10:08:40,070 - __main__ - INFO - Skipping full model checkpoint for iteration 1
2025-04-14 10:08:40,071 - __main__ - INFO - Iteration 1 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 1
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 1
Iteration 1 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 2
===========================================
Iteration 2 is in valid list - performing full update
Finding noisy pairs for iteration 2...
2025-04-14 10:09:01,053 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 10:09:01,604 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:09:01,748 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:09:01,762 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 10:09:01,762 - __main__ - INFO - Loading dataset
2025-04-14 10:09:02,819 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 10:09:24,458 - __main__ - INFO - Processing dataset
2025-04-14 10:09:24,706 - __main__ - INFO - Searching for noisy pairs starting from index 370
2025-04-14 10:09:30,868 - __main__ - INFO - Processed 30 samples without finding a noisy pair
2025-04-14 10:09:51,405 - __main__ - INFO - Processed 130 samples without finding a noisy pair
2025-04-14 10:10:12,889 - __main__ - INFO - Processed 230 samples without finding a noisy pair
2025-04-14 10:10:32,216 - __main__ - INFO - Processed 330 samples without finding a noisy pair
2025-04-14 10:10:52,467 - __main__ - INFO - Processed 430 samples without finding a noisy pair
2025-04-14 10:11:12,118 - __main__ - INFO - Processed 530 samples without finding a noisy pair
2025-04-14 10:11:30,755 - __main__ - INFO - Processed 630 samples without finding a noisy pair
2025-04-14 10:11:36,224 - __main__ - INFO - Noisy pair found at index 1030: chosen_logps=-280.0000, rejected_logps=-282.0000
2025-04-14 10:11:36,225 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 10:11:36,292 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_2
2025-04-14 10:11:36,293 - __main__ - INFO - New dataset offset: 1031
Updated dataset offset to: 1031 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 2
Worker node 1 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 10:11:52,003 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 10:11:52,195 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 10:11:52,316 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 10:11:52,562 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.29it/s]2025-04-14 10:11:52,707 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:11:52,723 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 10:11:52,723 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:11:52,741 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 10:11:52,871 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:11:52,896 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:11:52,911 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 10:11:52,911 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:11:53,014 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:11:53,026 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 10:11:53,026 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:12:09,300 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 10:12:09,418 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 10:12:09,558 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 10:12:19,476 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 10:12:19,588 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 10:12:19,735 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 10:12:23,555 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 10:12:23,668 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 10:12:23,819 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 10:12:26,548 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 10:12:26,593 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 10:12:26,659 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 10:12:27,472 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 10:12:27,706 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 10:12:27,914 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 10:12:30,522 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 10:12:30,779 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 10:12:31,185 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 10:12:31,204 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 10:12:31,518 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 10:12:32,004 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 10:12:34,521 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 10:12:34,759 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 10:12:35,039 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 10:12:35,061 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 10:12:35,473 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 10:12:35,988 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 10:12:38,090 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 10:12:38,204 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 10:12:38,899 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 10:12:39,263 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 10:12:39,776 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 10:12:40,198 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 10:12:41,977 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 10:12:42,806 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 10:12:42,864 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 10:12:42,924 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 10:12:43,989 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 10:12:44,211 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 10:12:46,524 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 10:12:47,136 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 10:12:47,251 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 10:12:47,482 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 10:12:48,322 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 10:12:48,339 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 10:12:49,868 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 10:12:51,233 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 10:12:51,310 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 10:12:51,317 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 10:12:51,656 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 10:12:53,105 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 10:12:53,111 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 10:12:53,216 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 10:12:53,218 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 10:12:54,286 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 10:12:55,611 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 10:12:55,705 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 10:12:58,422 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 10:12:58,435 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 10:13:00,388 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 10:13:00,519 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 10:13:01,494 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 10:13:01,575 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 10:14:16,440 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 10:14:17,527 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 10:14:19,303 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 10:14:20,673 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 10:14:20,745 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 10:14:21,614 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 10:14:23,853 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 10:14:25,253 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 10:14:26,714 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 10:14:29,387 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 10:14:29,549 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 10:14:32,303 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 10:14:32,394 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 10:14:33,571 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 10:14:35,708 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 10:14:36,467 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 10:14:37,167 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 10:14:38,318 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 10:14:39,985 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 10:14:40,968 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 10:14:41,279 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 10:14:43,511 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 10:14:47,260 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 10:14:47,877 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 10:14:47,944 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 10:14:48,838 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 10:14:48,983 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 10:14:49,637 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 10:14:50,849 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 10:14:53,734 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 10:16:37,725 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_0.npy
2025-04-14 10:16:37,726 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 124 negative comparisons
2025-04-14 10:16:38,035 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_1.npy
2025-04-14 10:16:38,035 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 135 negative comparisons
2025-04-14 10:16:40,640 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 10:16:40,876 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 10:16:44.498495779 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x741703e4d446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7416b91cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7416b91cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7416b91cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7416b91d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7416b91d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x741703fb45c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x741704a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x741704b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 10:16:44.520097387 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70f8398b2446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70f7eebcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70f7eebcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x70f7eebcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x70f7eebd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70f7eebd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70f839a195c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70f83a494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70f83a526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:16:44,849 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_2.npy
2025-04-14 10:16:44,849 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 129 negative comparisons
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 2
2025-04-14 10:16:47,722 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 2
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 10:17:05,672 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 10:17:06,222 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:17:06,362 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:17:06,375 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 10:17:22,937 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 10:17:25,868 - __main__ - INFO - Loaded gradients from node 0: 124 negative comparisons
2025-04-14 10:17:26,109 - __main__ - INFO - Loaded gradients from node 1: 135 negative comparisons
2025-04-14 10:17:26,345 - __main__ - INFO - Loaded gradients from node 2: 129 negative comparisons
2025-04-14 10:17:26,506 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 10:17:26,698 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 10:17:26,891 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119137287
2025-04-14 10:17:27,083 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107355429
2025-04-14 10:17:27,277 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84840883
2025-04-14 10:17:27,469 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47195233
2025-04-14 10:17:27,662 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22258196
2025-04-14 10:17:27,854 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802074
2025-04-14 10:17:28,047 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550292
2025-04-14 10:17:28,240 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554242
2025-04-14 10:17:28,590 - __main__ - INFO - Non-zero entries after thresholding: 47195233
2025-04-14 10:17:28,590 - __main__ - INFO - Testing stepsize: 0
2025-04-14 10:17:29,207 - __main__ - INFO -   chosen_logps: -279.80499, rejected_logps: -281.87360
2025-04-14 10:17:29,207 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 10:17:29,459 - __main__ - INFO -   chosen_logps: -279.41791, rejected_logps: -282.11713
2025-04-14 10:17:29,459 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 10:17:29,710 - __main__ - INFO -   chosen_logps: -277.43781, rejected_logps: -283.19147
2025-04-14 10:17:29,711 - __main__ - INFO - Testing stepsize: 1
2025-04-14 10:17:29,966 - __main__ - INFO -   chosen_logps: -276.06366, rejected_logps: -284.35046
2025-04-14 10:17:29,966 - __main__ - INFO - Testing stepsize: 2
2025-04-14 10:17:30,225 - __main__ - INFO -   chosen_logps: -273.02826, rejected_logps: -286.77826
2025-04-14 10:17:30,225 - __main__ - INFO - Testing stepsize: 5
2025-04-14 10:17:30,484 - __main__ - INFO -   chosen_logps: -262.19952, rejected_logps: -294.16367
2025-04-14 10:17:30,484 - __main__ - INFO - Testing stepsize: 10
2025-04-14 10:17:30,745 - __main__ - INFO -   chosen_logps: -246.38171, rejected_logps: -307.68195
2025-04-14 10:17:30,745 - __main__ - INFO - Testing stepsize: 15
2025-04-14 10:17:31,008 - __main__ - INFO -   chosen_logps: -231.59575, rejected_logps: -320.76968
2025-04-14 10:17:31,591 - __main__ - INFO - Update scale: 0.007544444444444445
2025-04-14 10:17:31,673 - __main__ - INFO - Model weights updated successfully
2025-04-14 10:17:32,513 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:17:32,545 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:17:33,418 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 10:17:33,451 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 10:17:33,451 - __main__ - INFO - Skipping full model checkpoint for iteration 2
2025-04-14 10:17:33,451 - __main__ - INFO - Iteration 2 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 2
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 2
Iteration 2 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 3
===========================================
Iteration 3 is in valid list - performing full update
Finding noisy pairs for iteration 3...
2025-04-14 10:17:54,465 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 10:17:55,015 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:17:55,160 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:17:55,174 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 10:17:55,174 - __main__ - INFO - Loading dataset
2025-04-14 10:17:56,032 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 10:18:17,562 - __main__ - INFO - Processing dataset
2025-04-14 10:18:17,807 - __main__ - INFO - Searching for noisy pairs starting from index 1031
2025-04-14 10:18:32,760 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 10:18:46,356 - __main__ - INFO - Noisy pair found at index 1163: chosen_logps=-147.0000, rejected_logps=-149.0000
2025-04-14 10:18:46,357 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 10:18:46,423 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_3
2025-04-14 10:18:46,424 - __main__ - INFO - New dataset offset: 1164
Updated dataset offset to: 1164 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 3
Worker node 1 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 10:19:02,067 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 10:19:02,560 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 10:19:02,624 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.19it/s]2025-04-14 10:19:02,768 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:19:02,784 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 10:19:02,784 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.74it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 10:19:03,019 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 10:19:03,103 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.24it/s]2025-04-14 10:19:03,256 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:19:03,271 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 10:19:03,271 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 10:19:03,569 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:19:03,709 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:19:03,722 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 10:19:03,722 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:19:19,335 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 10:19:19,790 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 10:19:20,289 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 10:19:29,852 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 10:19:29,895 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 10:19:30,343 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 10:19:33,679 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 10:19:33,728 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 10:19:34,224 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 10:19:37,098 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 10:19:37,208 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 10:19:37,309 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 10:19:37,657 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 10:19:37,709 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 10:19:38,199 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 10:19:40,696 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 10:19:40,909 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 10:19:41,506 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 10:19:41,503 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 10:19:41,752 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 10:19:42,013 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 10:19:44,749 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 10:19:45,007 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 10:19:45,386 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 10:19:45,480 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 10:19:45,548 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 10:19:46,021 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 10:19:48,841 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 10:19:48,993 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 10:19:49,467 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 10:19:49,489 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 10:19:49,960 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 10:19:50,345 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 10:19:52,614 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 10:19:53,108 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 10:19:53,214 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 10:19:53,742 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 10:19:53,930 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 10:19:54,242 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 10:19:56,568 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 10:19:57,331 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 10:19:57,776 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 10:19:58,146 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 10:19:58,164 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 10:19:58,555 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 10:20:01,190 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 10:20:01,275 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 10:20:01,582 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 10:20:02,559 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 10:20:02,565 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 10:20:02,658 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 10:20:02,665 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 10:20:02,685 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 10:20:02,695 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 10:20:05,378 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 10:20:05,910 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 10:20:06,443 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 10:20:10,046 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 10:20:10,175 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 10:20:10,214 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 10:20:10,248 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 10:20:10,265 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 10:20:10,287 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 10:21:34,428 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 10:21:34,831 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 10:21:36,921 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 10:21:38,450 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 10:21:38,742 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 10:21:39,969 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 10:21:41,919 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 10:21:42,276 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 10:21:44,799 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 10:21:46,073 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 10:21:46,258 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 10:21:48,012 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 10:21:49,524 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 10:21:49,747 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 10:21:50,599 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 10:21:52,989 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 10:21:53,015 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 10:21:57,563 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 10:21:58,926 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 10:21:59,020 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 10:22:00,637 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 10:22:02,452 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 10:22:03,389 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 10:22:04,287 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 10:22:06,115 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 10:22:06,549 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 10:22:07,692 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 10:22:07,942 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 10:22:10,459 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 10:22:10,613 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 10:24:00,090 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_0.npy
2025-04-14 10:24:00,090 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 154 negative comparisons
[E414 10:24:00.016812528 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79c6d1d6c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79c6875cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79c6875cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x79c6875cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x79c6875d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79c6875d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79c6d22985c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x79c6d2c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x79c6d2d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 10:24:00.044119409 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x739df7f3d446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x739dad3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x739dad3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x739dad3cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x739dad3d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x739dad3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x739df80a45c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x739df8a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x739df8b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 10:24:00.052753429 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79b18149d446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79b1367cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79b1367cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x79b1367cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x79b1367d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79b1367d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79b1816045c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x79b182094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x79b182126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 10:24:02.967774748 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x770f64ffd446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x770f1a3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x770f1a3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x770f1a3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x770f1a3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x770f1a3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x770f651645c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x770f65a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x770f65b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:24:02,226 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_1.npy
2025-04-14 10:24:02,226 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 160 negative comparisons
2025-04-14 10:24:03,219 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 10:24:05,113 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 10:24:07,138 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_2.npy
2025-04-14 10:24:07,139 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 148 negative comparisons
Waiting for worker nodes to complete gradient computation...
2025-04-14 10:24:10,293 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 3
Worker node 2 successfully completed gradient computation for iteration 3
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 10:24:28,385 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 10:24:28,933 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:24:29,073 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:24:29,085 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 10:24:45,616 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 10:24:48,498 - __main__ - INFO - Loaded gradients from node 0: 154 negative comparisons
2025-04-14 10:24:48,736 - __main__ - INFO - Loaded gradients from node 1: 160 negative comparisons
2025-04-14 10:24:48,968 - __main__ - INFO - Loaded gradients from node 2: 148 negative comparisons
2025-04-14 10:24:49,128 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 10:24:49,318 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 10:24:49,507 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119125720
2025-04-14 10:24:49,697 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107341417
2025-04-14 10:24:49,886 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84832820
2025-04-14 10:24:50,076 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47200606
2025-04-14 10:24:50,265 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22251769
2025-04-14 10:24:50,455 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802662
2025-04-14 10:24:50,644 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552827
2025-04-14 10:24:50,834 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555956
2025-04-14 10:24:51,179 - __main__ - INFO - Non-zero entries after thresholding: 47200606
2025-04-14 10:24:51,179 - __main__ - INFO - Testing stepsize: 0
2025-04-14 10:24:51,920 - __main__ - INFO -   chosen_logps: -147.81650, rejected_logps: -147.74109
2025-04-14 10:24:51,920 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 10:24:52,298 - __main__ - INFO -   chosen_logps: -147.46687, rejected_logps: -147.88171
2025-04-14 10:24:52,298 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 10:24:52,680 - __main__ - INFO -   chosen_logps: -145.69919, rejected_logps: -148.23039
2025-04-14 10:24:52,680 - __main__ - INFO - Testing stepsize: 1
2025-04-14 10:24:53,065 - __main__ - INFO -   chosen_logps: -144.22403, rejected_logps: -148.95909
2025-04-14 10:24:53,065 - __main__ - INFO - Testing stepsize: 2
2025-04-14 10:24:53,454 - __main__ - INFO -   chosen_logps: -140.82198, rejected_logps: -150.02199
2025-04-14 10:24:53,454 - __main__ - INFO - Testing stepsize: 5
2025-04-14 10:24:53,845 - __main__ - INFO -   chosen_logps: -132.30038, rejected_logps: -153.44557
2025-04-14 10:24:53,846 - __main__ - INFO - Testing stepsize: 10
2025-04-14 10:24:54,237 - __main__ - INFO -   chosen_logps: -120.81024, rejected_logps: -158.98709
2025-04-14 10:24:54,237 - __main__ - INFO - Testing stepsize: 15
2025-04-14 10:24:54,628 - __main__ - INFO -   chosen_logps: -111.17265, rejected_logps: -164.80333
2025-04-14 10:24:55,385 - __main__ - INFO - Update scale: 0.008983333333333334
2025-04-14 10:24:55,467 - __main__ - INFO - Model weights updated successfully
2025-04-14 10:24:56,310 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:24:56,347 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:24:57,230 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 10:24:57,265 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 10:24:57,265 - __main__ - INFO - Skipping full model checkpoint for iteration 3
2025-04-14 10:24:57,265 - __main__ - INFO - Iteration 3 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 3
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 3
Iteration 3 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 4
===========================================
Iteration 4 is in valid list - performing full update
Finding noisy pairs for iteration 4...
2025-04-14 10:25:18,416 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 10:25:18,966 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:25:19,112 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:25:19,127 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 10:25:19,127 - __main__ - INFO - Loading dataset
2025-04-14 10:25:20,080 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 10:25:41,421 - __main__ - INFO - Processing dataset
2025-04-14 10:25:41,664 - __main__ - INFO - Searching for noisy pairs starting from index 1164
2025-04-14 10:25:48,572 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 10:26:09,949 - __main__ - INFO - Processed 136 samples without finding a noisy pair
2025-04-14 10:26:20,078 - __main__ - INFO - Noisy pair found at index 1344: chosen_logps=-268.0000, rejected_logps=-270.0000
2025-04-14 10:26:20,079 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 10:26:20,146 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_4
2025-04-14 10:26:20,147 - __main__ - INFO - New dataset offset: 1345
Updated dataset offset to: 1345 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 4
Worker node 2 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 10:26:35,585 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 10:26:35,812 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 10:26:35,880 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.40it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]2025-04-14 10:26:36,142 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 10:26:36,286 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:26:36,301 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 10:26:36,301 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
2025-04-14 10:26:36,356 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:26:36,437 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:26:36,508 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:26:36,523 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 10:26:36,523 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:26:36,578 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:26:36,591 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 10:26:36,591 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:26:52,856 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 10:26:53,079 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 10:26:53,175 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 10:27:03,046 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 10:27:03,245 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 10:27:03,373 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 10:27:06,834 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 10:27:07,161 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 10:27:07,178 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 10:27:09,853 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 10:27:10,256 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 10:27:10,562 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 10:27:10,788 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 10:27:11,160 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 10:27:11,177 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 10:27:14,100 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 10:27:14,154 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 10:27:14,245 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 10:27:14,638 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 10:27:15,018 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 10:27:15,111 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 10:27:18,059 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 10:27:18,555 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 10:27:18,579 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 10:27:18,881 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 10:27:18,944 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 10:27:18,996 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 10:27:21,997 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 10:27:22,369 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 10:27:22,480 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 10:27:22,479 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 10:27:23,002 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 10:27:23,473 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 10:27:25,859 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 10:27:26,065 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 10:27:26,177 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 10:27:26,538 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 10:27:27,004 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 10:27:27,485 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 10:27:29,761 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 10:27:30,247 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 10:27:30,546 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 10:27:30,545 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 10:27:30,998 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 10:27:32,124 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 10:27:33,731 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 10:27:34,404 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 10:27:34,504 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 10:27:35,092 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 10:27:35,094 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 10:27:35,133 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 10:27:35,141 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 10:27:36,327 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 10:27:36,335 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 10:27:38,134 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 10:27:38,471 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 10:27:38,974 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 10:27:42,361 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 10:27:42,415 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 10:27:42,767 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 10:27:42,797 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 10:27:43,481 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 10:27:43,493 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 10:29:01,400 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 10:29:02,928 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 10:29:03,525 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 10:29:05,409 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 10:29:06,817 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 10:29:07,940 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 10:29:10,284 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 10:29:11,568 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 10:29:12,992 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 10:29:14,784 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 10:29:15,129 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 10:29:15,152 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 10:29:16,298 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 10:29:18,514 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 10:29:20,114 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 10:29:21,610 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 10:29:22,046 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 10:29:24,093 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 10:29:25,357 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 10:29:25,471 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 10:29:30,190 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 10:29:30,422 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 10:29:31,135 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 10:29:33,345 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 10:29:33,499 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 10:29:33,800 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 10:29:33,864 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 10:29:34,215 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 10:29:34,953 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 10:29:36,969 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 10:31:23.245665232 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74c3ed0a5446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74c3a23cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74c3a23cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x74c3a23cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x74c3a23d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74c3a23d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74c3ed20c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74c3edc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74c3edd26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:31:23,492 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_0.npy
2025-04-14 10:31:23,492 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 206 negative comparisons
2025-04-14 10:31:23,688 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_1.npy
2025-04-14 10:31:23,688 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 183 negative comparisons
2025-04-14 10:31:26,373 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 10:31:26,552 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 10:31:28.279819431 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7556a7693446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75565c9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75565c9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x75565c9cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x75565c9d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75565c9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7556a77ee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7556a8294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7556a8326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:31:28,633 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_2.npy
2025-04-14 10:31:28,633 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 209 negative comparisons
2025-04-14 10:31:31,454 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 4
Worker node 2 successfully completed gradient computation for iteration 4
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 10:31:49,483 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 10:31:50,031 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:31:50,173 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:31:50,185 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 10:32:06,626 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 10:32:09,516 - __main__ - INFO - Loaded gradients from node 0: 206 negative comparisons
2025-04-14 10:32:09,749 - __main__ - INFO - Loaded gradients from node 1: 183 negative comparisons
2025-04-14 10:32:09,983 - __main__ - INFO - Loaded gradients from node 2: 209 negative comparisons
2025-04-14 10:32:10,144 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 10:32:10,335 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 10:32:10,526 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119132817
2025-04-14 10:32:10,718 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107347974
2025-04-14 10:32:10,910 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84832973
2025-04-14 10:32:11,103 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47199155
2025-04-14 10:32:11,295 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22257081
2025-04-14 10:32:11,487 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803261
2025-04-14 10:32:11,677 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550315
2025-04-14 10:32:11,868 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555033
2025-04-14 10:32:12,216 - __main__ - INFO - Non-zero entries after thresholding: 47199155
2025-04-14 10:32:12,216 - __main__ - INFO - Testing stepsize: 0
2025-04-14 10:32:12,870 - __main__ - INFO -   chosen_logps: -267.77640, rejected_logps: -269.53967
2025-04-14 10:32:12,870 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 10:32:13,155 - __main__ - INFO -   chosen_logps: -267.47745, rejected_logps: -269.91571
2025-04-14 10:32:13,155 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 10:32:13,440 - __main__ - INFO -   chosen_logps: -265.58093, rejected_logps: -271.26245
2025-04-14 10:32:13,440 - __main__ - INFO - Testing stepsize: 1
2025-04-14 10:32:13,729 - __main__ - INFO -   chosen_logps: -263.16119, rejected_logps: -273.13257
2025-04-14 10:32:13,729 - __main__ - INFO - Testing stepsize: 2
2025-04-14 10:32:14,022 - __main__ - INFO -   chosen_logps: -259.33939, rejected_logps: -275.81110
2025-04-14 10:32:14,022 - __main__ - INFO - Testing stepsize: 5
2025-04-14 10:32:14,314 - __main__ - INFO -   chosen_logps: -247.35143, rejected_logps: -285.72571
2025-04-14 10:32:14,315 - __main__ - INFO - Testing stepsize: 10
2025-04-14 10:32:14,612 - __main__ - INFO -   chosen_logps: -228.90964, rejected_logps: -305.25122
2025-04-14 10:32:14,612 - __main__ - INFO - Testing stepsize: 15
2025-04-14 10:32:14,908 - __main__ - INFO -   chosen_logps: -212.64833, rejected_logps: -327.19223
2025-04-14 10:32:15,535 - __main__ - INFO - Update scale: 0.011627777777777779
2025-04-14 10:32:15,616 - __main__ - INFO - Model weights updated successfully
2025-04-14 10:32:16,462 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:32:16,499 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:32:17,760 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 10:32:17,798 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 10:32:17,798 - __main__ - INFO - Skipping full model checkpoint for iteration 4
2025-04-14 10:32:17,798 - __main__ - INFO - Iteration 4 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 4
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 4
Iteration 4 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 5
===========================================
Iteration 5 is in valid list - performing full update
Finding noisy pairs for iteration 5...
2025-04-14 10:32:38,899 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 10:32:39,449 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:32:39,595 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:32:39,609 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 10:32:39,609 - __main__ - INFO - Loading dataset
2025-04-14 10:32:40,653 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 10:33:02,134 - __main__ - INFO - Processing dataset
2025-04-14 10:33:02,379 - __main__ - INFO - Searching for noisy pairs starting from index 1345
2025-04-14 10:33:13,576 - __main__ - INFO - Processed 55 samples without finding a noisy pair
2025-04-14 10:33:34,207 - __main__ - INFO - Processed 155 samples without finding a noisy pair
2025-04-14 10:33:39,649 - __main__ - INFO - Noisy pair found at index 1530: chosen_logps=-360.0000, rejected_logps=-362.0000
2025-04-14 10:33:39,650 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 10:33:39,717 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_5
2025-04-14 10:33:39,718 - __main__ - INFO - New dataset offset: 1531
Updated dataset offset to: 1531 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 5
Worker node 2 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 10:33:55,111 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 10:33:55,579 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 10:33:55,657 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 10:33:55,801 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:33:55,816 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 10:33:55,816 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 10:33:56,135 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:33:56,277 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:33:56,290 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 10:33:56,290 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:33:56,367 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.62it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.78it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
2025-04-14 10:33:56,909 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:33:57,061 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:33:57,076 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 10:33:57,076 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:34:12,431 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 10:34:12,841 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 10:34:13,636 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 10:34:22,582 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 10:34:22,944 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 10:34:23,695 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 10:34:26,348 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 10:34:26,741 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 10:34:27,550 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 10:34:29,746 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 10:34:29,793 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 10:34:30,261 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 10:34:30,281 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 10:34:31,100 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 10:34:31,470 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 10:34:33,633 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 10:34:33,715 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 10:34:34,090 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 10:34:34,092 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 10:34:35,248 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 10:34:35,544 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 10:34:37,546 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 10:34:38,147 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 10:34:38,387 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 10:34:38,624 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 10:34:39,423 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 10:34:39,845 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 10:34:41,426 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 10:34:42,301 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 10:34:42,328 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 10:34:42,877 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 10:34:43,334 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 10:34:44,111 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 10:34:45,328 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 10:34:46,193 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 10:34:46,715 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 10:34:46,899 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 10:34:47,221 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 10:34:48,225 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 10:34:49,690 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 10:34:50,214 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 10:34:50,888 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 10:34:51,198 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 10:34:51,607 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 10:34:52,660 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 10:34:53,531 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 10:34:54,336 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 10:34:54,344 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 10:34:54,949 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 10:34:55,270 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 10:34:56,174 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 10:34:56,178 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 10:34:56,977 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 10:34:56,979 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 10:34:57,268 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 10:34:59,194 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 10:34:59,977 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 10:35:01,221 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 10:35:01,840 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 10:35:03,941 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 10:35:04,057 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 10:35:04,351 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 10:35:04,468 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 10:36:22,735 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 10:36:23,006 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 10:36:24,652 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 10:36:25,379 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 10:36:25,958 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 10:36:30,014 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 10:36:31,183 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 10:36:31,494 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 10:36:32,725 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 10:36:34,727 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 10:36:34,909 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 10:36:35,090 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 10:36:38,478 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 10:36:38,762 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 10:36:40,605 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 10:36:42,786 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 10:36:43,420 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 10:36:45,572 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 10:36:47,385 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 10:36:47,500 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 10:36:47,565 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 10:36:49,344 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 10:36:53,437 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 10:36:53,601 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 10:36:54,717 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 10:36:55,200 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 10:36:56,266 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 10:36:56,630 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 10:36:56,686 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 10:36:58,969 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 10:38:45.477435359 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74e2171ce446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74e1cc5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74e1cc5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x74e1cc5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x74e1cc5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74e1cc5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74e2173355c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74e217c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74e217d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:38:45,851 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_1.npy
2025-04-14 10:38:45,852 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 90 negative comparisons
2025-04-14 10:38:46,702 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_0.npy
2025-04-14 10:38:46,703 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 102 negative comparisons
[E414 10:38:46.649016275 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x737f9ba64446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x737f50dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x737f50dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x737f50dcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x737f50dd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x737f50dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x737f9bbcb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x737f9c694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x737f9c726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 10:38:46.650205773 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a1ee6bb2446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7a1e9bfcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7a1e9bfcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7a1e9bfcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7a1e9bfd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a1e9bfd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7a1ee6d195c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7a1ee7694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7a1ee7726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:38:48,751 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 10:38:49,892 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 10:38:52.498956284 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7652578de446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76520cbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76520cbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x76520cbcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x76520cbd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76520cbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x765257a455c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x765258494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x765258526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 10:38:52.511946060 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7757d7087446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x77578c3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x77578c3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x77578c3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x77578c3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x77578c3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7757d71ee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7757d7c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7757d7d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:38:52,863 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_2.npy
2025-04-14 10:38:52,864 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 113 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 5
2025-04-14 10:38:55,714 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 5
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 10:39:12,994 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.72it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
2025-04-14 10:39:13,539 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:39:13,680 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:39:13,692 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 10:39:30,147 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 10:39:33,080 - __main__ - INFO - Loaded gradients from node 0: 102 negative comparisons
2025-04-14 10:39:33,317 - __main__ - INFO - Loaded gradients from node 1: 90 negative comparisons
2025-04-14 10:39:33,551 - __main__ - INFO - Loaded gradients from node 2: 113 negative comparisons
2025-04-14 10:39:33,711 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 10:39:33,901 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 10:39:34,090 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119135269
2025-04-14 10:39:34,280 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107357084
2025-04-14 10:39:34,469 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84839534
2025-04-14 10:39:34,658 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47195867
2025-04-14 10:39:34,848 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252644
2025-04-14 10:39:35,038 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8805755
2025-04-14 10:39:35,227 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1549245
2025-04-14 10:39:35,417 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553643
2025-04-14 10:39:35,761 - __main__ - INFO - Non-zero entries after thresholding: 47195867
2025-04-14 10:39:35,761 - __main__ - INFO - Using relaxed comparison (count_negative: 305)
2025-04-14 10:39:35,761 - __main__ - INFO - Testing stepsize: 0
2025-04-14 10:39:36,431 - __main__ - INFO -   chosen_logps: -360.31830, rejected_logps: -361.51202
2025-04-14 10:39:36,431 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 10:39:36,730 - __main__ - INFO -   chosen_logps: -359.79114, rejected_logps: -361.67371
2025-04-14 10:39:36,730 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 10:39:37,031 - __main__ - INFO -   chosen_logps: -357.74493, rejected_logps: -363.55750
2025-04-14 10:39:37,031 - __main__ - INFO - Testing stepsize: 1
2025-04-14 10:39:37,334 - __main__ - INFO -   chosen_logps: -354.70114, rejected_logps: -364.29691
2025-04-14 10:39:37,334 - __main__ - INFO - Testing stepsize: 2
2025-04-14 10:39:37,643 - __main__ - INFO -   chosen_logps: -348.98541, rejected_logps: -367.79666
2025-04-14 10:39:37,643 - __main__ - INFO - Testing stepsize: 5
2025-04-14 10:39:37,951 - __main__ - INFO -   chosen_logps: -334.60852, rejected_logps: -377.85080
2025-04-14 10:39:37,951 - __main__ - INFO - Testing stepsize: 10
2025-04-14 10:39:38,261 - __main__ - INFO -   chosen_logps: -312.34274, rejected_logps: -397.89362
2025-04-14 10:39:38,261 - __main__ - INFO - Testing stepsize: 15
2025-04-14 10:39:38,573 - __main__ - INFO -   chosen_logps: -295.56299, rejected_logps: -419.17456
2025-04-14 10:39:39,224 - __main__ - INFO - Update scale: 0.005930555555555556
2025-04-14 10:39:39,226 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 10:39:39,305 - __main__ - INFO - Model weights updated successfully
2025-04-14 10:39:40,436 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:39:40,474 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:39:41,362 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 10:39:41,398 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 10:39:41,398 - __main__ - INFO - Skipping full model checkpoint for iteration 5
2025-04-14 10:39:41,398 - __main__ - INFO - Iteration 5 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 5
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 5
Iteration 5 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 6
===========================================
Iteration 6 is in valid list - performing full update
Finding noisy pairs for iteration 6...
2025-04-14 10:40:02,480 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 10:40:03,032 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:40:03,177 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:40:03,191 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 10:40:03,191 - __main__ - INFO - Loading dataset
2025-04-14 10:40:04,154 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 10:40:25,685 - __main__ - INFO - Processing dataset
2025-04-14 10:40:25,928 - __main__ - INFO - Searching for noisy pairs starting from index 1531
2025-04-14 10:40:39,982 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 10:40:44,533 - __main__ - INFO - Noisy pair found at index 1624: chosen_logps=-127.5000, rejected_logps=-130.0000
2025-04-14 10:40:44,533 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 10:40:44,600 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_6
2025-04-14 10:40:44,601 - __main__ - INFO - New dataset offset: 1625
Updated dataset offset to: 1625 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 6
Worker node 1 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 10:40:59,912 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 10:41:00,045 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]2025-04-14 10:41:00,268 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.64it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 10:41:00,460 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.78it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
2025-04-14 10:41:00,588 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]2025-04-14 10:41:00,604 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:41:00,619 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 10:41:00,619 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 10:41:00,744 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:41:00,759 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 10:41:00,759 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:41:00,825 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:41:00,965 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:41:00,978 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 10:41:00,978 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:41:17,162 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 10:41:17,352 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 10:41:17,486 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 10:41:27,384 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 10:41:27,477 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 10:41:27,542 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 10:41:31,196 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 10:41:31,337 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 10:41:31,459 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 10:41:34,532 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 10:41:34,560 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 10:41:34,590 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 10:41:35,109 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 10:41:35,161 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 10:41:35,495 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 10:41:38,543 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 10:41:38,803 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 10:41:38,889 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 10:41:38,961 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 10:41:38,992 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 10:41:39,393 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 10:41:42,337 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 10:41:42,633 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 10:41:42,877 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 10:41:42,970 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 10:41:43,274 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 10:41:43,312 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 10:41:45,766 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 10:41:46,291 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 10:41:46,863 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 10:41:47,221 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 10:41:47,222 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 10:41:47,251 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 10:41:50,095 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 10:41:50,732 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 10:41:50,740 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 10:41:51,158 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 10:41:51,166 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 10:41:51,428 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 10:41:54,500 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 10:41:54,574 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 10:41:54,788 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 10:41:55,289 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 10:41:55,528 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 10:41:55,923 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 10:41:58,426 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 10:41:59,029 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 10:41:59,143 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 10:41:59,712 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 10:41:59,721 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 10:42:00,076 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 10:42:00,086 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 10:42:00,265 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 10:42:00,272 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 10:42:02,523 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 10:42:02,874 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 10:42:04,046 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 10:42:06,910 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 10:42:07,139 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 10:42:07,605 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 10:42:07,714 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 10:42:08,409 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 10:42:08,454 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 10:43:30,350 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 10:43:31,384 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 10:43:33,719 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 10:43:35,455 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 10:43:35,982 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 10:43:37,503 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 10:43:39,290 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 10:43:41,078 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 10:43:41,135 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 10:43:43,051 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 10:43:43,552 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 10:43:46,408 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 10:43:46,547 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 10:43:46,997 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 10:43:48,298 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 10:43:49,719 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 10:43:51,114 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 10:43:51,970 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 10:43:55,104 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 10:43:56,861 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 10:43:58,560 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 10:43:58,583 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 10:44:00,583 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 10:44:01,929 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 10:44:02,211 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 10:44:03,499 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 10:44:03,536 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 10:44:04,027 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 10:44:07,045 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 10:44:10,081 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 10:45:56.418859709 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e69a2a31446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e6957dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e6957dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7e6957dcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7e6957dd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e6957dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e69a2b985c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e69a3494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e69a3526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:45:56,713 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_0.npy
2025-04-14 10:45:56,713 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 158 negative comparisons
2025-04-14 10:45:59,524 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 10:46:03,960 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_1.npy
2025-04-14 10:46:03,960 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 161 negative comparisons
Waiting for worker nodes to complete gradient computation...
2025-04-14 10:46:06,791 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 10:46:08.574921735 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72140c638446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7213c19cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7213c19cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7213c19cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7213c19d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7213c19d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72140c79f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72140d294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72140d326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 10:46:08.588140578 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x756649ba1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7565fefcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7565fefcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7565fefcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7565fefd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7565fefd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x756649d085c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x75664a894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x75664a926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:46:08,933 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_2.npy
2025-04-14 10:46:08,933 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 148 negative comparisons
2025-04-14 10:46:11,864 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 6
Worker node 2 successfully completed gradient computation for iteration 6
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 10:46:29,007 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 10:46:29,556 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:46:29,696 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:46:29,709 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 10:46:46,180 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 10:46:49,081 - __main__ - INFO - Loaded gradients from node 0: 158 negative comparisons
2025-04-14 10:46:49,319 - __main__ - INFO - Loaded gradients from node 1: 161 negative comparisons
2025-04-14 10:46:49,554 - __main__ - INFO - Loaded gradients from node 2: 148 negative comparisons
2025-04-14 10:46:49,714 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 10:46:49,904 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 10:46:50,095 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119131214
2025-04-14 10:46:50,286 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107343009
2025-04-14 10:46:50,477 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84829067
2025-04-14 10:46:50,668 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47197333
2025-04-14 10:46:50,859 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255883
2025-04-14 10:46:51,049 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804192
2025-04-14 10:46:51,240 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551733
2025-04-14 10:46:51,431 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554993
2025-04-14 10:46:51,779 - __main__ - INFO - Non-zero entries after thresholding: 47197333
2025-04-14 10:46:51,779 - __main__ - INFO - Testing stepsize: 0
2025-04-14 10:46:52,509 - __main__ - INFO -   chosen_logps: -127.93317, rejected_logps: -129.48666
2025-04-14 10:46:52,509 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 10:46:52,874 - __main__ - INFO -   chosen_logps: -127.56737, rejected_logps: -129.48666
2025-04-14 10:46:52,874 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 10:46:53,240 - __main__ - INFO -   chosen_logps: -125.69307, rejected_logps: -129.82385
2025-04-14 10:46:53,240 - __main__ - INFO - Testing stepsize: 1
2025-04-14 10:46:53,609 - __main__ - INFO -   chosen_logps: -123.85789, rejected_logps: -130.13362
2025-04-14 10:46:53,610 - __main__ - INFO - Testing stepsize: 2
2025-04-14 10:46:53,983 - __main__ - INFO -   chosen_logps: -120.26935, rejected_logps: -130.80901
2025-04-14 10:46:53,983 - __main__ - INFO - Testing stepsize: 5
2025-04-14 10:46:54,358 - __main__ - INFO -   chosen_logps: -110.51602, rejected_logps: -133.14046
2025-04-14 10:46:54,358 - __main__ - INFO - Testing stepsize: 10
2025-04-14 10:46:54,734 - __main__ - INFO -   chosen_logps: -96.61241, rejected_logps: -136.91936
2025-04-14 10:46:54,734 - __main__ - INFO - Testing stepsize: 15
2025-04-14 10:46:55,112 - __main__ - INFO -   chosen_logps: -85.22034, rejected_logps: -140.78029
2025-04-14 10:46:55,853 - __main__ - INFO - Update scale: 0.009080555555555555
2025-04-14 10:46:55,941 - __main__ - INFO - Model weights updated successfully
2025-04-14 10:46:56,927 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:46:56,963 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:46:57,989 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 10:46:58,024 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 10:46:58,024 - __main__ - INFO - Skipping full model checkpoint for iteration 6
2025-04-14 10:46:58,024 - __main__ - INFO - Iteration 6 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 6
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 6
Iteration 6 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 7
===========================================
Iteration 7 is in valid list - performing full update
Finding noisy pairs for iteration 7...
2025-04-14 10:47:19,694 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 10:47:20,243 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:47:20,388 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:47:20,403 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 10:47:20,403 - __main__ - INFO - Loading dataset
2025-04-14 10:47:21,255 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 10:47:42,838 - __main__ - INFO - Processing dataset
2025-04-14 10:47:43,083 - __main__ - INFO - Searching for noisy pairs starting from index 1625
2025-04-14 10:47:50,681 - __main__ - INFO - Noisy pair found at index 1663: chosen_logps=-49.7500, rejected_logps=-50.5000
2025-04-14 10:47:50,682 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 10:47:50,750 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_7
2025-04-14 10:47:50,751 - __main__ - INFO - New dataset offset: 1664
Updated dataset offset to: 1664 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 7
Worker node 1 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 10:48:05,127 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]2025-04-14 10:48:05,420 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 10:48:05,528 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]2025-04-14 10:48:05,679 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]2025-04-14 10:48:05,823 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:48:05,837 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 10:48:05,838 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 10:48:05,965 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 10:48:06,085 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:48:06,117 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:48:06,132 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 10:48:06,132 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:48:06,229 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:48:06,242 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 10:48:06,242 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:48:22,447 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 10:48:22,697 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 10:48:22,794 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 10:48:32,789 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 10:48:32,940 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 10:48:33,076 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 10:48:36,597 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 10:48:36,872 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 10:48:36,983 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 10:48:39,741 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 10:48:39,804 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 10:48:40,315 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 10:48:40,585 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 10:48:40,770 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 10:48:40,993 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 10:48:44,199 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 10:48:44,348 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 10:48:44,373 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 10:48:44,462 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 10:48:44,550 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 10:48:44,964 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 10:48:47,821 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 10:48:47,914 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 10:48:48,185 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 10:48:48,229 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 10:48:48,744 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 10:48:49,307 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 10:48:51,660 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 10:48:52,024 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 10:48:52,089 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 10:48:52,493 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 10:48:52,882 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 10:48:53,489 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 10:48:55,777 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 10:48:55,827 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 10:48:56,542 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 10:48:56,545 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 10:48:56,832 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 10:48:57,418 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 10:48:59,710 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 10:49:00,115 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 10:49:01,227 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 10:49:01,254 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 10:49:01,388 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 10:49:01,713 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 10:49:04,130 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 10:49:04,308 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 10:49:04,810 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 10:49:05,445 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 10:49:05,455 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 10:49:05,667 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 10:49:05,671 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 10:49:06,029 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 10:49:06,032 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 10:49:08,546 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 10:49:08,763 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 10:49:09,290 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 10:49:12,811 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 10:49:12,889 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 10:49:13,285 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 10:49:13,657 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 10:49:13,740 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 10:49:13,946 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 10:50:35,771 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 10:50:36,505 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 10:50:39,396 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 10:50:40,376 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 10:50:43,060 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 10:50:43,471 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 10:50:43,971 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 10:50:44,061 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 10:50:45,782 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 10:50:47,488 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 10:50:47,817 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 10:50:50,784 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 10:50:51,077 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 10:50:51,553 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 10:50:52,262 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 10:50:56,246 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 10:50:56,888 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 10:50:58,413 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 10:50:59,332 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 10:50:59,659 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 10:51:01,146 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 10:51:04,957 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 10:51:06,131 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 10:51:07,464 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 10:51:08,740 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 10:51:09,058 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 10:51:09,089 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 10:51:09,655 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 10:51:12,188 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 10:51:13,517 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 10:53:02,068 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_1.npy
2025-04-14 10:53:02,069 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 71 negative comparisons
[E414 10:53:02.952535538 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72c6ea5ef446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72c69f9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72c69f9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x72c69f9cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x72c69f9d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72c69f9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72c6ea7565c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72c6eb094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72c6eb126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:53:05,154 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 10:53:07.712257684 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fbd11c09446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7fbcc6fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7fbcc6fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7fbcc6fcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7fbcc6fd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fbcc6fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7fbd11d705c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7fbd12894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7fbd12926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:53:08,010 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_2.npy
2025-04-14 10:53:08,010 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 99 negative comparisons
[E414 10:53:10.297528766 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79b3c8043446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79b37d3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79b37d3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x79b37d3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x79b37d3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79b37d3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79b3c81aa5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x79b3c8c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x79b3c8d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 10:53:10.329372173 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70061cda9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7005d21cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7005d21cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7005d21cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7005d21d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7005d21d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70061cf105c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70061d894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70061d926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 10:53:10.336882515 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x756c5a3ce446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x756c0f7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x756c0f7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x756c0f7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x756c0f7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x756c0f7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x756c5a5355c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x756c5ae94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x756c5af26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 10:53:10,640 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_0.npy
2025-04-14 10:53:10,640 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 94 negative comparisons
2025-04-14 10:53:10,909 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 7
2025-04-14 10:53:13,383 - __main__ - INFO - Node 0: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 7
Waiting for worker nodes to complete gradient computation...
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 10:53:31,941 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 10:53:32,486 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:53:32,629 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:53:32,642 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 10:53:49,111 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 10:53:52,059 - __main__ - INFO - Loaded gradients from node 0: 94 negative comparisons
2025-04-14 10:53:52,293 - __main__ - INFO - Loaded gradients from node 1: 71 negative comparisons
2025-04-14 10:53:52,527 - __main__ - INFO - Loaded gradients from node 2: 99 negative comparisons
2025-04-14 10:53:52,686 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 10:53:52,875 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 10:53:53,064 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119136923
2025-04-14 10:53:53,254 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107357189
2025-04-14 10:53:53,443 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84838722
2025-04-14 10:53:53,631 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47190678
2025-04-14 10:53:53,820 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22253304
2025-04-14 10:53:54,011 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802826
2025-04-14 10:53:54,200 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552966
2025-04-14 10:53:54,389 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555277
2025-04-14 10:53:54,734 - __main__ - INFO - Non-zero entries after thresholding: 47190678
2025-04-14 10:53:54,734 - __main__ - INFO - Using relaxed comparison (count_negative: 264)
2025-04-14 10:53:54,734 - __main__ - INFO - Testing stepsize: 0
2025-04-14 10:53:55,465 - __main__ - INFO -   chosen_logps: -49.79003, rejected_logps: -50.12822
2025-04-14 10:53:55,466 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 10:53:55,828 - __main__ - INFO -   chosen_logps: -49.79000, rejected_logps: -50.12737
2025-04-14 10:53:55,828 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 10:53:56,191 - __main__ - INFO -   chosen_logps: -49.50085, rejected_logps: -50.45496
2025-04-14 10:53:56,191 - __main__ - INFO - Testing stepsize: 1
2025-04-14 10:53:56,556 - __main__ - INFO -   chosen_logps: -48.88868, rejected_logps: -50.62043
2025-04-14 10:53:56,556 - __main__ - INFO - Testing stepsize: 2
2025-04-14 10:53:56,925 - __main__ - INFO -   chosen_logps: -47.90524, rejected_logps: -51.39232
2025-04-14 10:53:56,925 - __main__ - INFO - Testing stepsize: 5
2025-04-14 10:53:57,295 - __main__ - INFO -   chosen_logps: -45.00785, rejected_logps: -53.23857
2025-04-14 10:53:57,295 - __main__ - INFO - Testing stepsize: 10
2025-04-14 10:53:57,666 - __main__ - INFO -   chosen_logps: -40.97001, rejected_logps: -56.89095
2025-04-14 10:53:57,666 - __main__ - INFO - Testing stepsize: 15
2025-04-14 10:53:58,038 - __main__ - INFO -   chosen_logps: -37.76635, rejected_logps: -60.89042
2025-04-14 10:53:58,769 - __main__ - INFO - Update scale: 0.0051333333333333335
2025-04-14 10:53:58,770 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 10:53:58,849 - __main__ - INFO - Model weights updated successfully
2025-04-14 10:53:59,685 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:53:59,726 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:54:00,744 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 10:54:00,780 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 10:54:00,780 - __main__ - INFO - Skipping full model checkpoint for iteration 7
2025-04-14 10:54:00,780 - __main__ - INFO - Iteration 7 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 7
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 7
Iteration 7 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 8
===========================================
Iteration 8 is in valid list - performing full update
Finding noisy pairs for iteration 8...
2025-04-14 10:54:22,182 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 10:54:22,734 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:54:22,879 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 10:54:22,893 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 10:54:22,893 - __main__ - INFO - Loading dataset
2025-04-14 10:54:23,698 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 10:54:45,648 - __main__ - INFO - Processing dataset
2025-04-14 10:54:45,900 - __main__ - INFO - Searching for noisy pairs starting from index 1664
2025-04-14 10:54:54,373 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 10:55:03,151 - __main__ - INFO - Noisy pair found at index 1746: chosen_logps=-23.8750, rejected_logps=-24.2500
2025-04-14 10:55:03,152 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 10:55:03,220 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_8
2025-04-14 10:55:03,221 - __main__ - INFO - New dataset offset: 1747
Updated dataset offset to: 1747 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 8
Worker node 1 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 10:55:18,523 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]2025-04-14 10:55:18,763 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 10:55:18,823 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.40it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 10:55:19,073 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]2025-04-14 10:55:19,217 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:55:19,232 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 10:55:19,232 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
2025-04-14 10:55:19,309 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:55:19,379 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 10:55:19,463 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:55:19,478 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 10:55:19,478 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:55:19,521 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 10:55:19,533 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 10:55:19,533 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 10:55:35,845 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 10:55:36,042 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 10:55:36,084 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 10:55:45,832 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 10:55:46,204 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 10:55:46,453 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 10:55:49,818 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 10:55:49,996 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 10:55:50,180 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 10:55:52,634 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 10:55:53,432 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 10:55:53,794 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 10:55:53,870 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 10:55:54,254 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 10:55:54,375 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 10:55:57,182 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 10:55:57,443 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 10:55:57,565 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 10:55:57,725 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 10:55:58,035 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 10:55:58,239 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 10:56:01,399 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 10:56:01,452 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 10:56:01,489 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 10:56:01,788 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 10:56:01,923 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 10:56:02,106 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 10:56:05,301 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 10:56:05,416 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 10:56:05,496 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 10:56:05,543 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 10:56:05,853 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 10:56:06,146 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 10:56:08,913 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 10:56:09,338 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 10:56:09,631 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 10:56:09,753 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 10:56:09,831 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 10:56:10,149 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 10:56:12,884 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 10:56:13,322 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 10:56:13,614 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 10:56:13,860 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 10:56:14,019 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 10:56:14,237 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 10:56:16,791 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 10:56:17,270 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 10:56:17,331 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 10:56:17,781 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 10:56:17,784 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 10:56:18,228 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 10:56:18,239 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 10:56:18,500 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 10:56:18,501 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 10:56:21,202 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 10:56:21,651 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 10:56:21,963 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 10:56:25,360 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 10:56:25,581 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 10:56:25,680 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 10:56:25,742 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 10:56:26,742 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 10:56:26,749 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 10:57:50,766 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 10:57:51,570 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 10:57:52,752 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 10:57:54,159 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 10:57:56,692 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 10:57:57,894 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 10:57:59,710 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 10:58:00,332 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 10:58:02,391 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 10:58:02,654 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 10:58:02,760 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 10:58:03,346 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 10:58:07,364 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 10:58:07,633 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 10:58:09,935 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 10:58:10,955 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 10:58:12,169 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 10:58:13,200 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 10:58:15,516 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 10:58:15,783 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 10:58:15,783 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 10:58:18,524 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 10:58:20,801 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 10:58:22,384 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 10:58:22,625 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 10:58:22,924 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 10:58:23,046 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 10:58:23,114 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 10:58:28,591 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 10:58:28,756 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 11:00:17,439 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_0.npy
2025-04-14 11:00:17,440 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 94 negative comparisons
[E414 11:00:17.362346447 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a7c9a06f446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7a7c4f3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7a7c4f3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7a7c4f3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7a7c4f3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a7c4f3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7a7c9a1d65c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7a7c9ac94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7a7c9ad26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:00:17.381715807 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x722ac475c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x722a79bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x722a79bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x722a79bcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x722a79bd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x722a79bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x722ac48c35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x722ac5294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x722ac5326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:00:18.536073412 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b3c6bb4d446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b3c20fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b3c20fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7b3c20fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7b3c20fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b3c20fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b3c6bcb45c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b3c6c694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b3c6c726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:00:18,830 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_1.npy
2025-04-14 11:00:18,831 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 95 negative comparisons
2025-04-14 11:00:20,518 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 11:00:21,973 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 11:00:26.938848960 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b5b166fd446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b5acb9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b5acb9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7b5acb9cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7b5acb9d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b5acb9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b5b168645c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b5b17294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b5b17326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:00:26.951142886 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79854b0e8446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7985003cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7985003cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7985003cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7985003d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7985003d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79854b24f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x79854bc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x79854bd26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:00:26,286 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_2.npy
2025-04-14 11:00:26,286 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 117 negative comparisons
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 8
2025-04-14 11:00:29,187 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 8
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 11:00:47,296 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.37it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 11:00:47,844 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:00:47,986 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:00:47,998 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 11:01:04,624 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 11:01:07,511 - __main__ - INFO - Loaded gradients from node 0: 94 negative comparisons
2025-04-14 11:01:07,744 - __main__ - INFO - Loaded gradients from node 1: 95 negative comparisons
2025-04-14 11:01:07,978 - __main__ - INFO - Loaded gradients from node 2: 117 negative comparisons
2025-04-14 11:01:08,138 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 11:01:08,328 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 11:01:08,518 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119132735
2025-04-14 11:01:08,708 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352669
2025-04-14 11:01:08,898 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84833686
2025-04-14 11:01:09,088 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47193046
2025-04-14 11:01:09,278 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22253160
2025-04-14 11:01:09,467 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806800
2025-04-14 11:01:09,657 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552806
2025-04-14 11:01:09,849 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555222
2025-04-14 11:01:10,197 - __main__ - INFO - Non-zero entries after thresholding: 47193046
2025-04-14 11:01:10,198 - __main__ - INFO - Using relaxed comparison (count_negative: 306)
2025-04-14 11:01:10,198 - __main__ - INFO - Testing stepsize: 0
2025-04-14 11:01:10,947 - __main__ - INFO -   chosen_logps: -23.92811, rejected_logps: -23.81994
2025-04-14 11:01:10,947 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 11:01:11,332 - __main__ - INFO -   chosen_logps: -23.88334, rejected_logps: -23.81997
2025-04-14 11:01:11,332 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 11:01:11,715 - __main__ - INFO -   chosen_logps: -23.67607, rejected_logps: -23.91568
2025-04-14 11:01:11,715 - __main__ - INFO - Testing stepsize: 1
2025-04-14 11:01:12,103 - __main__ - INFO -   chosen_logps: -23.06592, rejected_logps: -24.48557
2025-04-14 11:01:12,103 - __main__ - INFO - Testing stepsize: 2
2025-04-14 11:01:12,492 - __main__ - INFO -   chosen_logps: -22.17924, rejected_logps: -24.84516
2025-04-14 11:01:12,493 - __main__ - INFO - Testing stepsize: 5
2025-04-14 11:01:12,884 - __main__ - INFO -   chosen_logps: -19.53205, rejected_logps: -26.54039
2025-04-14 11:01:12,884 - __main__ - INFO - Testing stepsize: 10
2025-04-14 11:01:13,276 - __main__ - INFO -   chosen_logps: -15.49342, rejected_logps: -29.29842
2025-04-14 11:01:13,277 - __main__ - INFO - Testing stepsize: 15
2025-04-14 11:01:13,671 - __main__ - INFO -   chosen_logps: -11.96347, rejected_logps: -32.24550
2025-04-14 11:01:14,436 - __main__ - INFO - Update scale: 0.005950000000000001
2025-04-14 11:01:14,437 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 11:01:14,518 - __main__ - INFO - Model weights updated successfully
2025-04-14 11:01:15,355 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:01:15,390 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:01:16,272 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 11:01:16,307 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 11:01:16,307 - __main__ - INFO - Skipping full model checkpoint for iteration 8
2025-04-14 11:01:16,307 - __main__ - INFO - Iteration 8 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 8
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 8
Iteration 8 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 9
===========================================
Iteration 9 is in valid list - performing full update
Finding noisy pairs for iteration 9...
2025-04-14 11:01:37,363 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 11:01:37,912 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:01:38,058 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:01:38,073 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 11:01:38,073 - __main__ - INFO - Loading dataset
2025-04-14 11:01:39,292 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 11:02:00,881 - __main__ - INFO - Processing dataset
2025-04-14 11:02:01,128 - __main__ - INFO - Searching for noisy pairs starting from index 1747
2025-04-14 11:02:11,396 - __main__ - INFO - Processed 53 samples without finding a noisy pair
2025-04-14 11:02:29,570 - __main__ - INFO - Noisy pair found at index 1892: chosen_logps=-58.0000, rejected_logps=-59.7500
2025-04-14 11:02:29,571 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 11:02:29,637 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_9
2025-04-14 11:02:29,638 - __main__ - INFO - New dataset offset: 1893
Updated dataset offset to: 1893 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 9
Worker node 1 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 11:02:45,201 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 11:02:45,453 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]
2025-04-14 11:02:45,760 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]2025-04-14 11:02:45,905 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:02:45,919 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 11:02:45,919 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 11:02:46,008 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:02:46,150 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:02:46,163 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 11:02:46,163 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:02:46,270 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.76it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
2025-04-14 11:02:46,813 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:02:46,966 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:02:46,981 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 11:02:46,981 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:03:02,447 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 11:03:02,710 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 11:03:03,532 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 11:03:12,812 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 11:03:12,884 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 11:03:13,611 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 11:03:16,621 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 11:03:17,007 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 11:03:17,390 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 11:03:20,121 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 11:03:20,157 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 11:03:20,570 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 11:03:20,922 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 11:03:21,002 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 11:03:21,667 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 11:03:24,055 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 11:03:24,256 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 11:03:24,585 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 11:03:24,679 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 11:03:24,983 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 11:03:25,645 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 11:03:28,007 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 11:03:28,109 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 11:03:28,942 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 11:03:29,006 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 11:03:29,233 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 11:03:29,800 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 11:03:31,657 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 11:03:32,046 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 11:03:32,431 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 11:03:32,915 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 11:03:33,239 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 11:03:33,741 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 11:03:35,263 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 11:03:35,950 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 11:03:36,688 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 11:03:36,931 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 11:03:37,407 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 11:03:37,596 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 11:03:39,465 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 11:03:39,923 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 11:03:40,770 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 11:03:40,804 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 11:03:41,483 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 11:03:41,588 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 11:03:44,101 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 11:03:44,270 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 11:03:44,281 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 11:03:44,857 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 11:03:45,396 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 11:03:45,629 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 11:03:45,634 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 11:03:46,037 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 11:03:46,039 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 11:03:47,722 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 11:03:48,910 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 11:03:48,977 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 11:03:51,193 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 11:03:51,492 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 11:03:53,670 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 11:03:53,699 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 11:03:54,102 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 11:03:54,116 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 11:05:17,966 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 11:05:17,984 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 11:05:20,213 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 11:05:21,021 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 11:05:21,159 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 11:05:24,988 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 11:05:25,695 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 11:05:26,365 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 11:05:28,458 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 11:05:29,402 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 11:05:30,070 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 11:05:31,190 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 11:05:32,237 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 11:05:33,946 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 11:05:36,458 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 11:05:36,877 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 11:05:36,915 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 11:05:40,561 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 11:05:40,646 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 11:05:43,120 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 11:05:44,893 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 11:05:46,374 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 11:05:46,965 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 11:05:47,127 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 11:05:48,099 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 11:05:50,284 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 11:05:50,461 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 11:05:50,933 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 11:05:53,276 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 11:05:54,016 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
[E414 11:07:41.646908900 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77d78a4a9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x77d73f7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x77d73f7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x77d73f7cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x77d73f7d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x77d73f7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x77d78a6105c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x77d78b094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x77d78b126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:07:41.661517373 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79f0486ea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79effd9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79effd9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x79effd9cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x79effd9d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79effd9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79f0488515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x79f049294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x79f049326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:07:41,979 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_0.npy
2025-04-14 11:07:41,979 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 116 negative comparisons
[E414 11:07:44.977189254 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x785960e64446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7859161cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7859161cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7859161cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7859161d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7859161d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x785960fcb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x785961a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x785961b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:07:44.988232186 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74aaf11e3446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74aaa65cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74aaa65cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x74aaa65cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x74aaa65d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74aaa65d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74aaf134a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74aaf1c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74aaf1d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:07:44,390 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_1.npy
2025-04-14 11:07:44,390 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 111 negative comparisons
2025-04-14 11:07:44,882 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 11:07:47,300 - __main__ - INFO - Node 1: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
[E414 11:07:53.912451242 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b1a7b755446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b1a30bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b1a30bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7b1a30bcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7b1a30bd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b1a30bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b1a7b8bc5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b1a7c494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b1a7c526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:07:53,187 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_2.npy
2025-04-14 11:07:53,187 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 127 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 9
2025-04-14 11:07:56,049 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 9
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 11:08:13,370 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]
2025-04-14 11:08:13,925 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:08:14,065 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:08:14,078 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 11:08:30,643 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 11:08:33,615 - __main__ - INFO - Loaded gradients from node 0: 116 negative comparisons
2025-04-14 11:08:33,853 - __main__ - INFO - Loaded gradients from node 1: 111 negative comparisons
2025-04-14 11:08:34,092 - __main__ - INFO - Loaded gradients from node 2: 127 negative comparisons
2025-04-14 11:08:34,256 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 11:08:34,454 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 11:08:34,650 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119137995
2025-04-14 11:08:34,847 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107354908
2025-04-14 11:08:35,045 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84833951
2025-04-14 11:08:35,240 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47203333
2025-04-14 11:08:35,436 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255483
2025-04-14 11:08:35,631 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804924
2025-04-14 11:08:35,828 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550827
2025-04-14 11:08:36,023 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553477
2025-04-14 11:08:36,376 - __main__ - INFO - Non-zero entries after thresholding: 47203333
2025-04-14 11:08:36,376 - __main__ - INFO - Using relaxed comparison (count_negative: 354)
2025-04-14 11:08:36,376 - __main__ - INFO - Testing stepsize: 0
2025-04-14 11:08:37,126 - __main__ - INFO -   chosen_logps: -57.55360, rejected_logps: -59.94020
2025-04-14 11:08:37,126 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 11:08:37,510 - __main__ - INFO -   chosen_logps: -57.31010, rejected_logps: -60.03181
2025-04-14 11:08:37,511 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 11:08:37,896 - __main__ - INFO -   chosen_logps: -56.49121, rejected_logps: -60.40557
2025-04-14 11:08:37,896 - __main__ - INFO - Testing stepsize: 1
2025-04-14 11:08:38,283 - __main__ - INFO -   chosen_logps: -55.68283, rejected_logps: -60.83839
2025-04-14 11:08:38,283 - __main__ - INFO - Testing stepsize: 2
2025-04-14 11:08:38,675 - __main__ - INFO -   chosen_logps: -53.31127, rejected_logps: -61.61618
2025-04-14 11:08:38,675 - __main__ - INFO - Testing stepsize: 5
2025-04-14 11:08:39,068 - __main__ - INFO -   chosen_logps: -47.38517, rejected_logps: -63.85893
2025-04-14 11:08:39,068 - __main__ - INFO - Testing stepsize: 10
2025-04-14 11:08:39,462 - __main__ - INFO -   chosen_logps: -39.97919, rejected_logps: -67.76202
2025-04-14 11:08:39,462 - __main__ - INFO - Testing stepsize: 15
2025-04-14 11:08:39,858 - __main__ - INFO -   chosen_logps: -35.10711, rejected_logps: -71.77808
2025-04-14 11:08:40,630 - __main__ - INFO - Update scale: 0.006883333333333333
2025-04-14 11:08:40,632 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 11:08:40,714 - __main__ - INFO - Model weights updated successfully
2025-04-14 11:08:41,753 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:08:41,794 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:08:42,827 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 11:08:42,861 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 11:08:42,861 - __main__ - INFO - Skipping full model checkpoint for iteration 9
2025-04-14 11:08:42,861 - __main__ - INFO - Iteration 9 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 9
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 9
Iteration 9 completed successfully
Cleaning up iteration files...
Multi-node training completed successfully!
Copying scripts to worker nodes...
Starting with dataset offset: 0
===========================================
Processing iteration 0
===========================================
Iteration 0 is in valid list - performing full update
Finding noisy pairs for iteration 0...
2025-04-14 11:09:07,158 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 11:09:07,708 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:09:07,852 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:09:07,866 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 11:09:07,866 - __main__ - INFO - Loading dataset
2025-04-14 11:09:09,108 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 11:09:30,657 - __main__ - INFO - Processing dataset
2025-04-14 11:09:30,906 - __main__ - INFO - Searching for noisy pairs starting from index 0
2025-04-14 11:09:31,541 - __main__ - INFO - Processed 0 samples without finding a noisy pair
2025-04-14 11:09:51,907 - __main__ - INFO - Processed 100 samples without finding a noisy pair
2025-04-14 11:10:03,159 - __main__ - INFO - Noisy pair found at index 167: chosen_logps=-51.2500, rejected_logps=-51.5000
2025-04-14 11:10:03,160 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 11:10:03,227 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_0
2025-04-14 11:10:03,228 - __main__ - INFO - New dataset offset: 168
Updated dataset offset to: 168 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 0
Worker node 1 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 11:10:18,910 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 11:10:19,136 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.65it/s]2025-04-14 11:10:19,467 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:10:19,548 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
2025-04-14 11:10:19,612 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:10:19,627 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 11:10:19,627 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:10:19,683 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.19it/s]2025-04-14 11:10:19,836 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:10:19,851 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 11:10:19,851 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
2025-04-14 11:10:20,096 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:10:20,237 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:10:20,250 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 11:10:20,250 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:10:36,165 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 11:10:36,419 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 11:10:36,881 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 11:10:46,337 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 11:10:46,690 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 11:10:46,942 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 11:10:50,549 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 11:10:50,581 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 11:10:50,797 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 11:10:53,427 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 11:10:53,700 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 11:10:53,824 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 11:10:54,443 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 11:10:54,532 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 11:10:54,773 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 11:10:57,790 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 11:10:57,902 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 11:10:58,126 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 11:10:58,338 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 11:10:58,424 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 11:10:59,014 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 11:11:01,700 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 11:11:01,909 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 11:11:02,002 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 11:11:02,368 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 11:11:02,490 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 11:11:02,813 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 11:11:05,409 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 11:11:05,915 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 11:11:06,453 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 11:11:06,709 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 11:11:06,741 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 11:11:06,802 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 11:11:09,458 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 11:11:09,807 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 11:11:10,164 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 11:11:10,711 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 11:11:10,867 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 11:11:10,940 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 11:11:13,916 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 11:11:14,253 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 11:11:14,809 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 11:11:14,942 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 11:11:15,002 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 11:11:15,169 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 11:11:17,852 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 11:11:18,194 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 11:11:18,243 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 11:11:19,113 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 11:11:19,120 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 11:11:19,174 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 11:11:19,178 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 11:11:19,411 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 11:11:19,413 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 11:11:22,262 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 11:11:22,421 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 11:11:22,585 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 11:11:26,337 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 11:11:26,374 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 11:11:26,734 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 11:11:26,871 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 11:11:27,115 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 11:11:27,167 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 11:12:45,402 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 11:12:45,475 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 11:12:48,460 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 11:12:50,066 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 11:12:50,348 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 11:12:51,130 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 11:12:53,875 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 11:12:54,049 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 11:12:57,811 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 11:12:58,695 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 11:13:01,734 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 11:13:02,275 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 11:13:02,631 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 11:13:02,890 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 11:13:05,570 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 11:13:05,853 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 11:13:06,906 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 11:13:09,321 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 11:13:10,110 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 11:13:11,125 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 11:13:12,299 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 11:13:14,183 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 11:13:14,649 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 11:13:16,939 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 11:13:18,463 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 11:13:18,699 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 11:13:18,898 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 11:13:19,539 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 11:13:21,851 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 11:13:25,442 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
[E414 11:15:08.574806229 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73bb24088446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x73bad93cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x73bad93cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x73bad93cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x73bad93d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73bad93d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x73bb241ef5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x73bb24c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x73bb24d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:15:08,920 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_1.npy
2025-04-14 11:15:08,920 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 129 negative comparisons
[E414 11:15:10.216921391 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ac899810446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ac84ebcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ac84ebcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7ac84ebcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7ac84ebd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ac84ebd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7ac8999775c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7ac89a294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7ac89a326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:15:10.217411243 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e33f3b87446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e33a8fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e33a8fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7e33a8fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7e33a8fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e33a8fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e33f3cee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e33f4694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e33f4726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:15:10,504 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_0.npy
2025-04-14 11:15:10,504 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 147 negative comparisons
2025-04-14 11:15:11,808 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 11:15:13,268 - __main__ - INFO - Node 0: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 0
Waiting for worker nodes to complete gradient computation...
[E414 11:15:20.399474829 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7de43b21e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7de3f05cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7de3f05cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7de3f05cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7de3f05d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7de3f05d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7de43b3855c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7de43be94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7de43bf26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:15:20.407612297 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b82ac287446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b82615cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b82615cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7b82615cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7b82615d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b82615d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b82ac3ee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b82ace94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b82acf26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:15:20.422605561 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75cd79578446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75cd2e9cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75cd2e9cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x75cd2e9cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x75cd2e9d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75cd2e9d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x75cd796df5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x75cd7a294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x75cd7a326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:15:20,702 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_2.npy
2025-04-14 11:15:20,702 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 124 negative comparisons
2025-04-14 11:15:23,362 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 0
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 11:15:40,199 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 11:15:40,745 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:15:40,886 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:15:40,898 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 11:15:57,660 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 11:16:00,541 - __main__ - INFO - Loaded gradients from node 0: 147 negative comparisons
2025-04-14 11:16:00,774 - __main__ - INFO - Loaded gradients from node 1: 129 negative comparisons
2025-04-14 11:16:01,008 - __main__ - INFO - Loaded gradients from node 2: 124 negative comparisons
2025-04-14 11:16:01,169 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 11:16:01,361 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 11:16:01,553 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119134992
2025-04-14 11:16:01,745 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352789
2025-04-14 11:16:01,936 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84837746
2025-04-14 11:16:02,129 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47196539
2025-04-14 11:16:02,319 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252866
2025-04-14 11:16:02,510 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802992
2025-04-14 11:16:02,700 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551431
2025-04-14 11:16:02,892 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555560
2025-04-14 11:16:03,239 - __main__ - INFO - Non-zero entries after thresholding: 47196539
2025-04-14 11:16:03,239 - __main__ - INFO - Testing stepsize: 0
2025-04-14 11:16:03,913 - __main__ - INFO -   chosen_logps: -51.81273, rejected_logps: -51.05098
2025-04-14 11:16:03,913 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 11:16:04,215 - __main__ - INFO -   chosen_logps: -51.77936, rejected_logps: -51.05097
2025-04-14 11:16:04,215 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 11:16:04,516 - __main__ - INFO -   chosen_logps: -51.26548, rejected_logps: -51.59348
2025-04-14 11:16:04,516 - __main__ - INFO - Testing stepsize: 1
2025-04-14 11:16:04,823 - __main__ - INFO -   chosen_logps: -50.85064, rejected_logps: -52.32625
2025-04-14 11:16:04,823 - __main__ - INFO - Testing stepsize: 2
2025-04-14 11:16:05,132 - __main__ - INFO -   chosen_logps: -49.94723, rejected_logps: -53.72593
2025-04-14 11:16:05,132 - __main__ - INFO - Testing stepsize: 5
2025-04-14 11:16:05,442 - __main__ - INFO -   chosen_logps: -47.25175, rejected_logps: -57.60462
2025-04-14 11:16:05,442 - __main__ - INFO - Testing stepsize: 10
2025-04-14 11:16:05,753 - __main__ - INFO -   chosen_logps: -42.68220, rejected_logps: -64.21220
2025-04-14 11:16:05,753 - __main__ - INFO - Testing stepsize: 15
2025-04-14 11:16:06,066 - __main__ - INFO -   chosen_logps: -38.30220, rejected_logps: -71.15797
2025-04-14 11:16:06,723 - __main__ - INFO - Update scale: 0.007777777777777778
2025-04-14 11:16:06,804 - __main__ - INFO - Model weights updated successfully
2025-04-14 11:16:08,036 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:16:08,071 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:16:09,114 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 11:16:09,154 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 11:16:09,154 - __main__ - INFO - Skipping full model checkpoint for iteration 0
2025-04-14 11:16:09,155 - __main__ - INFO - Iteration 0 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 0
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 0
Iteration 0 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 1
===========================================
Iteration 1 is in valid list - performing full update
Finding noisy pairs for iteration 1...
2025-04-14 11:16:30,068 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 11:16:30,617 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:16:30,762 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:16:30,777 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 11:16:30,777 - __main__ - INFO - Loading dataset
2025-04-14 11:16:31,521 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 11:16:53,057 - __main__ - INFO - Processing dataset
2025-04-14 11:16:53,307 - __main__ - INFO - Searching for noisy pairs starting from index 168
2025-04-14 11:17:00,368 - __main__ - INFO - Processed 32 samples without finding a noisy pair
2025-04-14 11:17:19,181 - __main__ - INFO - Processed 132 samples without finding a noisy pair
2025-04-14 11:17:25,851 - __main__ - INFO - Noisy pair found at index 337: chosen_logps=-90.0000, rejected_logps=-92.0000
2025-04-14 11:17:25,852 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 11:17:25,920 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_1
2025-04-14 11:17:25,921 - __main__ - INFO - New dataset offset: 338
Updated dataset offset to: 338 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 1
Worker node 2 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 11:17:41,573 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 11:17:41,666 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.12it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]2025-04-14 11:17:41,995 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 11:17:42,123 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]2025-04-14 11:17:42,222 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:17:42,266 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:17:42,281 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 11:17:42,281 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.63it/s]2025-04-14 11:17:42,364 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:17:42,377 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 11:17:42,377 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.78it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
2025-04-14 11:17:42,539 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:17:42,691 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:17:42,706 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 11:17:42,707 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:17:58,813 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 11:17:58,921 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 11:17:59,235 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 11:18:08,765 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 11:18:09,090 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 11:18:09,416 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 11:18:12,667 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 11:18:12,942 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 11:18:13,136 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 11:18:15,412 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 11:18:15,880 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 11:18:16,364 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 11:18:17,024 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 11:18:17,037 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 11:18:17,284 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 11:18:19,549 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 11:18:20,041 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 11:18:20,189 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 11:18:20,774 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 11:18:21,304 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 11:18:21,352 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 11:18:24,014 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 11:18:24,196 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 11:18:24,375 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 11:18:24,651 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 11:18:25,095 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 11:18:25,477 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 11:18:27,817 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 11:18:28,454 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 11:18:28,525 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 11:18:28,539 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 11:18:29,132 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 11:18:29,519 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 11:18:31,564 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 11:18:32,402 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 11:18:32,770 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 11:18:32,808 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 11:18:33,568 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 11:18:33,668 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 11:18:35,631 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 11:18:36,512 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 11:18:36,518 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 11:18:36,799 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 11:18:37,870 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 11:18:38,074 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 11:18:39,921 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 11:18:40,726 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 11:18:40,938 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 11:18:41,261 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 11:18:41,266 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 11:18:42,140 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 11:18:42,151 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 11:18:42,158 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 11:18:42,162 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 11:18:44,276 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 11:18:45,093 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 11:18:45,315 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 11:18:48,521 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 11:18:48,548 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 11:18:49,201 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 11:18:49,461 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 11:18:49,999 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 11:18:50,100 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 11:20:06,105 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 11:20:06,891 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 11:20:07,685 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 11:20:11,011 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 11:20:12,975 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 11:20:13,244 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 11:20:15,020 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 11:20:15,928 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 11:20:16,262 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 11:20:17,616 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 11:20:17,956 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 11:20:22,068 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 11:20:22,763 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 11:20:23,322 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 11:20:25,234 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 11:20:25,507 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 11:20:26,709 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 11:20:27,425 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 11:20:31,000 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 11:20:31,207 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 11:20:31,461 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 11:20:34,961 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 11:20:35,104 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 11:20:38,175 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 11:20:38,601 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 11:20:39,399 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 11:20:39,562 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 11:20:39,718 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 11:20:42,334 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 11:20:42,613 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 11:22:27,073 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_0.npy
2025-04-14 11:22:27,073 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 121 negative comparisons
[E414 11:22:27.017612436 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7637a612a446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76375b3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76375b3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x76375b3cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x76375b3d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76375b3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7637a62915c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7637a6c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7637a6d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:22:30,301 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 11:22:30,749 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_1.npy
2025-04-14 11:22:30,749 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 128 negative comparisons
2025-04-14 11:22:32,477 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_2.npy
2025-04-14 11:22:32,477 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 123 negative comparisons
[E414 11:22:32.431879082 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78b1afe68446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78b1651cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78b1651cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x78b1651cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x78b1651d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78b1651d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78b1affc35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78b1b0a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78b1b0b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:22:32.439526588 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x732ca4798446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x732c59bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x732c59bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x732c59bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x732c59bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x732c59bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x732ca48ff5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x732ca5494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x732ca5526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:22:33,714 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 11:22:35,568 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 1
Worker node 2 successfully completed gradient computation for iteration 1
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 11:22:53,086 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 11:22:53,630 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:22:53,770 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:22:53,783 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 11:23:10,251 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 11:23:13,280 - __main__ - INFO - Loaded gradients from node 0: 121 negative comparisons
2025-04-14 11:23:13,519 - __main__ - INFO - Loaded gradients from node 1: 128 negative comparisons
2025-04-14 11:23:13,758 - __main__ - INFO - Loaded gradients from node 2: 123 negative comparisons
2025-04-14 11:23:13,921 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 11:23:14,116 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 11:23:14,311 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119136005
2025-04-14 11:23:14,507 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107357171
2025-04-14 11:23:14,703 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84844099
2025-04-14 11:23:14,899 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47195521
2025-04-14 11:23:15,096 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22253563
2025-04-14 11:23:15,292 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803941
2025-04-14 11:23:15,486 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551909
2025-04-14 11:23:15,682 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554863
2025-04-14 11:23:16,035 - __main__ - INFO - Non-zero entries after thresholding: 47195521
2025-04-14 11:23:16,036 - __main__ - INFO - Testing stepsize: 0
2025-04-14 11:23:16,674 - __main__ - INFO -   chosen_logps: -89.94104, rejected_logps: -91.92723
2025-04-14 11:23:16,674 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 11:23:16,946 - __main__ - INFO -   chosen_logps: -89.60886, rejected_logps: -91.92719
2025-04-14 11:23:16,946 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 11:23:17,217 - __main__ - INFO -   chosen_logps: -88.84774, rejected_logps: -92.71684
2025-04-14 11:23:17,218 - __main__ - INFO - Testing stepsize: 1
2025-04-14 11:23:17,491 - __main__ - INFO -   chosen_logps: -87.81324, rejected_logps: -93.36388
2025-04-14 11:23:17,491 - __main__ - INFO - Testing stepsize: 2
2025-04-14 11:23:17,772 - __main__ - INFO -   chosen_logps: -85.68242, rejected_logps: -94.69342
2025-04-14 11:23:17,772 - __main__ - INFO - Testing stepsize: 5
2025-04-14 11:23:18,051 - __main__ - INFO -   chosen_logps: -79.14632, rejected_logps: -98.61634
2025-04-14 11:23:18,051 - __main__ - INFO - Testing stepsize: 10
2025-04-14 11:23:18,333 - __main__ - INFO -   chosen_logps: -69.64822, rejected_logps: -105.18192
2025-04-14 11:23:18,333 - __main__ - INFO - Testing stepsize: 15
2025-04-14 11:23:18,615 - __main__ - INFO -   chosen_logps: -61.22248, rejected_logps: -112.41812
2025-04-14 11:23:19,225 - __main__ - INFO - Update scale: 0.007233333333333334
2025-04-14 11:23:19,307 - __main__ - INFO - Model weights updated successfully
2025-04-14 11:23:20,157 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:23:20,192 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:23:21,448 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 11:23:21,485 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 11:23:21,485 - __main__ - INFO - Skipping full model checkpoint for iteration 1
2025-04-14 11:23:21,485 - __main__ - INFO - Iteration 1 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 1
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 1
Iteration 1 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 2
===========================================
Iteration 2 is in valid list - performing full update
Finding noisy pairs for iteration 2...
2025-04-14 11:23:42,547 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 11:23:43,096 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:23:43,242 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:23:43,259 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 11:23:43,259 - __main__ - INFO - Loading dataset
2025-04-14 11:23:44,113 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 11:24:05,893 - __main__ - INFO - Processing dataset
2025-04-14 11:24:06,155 - __main__ - INFO - Searching for noisy pairs starting from index 338
2025-04-14 11:24:12,915 - __main__ - INFO - Noisy pair found at index 369: chosen_logps=-153.0000, rejected_logps=-154.0000
2025-04-14 11:24:12,916 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 11:24:12,984 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_2
2025-04-14 11:24:12,985 - __main__ - INFO - New dataset offset: 370
Updated dataset offset to: 370 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 2
Worker node 1 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 11:24:27,369 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]2025-04-14 11:24:27,635 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.50it/s]2025-04-14 11:24:27,719 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]2025-04-14 11:24:27,925 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]2025-04-14 11:24:28,070 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:24:28,085 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 11:24:28,085 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 11:24:28,191 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:24:28,264 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:24:28,331 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:24:28,344 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 11:24:28,344 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:24:28,417 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:24:28,432 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 11:24:28,432 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:24:44,641 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 11:24:44,874 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 11:24:45,012 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 11:24:54,886 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 11:24:55,041 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 11:24:55,366 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 11:24:58,749 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 11:24:58,780 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 11:24:59,503 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 11:25:01,985 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 11:25:02,049 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 11:25:02,204 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 11:25:02,668 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 11:25:02,744 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 11:25:03,414 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 11:25:05,966 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 11:25:06,019 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 11:25:06,531 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 11:25:06,888 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 11:25:06,939 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 11:25:07,394 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 11:25:09,904 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 11:25:09,940 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 11:25:10,390 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 11:25:10,431 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 11:25:10,766 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 11:25:11,387 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 11:25:13,677 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 11:25:14,229 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 11:25:14,435 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 11:25:14,604 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 11:25:15,001 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 11:25:15,385 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 11:25:17,553 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 11:25:18,119 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 11:25:18,527 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 11:25:18,552 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 11:25:19,224 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 11:25:19,730 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 11:25:21,767 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 11:25:22,174 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 11:25:22,522 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 11:25:23,028 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 11:25:23,305 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 11:25:23,924 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 11:25:25,679 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 11:25:26,687 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 11:25:26,898 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 11:25:27,523 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 11:25:27,533 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 11:25:27,846 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 11:25:27,855 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 11:25:27,869 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 11:25:27,870 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 11:25:30,249 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 11:25:30,690 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 11:25:31,069 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 11:25:35,056 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 11:25:35,186 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 11:25:35,213 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 11:25:35,399 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 11:25:35,492 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 11:25:35,609 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 11:26:52,076 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 11:26:52,098 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 11:26:53,154 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 11:26:57,068 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 11:26:57,146 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 11:26:59,151 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 11:26:59,588 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 11:27:00,173 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 11:27:01,313 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 11:27:03,701 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 11:27:05,327 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 11:27:07,653 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 11:27:07,810 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 11:27:08,492 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 11:27:11,154 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 11:27:11,231 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 11:27:12,582 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 11:27:15,645 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 11:27:15,758 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 11:27:18,166 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 11:27:18,782 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 11:27:19,757 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 11:27:21,357 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 11:27:23,560 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 11:27:25,222 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 11:27:25,481 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 11:27:25,878 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 11:27:26,091 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 11:27:28,785 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 11:27:30,116 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 11:29:14,799 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_1.npy
2025-04-14 11:29:14,799 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 112 negative comparisons
2025-04-14 11:29:17,776 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 11:29:19.907456783 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7807f20ce446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7807a73cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7807a73cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7807a73cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7807a73d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7807a73d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7807f22355c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7807f2c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7807f2d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:29:19.933586390 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e9537da5446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e94ed1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e94ed1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7e94ed1cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7e94ed1d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e94ed1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e9537f0c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e9538a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e9538b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:29:19.945811767 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bc2acd57446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7bc2621cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7bc2621cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7bc2621cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7bc2621d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bc2621d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7bc2acebe5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7bc2ada94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7bc2adb26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:29:19,205 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_2.npy
2025-04-14 11:29:19,205 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 131 negative comparisons
[E414 11:29:21.094720141 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73b020db7446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x73afd61cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x73afd61cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x73afd61cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x73afd61d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73afd61d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x73b020f1e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x73b021894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x73b021926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:29:21.117075489 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b37d1fb9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b37873cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b37873cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7b37873cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7b37873d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b37873d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b37d21205c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b37d2a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b37d2b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:29:21.117688227 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78d2b12be446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78d2665cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78d2665cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x78d2665cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x78d2665d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78d2665d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78d2b14255c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78d2b1e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78d2b1f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:29:21,466 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_0.npy
2025-04-14 11:29:21,466 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 105 negative comparisons
2025-04-14 11:29:21,929 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 2
2025-04-14 11:29:24,386 - __main__ - INFO - Node 0: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 2
Waiting for worker nodes to complete gradient computation...
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 11:29:42,507 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.54it/s]
2025-04-14 11:29:43,057 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:29:43,203 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:29:43,216 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 11:29:59,753 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 11:30:02,763 - __main__ - INFO - Loaded gradients from node 0: 105 negative comparisons
2025-04-14 11:30:03,009 - __main__ - INFO - Loaded gradients from node 1: 112 negative comparisons
2025-04-14 11:30:03,252 - __main__ - INFO - Loaded gradients from node 2: 131 negative comparisons
2025-04-14 11:30:03,418 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 11:30:03,615 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 11:30:03,812 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119131250
2025-04-14 11:30:04,009 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107346302
2025-04-14 11:30:04,207 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84835274
2025-04-14 11:30:04,405 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47194106
2025-04-14 11:30:04,602 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22258218
2025-04-14 11:30:04,799 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8800401
2025-04-14 11:30:04,998 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551824
2025-04-14 11:30:05,195 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555326
2025-04-14 11:30:05,552 - __main__ - INFO - Non-zero entries after thresholding: 47194106
2025-04-14 11:30:05,552 - __main__ - INFO - Using relaxed comparison (count_negative: 348)
2025-04-14 11:30:05,552 - __main__ - INFO - Testing stepsize: 0
2025-04-14 11:30:06,192 - __main__ - INFO -   chosen_logps: -152.85397, rejected_logps: -153.89148
2025-04-14 11:30:06,192 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 11:30:06,458 - __main__ - INFO -   chosen_logps: -152.76056, rejected_logps: -154.00294
2025-04-14 11:30:06,458 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 11:30:06,725 - __main__ - INFO -   chosen_logps: -152.47408, rejected_logps: -154.27319
2025-04-14 11:30:06,725 - __main__ - INFO - Testing stepsize: 1
2025-04-14 11:30:06,993 - __main__ - INFO -   chosen_logps: -151.69824, rejected_logps: -154.76302
2025-04-14 11:30:06,994 - __main__ - INFO - Testing stepsize: 2
2025-04-14 11:30:07,267 - __main__ - INFO -   chosen_logps: -150.89505, rejected_logps: -155.84860
2025-04-14 11:30:07,268 - __main__ - INFO - Testing stepsize: 5
2025-04-14 11:30:07,542 - __main__ - INFO -   chosen_logps: -148.50552, rejected_logps: -158.52159
2025-04-14 11:30:07,542 - __main__ - INFO - Testing stepsize: 10
2025-04-14 11:30:07,820 - __main__ - INFO -   chosen_logps: -144.76300, rejected_logps: -163.32008
2025-04-14 11:30:07,820 - __main__ - INFO - Testing stepsize: 15
2025-04-14 11:30:08,098 - __main__ - INFO -   chosen_logps: -141.70059, rejected_logps: -168.20238
2025-04-14 11:30:08,706 - __main__ - INFO - Update scale: 0.006766666666666667
2025-04-14 11:30:08,708 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 11:30:08,790 - __main__ - INFO - Model weights updated successfully
2025-04-14 11:30:09,633 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:30:09,665 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:30:10,570 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 11:30:10,603 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 11:30:10,603 - __main__ - INFO - Skipping full model checkpoint for iteration 2
2025-04-14 11:30:10,603 - __main__ - INFO - Iteration 2 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 2
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 2
Iteration 2 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 3
===========================================
Iteration 3 is in valid list - performing full update
Finding noisy pairs for iteration 3...
2025-04-14 11:30:31,689 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 11:30:32,240 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:30:32,386 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:30:32,400 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 11:30:32,400 - __main__ - INFO - Loading dataset
2025-04-14 11:30:33,267 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 11:30:54,918 - __main__ - INFO - Processing dataset
2025-04-14 11:30:55,161 - __main__ - INFO - Searching for noisy pairs starting from index 370
2025-04-14 11:31:01,326 - __main__ - INFO - Processed 30 samples without finding a noisy pair
2025-04-14 11:31:21,858 - __main__ - INFO - Processed 130 samples without finding a noisy pair
2025-04-14 11:31:43,343 - __main__ - INFO - Processed 230 samples without finding a noisy pair
2025-04-14 11:32:02,667 - __main__ - INFO - Processed 330 samples without finding a noisy pair
2025-04-14 11:32:22,946 - __main__ - INFO - Processed 430 samples without finding a noisy pair
2025-04-14 11:32:42,603 - __main__ - INFO - Processed 530 samples without finding a noisy pair
2025-04-14 11:33:01,260 - __main__ - INFO - Processed 630 samples without finding a noisy pair
2025-04-14 11:33:06,738 - __main__ - INFO - Noisy pair found at index 1030: chosen_logps=-280.0000, rejected_logps=-282.0000
2025-04-14 11:33:06,740 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 11:33:06,807 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_3
2025-04-14 11:33:06,808 - __main__ - INFO - New dataset offset: 1031
Updated dataset offset to: 1031 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 3
Worker node 2 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 11:33:22,487 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 11:33:22,684 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 11:33:22,714 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.67it/s]2025-04-14 11:33:23,046 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 11:33:23,190 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:33:23,205 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 11:33:23,205 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:33:23,227 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:33:23,271 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:33:23,381 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:33:23,396 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 11:33:23,396 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:33:23,412 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:33:23,425 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 11:33:23,425 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:33:39,816 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 11:33:39,971 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 11:33:39,983 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 11:33:49,957 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 11:33:50,016 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 11:33:50,122 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 11:33:53,983 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 11:33:54,141 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 11:33:54,224 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 11:33:56,424 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 11:33:57,113 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 11:33:57,127 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 11:33:57,967 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 11:33:58,197 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 11:33:58,308 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 11:34:00,734 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 11:34:01,031 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 11:34:01,317 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 11:34:01,898 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 11:34:02,119 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 11:34:02,273 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 11:34:05,191 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 11:34:05,428 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 11:34:05,733 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 11:34:05,821 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 11:34:05,930 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 11:34:06,297 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 11:34:09,343 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 11:34:09,522 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 11:34:09,578 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 11:34:09,933 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 11:34:10,167 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 11:34:10,213 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 11:34:13,147 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 11:34:13,235 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 11:34:13,246 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 11:34:14,123 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 11:34:14,143 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 11:34:14,207 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 11:34:17,061 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 11:34:17,381 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 11:34:17,410 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 11:34:18,314 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 11:34:18,657 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 11:34:18,688 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 11:34:21,463 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 11:34:21,473 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 11:34:21,491 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 11:34:22,620 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 11:34:22,626 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 11:34:22,883 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 11:34:22,891 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 11:34:22,959 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 11:34:22,970 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 11:34:25,838 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 11:34:25,973 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 11:34:26,138 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 11:34:30,000 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 11:34:30,263 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 11:34:30,338 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 11:34:30,337 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 11:34:30,371 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 11:34:30,457 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 11:35:46,292 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 11:35:46,484 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 11:35:50,247 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 11:35:50,660 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 11:35:51,156 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 11:35:51,648 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 11:35:54,395 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 11:35:54,922 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 11:35:55,564 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 11:35:59,347 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 11:35:59,351 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 11:36:02,177 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 11:36:02,546 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 11:36:03,946 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 11:36:05,214 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 11:36:05,530 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 11:36:05,725 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 11:36:06,518 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 11:36:10,849 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 11:36:10,853 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 11:36:12,726 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 11:36:13,673 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 11:36:14,200 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 11:36:17,444 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 11:36:17,859 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 11:36:19,125 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 11:36:19,293 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 11:36:21,845 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 11:36:21,970 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 11:36:22,165 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 11:38:04,936 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_1.npy
2025-04-14 11:38:04,936 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 123 negative comparisons
[E414 11:38:05.839283715 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x763986878446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76393bbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76393bbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x76393bbcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x76393bbd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76393bbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7639869df5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x763987494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x763987526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:38:08,231 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 11:38:09,880 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_0.npy
2025-04-14 11:38:09,880 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 146 negative comparisons
[E414 11:38:09.802322658 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x77e0d22a9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x77e0875cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x77e0875cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x77e0875cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x77e0875d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x77e0875d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x77e0d24105c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x77e0d2e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x77e0d2f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:38:10.711057279 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75121efb7446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7511d43cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7511d43cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7511d43cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7511d43d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7511d43d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x75121f11e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x75121fc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x75121fd26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:38:10.719495029 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x720aaaba9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x720a5ffcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x720a5ffcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x720a5ffcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x720a5ffd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x720a5ffd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x720aaad105c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x720aab894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x720aab926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:38:10.729627312 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78f1cf278446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78f1845cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78f1845cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x78f1845cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x78f1845d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78f1845d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78f1cf3df5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78f1cfe94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78f1cff26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:38:11,076 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_2.npy
2025-04-14 11:38:11,076 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 126 negative comparisons
2025-04-14 11:38:12,898 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 11:38:13,985 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 3
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 3
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 11:38:31,614 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 11:38:32,163 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:38:32,303 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:38:32,315 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 11:38:48,905 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 11:38:51,897 - __main__ - INFO - Loaded gradients from node 0: 146 negative comparisons
2025-04-14 11:38:52,138 - __main__ - INFO - Loaded gradients from node 1: 123 negative comparisons
2025-04-14 11:38:52,380 - __main__ - INFO - Loaded gradients from node 2: 126 negative comparisons
2025-04-14 11:38:52,547 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 11:38:52,745 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 11:38:52,943 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119135138
2025-04-14 11:38:53,141 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107353926
2025-04-14 11:38:53,340 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84827402
2025-04-14 11:38:53,536 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47189414
2025-04-14 11:38:53,734 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22253519
2025-04-14 11:38:53,931 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804532
2025-04-14 11:38:54,129 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1553639
2025-04-14 11:38:54,326 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555591
2025-04-14 11:38:54,682 - __main__ - INFO - Non-zero entries after thresholding: 47189414
2025-04-14 11:38:54,682 - __main__ - INFO - Testing stepsize: 0
2025-04-14 11:38:55,305 - __main__ - INFO -   chosen_logps: -279.80569, rejected_logps: -281.87402
2025-04-14 11:38:55,305 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 11:38:55,561 - __main__ - INFO -   chosen_logps: -279.41293, rejected_logps: -281.97348
2025-04-14 11:38:55,561 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 11:38:55,817 - __main__ - INFO -   chosen_logps: -277.57367, rejected_logps: -283.13049
2025-04-14 11:38:55,818 - __main__ - INFO - Testing stepsize: 1
2025-04-14 11:38:56,077 - __main__ - INFO -   chosen_logps: -275.77789, rejected_logps: -284.36328
2025-04-14 11:38:56,077 - __main__ - INFO - Testing stepsize: 2
2025-04-14 11:38:56,339 - __main__ - INFO -   chosen_logps: -272.16779, rejected_logps: -287.15393
2025-04-14 11:38:56,339 - __main__ - INFO - Testing stepsize: 5
2025-04-14 11:38:56,603 - __main__ - INFO -   chosen_logps: -260.53220, rejected_logps: -294.81372
2025-04-14 11:38:56,603 - __main__ - INFO - Testing stepsize: 10
2025-04-14 11:38:56,868 - __main__ - INFO -   chosen_logps: -243.33777, rejected_logps: -308.31192
2025-04-14 11:38:56,868 - __main__ - INFO - Testing stepsize: 15
2025-04-14 11:38:57,135 - __main__ - INFO -   chosen_logps: -227.54192, rejected_logps: -322.04822
2025-04-14 11:38:57,726 - __main__ - INFO - Update scale: 0.007680555555555556
2025-04-14 11:38:57,808 - __main__ - INFO - Model weights updated successfully
2025-04-14 11:38:58,974 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:38:59,010 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:39:00,137 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 11:39:00,171 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 11:39:00,172 - __main__ - INFO - Skipping full model checkpoint for iteration 3
2025-04-14 11:39:00,172 - __main__ - INFO - Iteration 3 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 3
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 3
Iteration 3 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 4
===========================================
Iteration 4 is in valid list - performing full update
Finding noisy pairs for iteration 4...
2025-04-14 11:39:21,284 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 11:39:21,833 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:39:21,978 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:39:21,993 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 11:39:21,993 - __main__ - INFO - Loading dataset
2025-04-14 11:39:22,971 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 11:39:44,667 - __main__ - INFO - Processing dataset
2025-04-14 11:39:44,913 - __main__ - INFO - Searching for noisy pairs starting from index 1031
2025-04-14 11:39:59,875 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 11:40:13,490 - __main__ - INFO - Noisy pair found at index 1163: chosen_logps=-147.0000, rejected_logps=-149.0000
2025-04-14 11:40:13,491 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 11:40:13,558 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_4
2025-04-14 11:40:13,558 - __main__ - INFO - New dataset offset: 1164
Updated dataset offset to: 1164 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 4
Worker node 2 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 11:40:28,973 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]2025-04-14 11:40:29,216 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 11:40:29,246 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]2025-04-14 11:40:29,531 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 11:40:29,675 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:40:29,690 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 11:40:29,690 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]
2025-04-14 11:40:29,762 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:40:29,802 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:40:29,916 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:40:29,931 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 11:40:29,931 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:40:29,943 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:40:29,956 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 11:40:29,956 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:40:46,189 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 11:40:46,471 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 11:40:46,506 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 11:40:56,448 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 11:40:56,610 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 11:40:56,705 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 11:41:00,309 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 11:41:00,409 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 11:41:00,559 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 11:41:03,384 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 11:41:03,591 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 11:41:03,798 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 11:41:04,349 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 11:41:04,556 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 11:41:05,169 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 11:41:07,457 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 11:41:07,652 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 11:41:08,061 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 11:41:08,299 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 11:41:08,828 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 11:41:09,494 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 11:41:11,646 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 11:41:11,892 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 11:41:12,099 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 11:41:12,694 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 11:41:12,966 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 11:41:13,350 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 11:41:15,728 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 11:41:16,067 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 11:41:16,260 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 11:41:17,058 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 11:41:17,086 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 11:41:17,481 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 11:41:19,644 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 11:41:20,094 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 11:41:20,552 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 11:41:20,669 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 11:41:21,165 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 11:41:21,872 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 11:41:24,097 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 11:41:24,746 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 11:41:24,907 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 11:41:24,924 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 11:41:25,516 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 11:41:25,857 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 11:41:27,885 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 11:41:28,238 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 11:41:29,223 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 11:41:29,226 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 11:41:29,533 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 11:41:29,723 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 11:41:29,729 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 11:41:29,828 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 11:41:29,829 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 11:41:32,527 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 11:41:32,723 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 11:41:33,614 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 11:41:36,948 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 11:41:36,989 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 11:41:37,153 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 11:41:37,170 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 11:41:37,231 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 11:41:37,682 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 11:43:00,976 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 11:43:02,302 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 11:43:02,803 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 11:43:04,426 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 11:43:07,073 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 11:43:07,974 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 11:43:09,369 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 11:43:10,168 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 11:43:11,232 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 11:43:11,291 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 11:43:13,299 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 11:43:13,368 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 11:43:15,946 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 11:43:17,691 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 11:43:19,173 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 11:43:20,645 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 11:43:21,585 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 11:43:22,033 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 11:43:23,220 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 11:43:24,393 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 11:43:25,726 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 11:43:28,934 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 11:43:29,950 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 11:43:31,393 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 11:43:32,887 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 11:43:33,108 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 11:43:33,389 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 11:43:33,695 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 11:43:35,283 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 11:43:35,823 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 11:45:27.872684715 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70455828f446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70450d5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70450d5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x70450d5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x70450d5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70450d5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7045583f65c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x704558e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x704558f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:45:27,182 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_1.npy
2025-04-14 11:45:27,182 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 132 negative comparisons
2025-04-14 11:45:28,772 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_0.npy
2025-04-14 11:45:28,772 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 144 negative comparisons
2025-04-14 11:45:30,391 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 11:45:31.405433798 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79e76e868446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79e723bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79e723bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x79e723bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x79e723bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79e723bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79e76e9c35c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x79e76f494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x79e76f526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:45:31,716 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_2.npy
2025-04-14 11:45:31,716 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 146 negative comparisons
2025-04-14 11:45:31,807 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 11:45:34,481 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 4
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 4
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 11:45:51,859 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.54it/s]
2025-04-14 11:45:52,409 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:45:52,551 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:45:52,563 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 11:46:08,987 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 11:46:11,894 - __main__ - INFO - Loaded gradients from node 0: 144 negative comparisons
2025-04-14 11:46:12,129 - __main__ - INFO - Loaded gradients from node 1: 132 negative comparisons
2025-04-14 11:46:12,367 - __main__ - INFO - Loaded gradients from node 2: 146 negative comparisons
2025-04-14 11:46:12,528 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 11:46:12,720 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 11:46:12,913 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119126647
2025-04-14 11:46:13,105 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107343263
2025-04-14 11:46:13,298 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84831303
2025-04-14 11:46:13,490 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47187950
2025-04-14 11:46:13,683 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255879
2025-04-14 11:46:13,875 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806899
2025-04-14 11:46:14,068 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552285
2025-04-14 11:46:14,262 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555582
2025-04-14 11:46:14,615 - __main__ - INFO - Non-zero entries after thresholding: 47187950
2025-04-14 11:46:14,615 - __main__ - INFO - Testing stepsize: 0
2025-04-14 11:46:15,367 - __main__ - INFO -   chosen_logps: -147.81644, rejected_logps: -147.74109
2025-04-14 11:46:15,367 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 11:46:15,750 - __main__ - INFO -   chosen_logps: -147.54079, rejected_logps: -147.88171
2025-04-14 11:46:15,750 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 11:46:16,133 - __main__ - INFO -   chosen_logps: -145.92810, rejected_logps: -148.58331
2025-04-14 11:46:16,133 - __main__ - INFO - Testing stepsize: 1
2025-04-14 11:46:16,519 - __main__ - INFO -   chosen_logps: -145.03285, rejected_logps: -149.05200
2025-04-14 11:46:16,519 - __main__ - INFO - Testing stepsize: 2
2025-04-14 11:46:16,910 - __main__ - INFO -   chosen_logps: -142.07181, rejected_logps: -150.23996
2025-04-14 11:46:16,910 - __main__ - INFO - Testing stepsize: 5
2025-04-14 11:46:17,301 - __main__ - INFO -   chosen_logps: -134.33563, rejected_logps: -154.22490
2025-04-14 11:46:17,301 - __main__ - INFO - Testing stepsize: 10
2025-04-14 11:46:17,733 - __main__ - INFO -   chosen_logps: -123.27103, rejected_logps: -160.65054
2025-04-14 11:46:17,733 - __main__ - INFO - Testing stepsize: 15
2025-04-14 11:46:18,171 - __main__ - INFO -   chosen_logps: -113.87166, rejected_logps: -166.79184
2025-04-14 11:46:19,039 - __main__ - INFO - Update scale: 0.008205555555555556
2025-04-14 11:46:19,128 - __main__ - INFO - Model weights updated successfully
2025-04-14 11:46:20,124 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:46:20,160 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:46:21,213 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 11:46:21,249 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 11:46:21,249 - __main__ - INFO - Skipping full model checkpoint for iteration 4
2025-04-14 11:46:21,249 - __main__ - INFO - Iteration 4 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 4
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 4
Iteration 4 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 5
===========================================
Iteration 5 is in valid list - performing full update
Finding noisy pairs for iteration 5...
2025-04-14 11:46:42,368 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 11:46:42,918 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:46:43,063 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:46:43,077 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 11:46:43,077 - __main__ - INFO - Loading dataset
2025-04-14 11:46:43,825 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 11:47:05,366 - __main__ - INFO - Processing dataset
2025-04-14 11:47:05,609 - __main__ - INFO - Searching for noisy pairs starting from index 1164
2025-04-14 11:47:12,543 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 11:47:33,921 - __main__ - INFO - Processed 136 samples without finding a noisy pair
2025-04-14 11:47:44,058 - __main__ - INFO - Noisy pair found at index 1344: chosen_logps=-268.0000, rejected_logps=-270.0000
2025-04-14 11:47:44,059 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 11:47:44,126 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_5
2025-04-14 11:47:44,127 - __main__ - INFO - New dataset offset: 1345
Updated dataset offset to: 1345 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 5
Worker node 1 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 11:47:59,880 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 11:47:59,887 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 11:47:59,957 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.63it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.62it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.79it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
2025-04-14 11:48:00,433 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:48:00,436 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:48:00,503 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:48:00,586 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 11:48:00,580 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:48:00,592 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 11:48:00,592 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:48:00,601 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 11:48:00,601 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:48:00,646 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:48:00,661 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 11:48:00,661 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:48:17,196 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 11:48:17,191 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 11:48:17,194 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 11:48:27,245 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 11:48:27,349 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 11:48:27,373 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 11:48:31,005 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 11:48:31,204 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 11:48:31,279 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 11:48:33,825 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 11:48:34,455 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 11:48:34,604 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 11:48:35,054 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 11:48:35,282 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 11:48:35,517 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 11:48:38,331 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 11:48:38,404 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 11:48:38,434 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 11:48:38,984 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 11:48:39,343 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 11:48:39,516 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 11:48:42,323 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 11:48:42,574 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 11:48:42,604 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 11:48:43,261 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 11:48:43,393 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 11:48:43,535 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 11:48:46,287 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 11:48:46,577 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 11:48:46,646 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 11:48:47,415 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 11:48:47,471 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 11:48:47,513 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 11:48:50,332 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 11:48:50,780 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 11:48:51,108 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 11:48:51,325 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 11:48:51,548 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 11:48:51,851 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 11:48:54,455 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 11:48:54,450 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 11:48:54,639 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 11:48:55,668 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 11:48:55,803 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 11:48:56,194 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 11:48:58,517 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 11:48:58,787 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 11:48:59,050 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 11:48:59,909 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 11:48:59,911 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 11:48:59,911 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 11:48:59,916 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 11:49:00,875 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 11:49:00,884 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 11:49:02,657 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 11:49:03,562 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 11:49:03,653 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 11:49:07,164 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 11:49:07,317 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 11:49:07,479 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 11:49:07,642 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 11:49:08,176 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 11:49:08,214 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 11:50:24,818 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 11:50:25,658 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 11:50:27,050 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 11:50:29,920 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 11:50:31,603 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 11:50:32,791 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 11:50:33,607 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 11:50:35,338 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 11:50:37,582 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 11:50:38,054 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 11:50:39,379 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 11:50:41,405 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 11:50:41,646 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 11:50:42,471 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 11:50:45,780 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 11:50:45,975 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 11:50:46,606 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 11:50:46,615 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 11:50:49,355 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 11:50:49,744 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 11:50:53,748 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 11:50:54,602 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 11:50:54,897 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 11:50:58,026 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 11:50:58,898 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 11:50:59,119 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 11:50:59,501 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 11:51:01,274 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 11:51:05,922 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 11:51:08,642 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 11:52:47,362 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_0.npy
2025-04-14 11:52:47,362 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 194 negative comparisons
2025-04-14 11:52:50,498 - __main__ - INFO - Node 0: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
[E414 11:53:00.960713831 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7c6765be3446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7c671afcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7c671afcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7c671afcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7c671afd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7c671afd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7c6765d4a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7c6766894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7c6766926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:53:00.990423123 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72f4cabde446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72f47ffcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72f47ffcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x72f47ffcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x72f47ffd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72f47ffd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72f4cad455c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72f4cb894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72f4cb926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:53:00,293 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_2.npy
2025-04-14 11:53:00,293 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 184 negative comparisons
2025-04-14 11:53:03,354 - __main__ - INFO - Node 2: Gradient computation completed successfully
2025-04-14 11:53:05,915 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_1.npy
2025-04-14 11:53:05,915 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 197 negative comparisons
[E414 11:53:05.802498933 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e348e7ea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e3443bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e3443bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7e3443bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7e3443bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e3443bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e348e9515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e348f294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e348f326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:53:06.811422031 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74f7e94de446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74f79e7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74f79e7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x74f79e7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x74f79e7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74f79e7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74f7e96455c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74f7ea094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74f7ea126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 11:53:06.813272022 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7212947f1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x721249bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x721249bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x721249bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x721249bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x721249bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7212949585c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x721295294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x721295326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 11:53:09,160 - __main__ - INFO - Node 1: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 5
Worker node 1 successfully completed gradient computation for iteration 5
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 11:53:27,021 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 11:53:27,570 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:53:27,711 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:53:27,724 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 11:53:44,168 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 11:53:47,137 - __main__ - INFO - Loaded gradients from node 0: 194 negative comparisons
2025-04-14 11:53:47,374 - __main__ - INFO - Loaded gradients from node 1: 197 negative comparisons
2025-04-14 11:53:47,610 - __main__ - INFO - Loaded gradients from node 2: 184 negative comparisons
2025-04-14 11:53:47,771 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 11:53:47,964 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 11:53:48,157 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119135263
2025-04-14 11:53:48,349 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107348908
2025-04-14 11:53:48,542 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84832328
2025-04-14 11:53:48,735 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47187581
2025-04-14 11:53:48,928 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22257716
2025-04-14 11:53:49,121 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802297
2025-04-14 11:53:49,314 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552502
2025-04-14 11:53:49,506 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555037
2025-04-14 11:53:49,858 - __main__ - INFO - Non-zero entries after thresholding: 47187581
2025-04-14 11:53:49,858 - __main__ - INFO - Testing stepsize: 0
2025-04-14 11:53:50,517 - __main__ - INFO -   chosen_logps: -267.74521, rejected_logps: -269.53992
2025-04-14 11:53:50,517 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 11:53:50,806 - __main__ - INFO -   chosen_logps: -267.45367, rejected_logps: -269.93921
2025-04-14 11:53:50,806 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 11:53:51,095 - __main__ - INFO -   chosen_logps: -265.36942, rejected_logps: -271.30121
2025-04-14 11:53:51,095 - __main__ - INFO - Testing stepsize: 1
2025-04-14 11:53:51,389 - __main__ - INFO -   chosen_logps: -263.31183, rejected_logps: -273.56522
2025-04-14 11:53:51,389 - __main__ - INFO - Testing stepsize: 2
2025-04-14 11:53:51,685 - __main__ - INFO -   chosen_logps: -258.73233, rejected_logps: -276.41922
2025-04-14 11:53:51,685 - __main__ - INFO - Testing stepsize: 5
2025-04-14 11:53:51,982 - __main__ - INFO -   chosen_logps: -246.18671, rejected_logps: -287.87708
2025-04-14 11:53:51,982 - __main__ - INFO - Testing stepsize: 10
2025-04-14 11:53:52,282 - __main__ - INFO -   chosen_logps: -226.79044, rejected_logps: -308.60513
2025-04-14 11:53:52,282 - __main__ - INFO - Testing stepsize: 15
2025-04-14 11:53:52,582 - __main__ - INFO -   chosen_logps: -209.84262, rejected_logps: -332.54727
2025-04-14 11:53:53,218 - __main__ - INFO - Update scale: 0.011180555555555556
2025-04-14 11:53:53,302 - __main__ - INFO - Model weights updated successfully
2025-04-14 11:53:54,141 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:53:54,176 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:53:55,076 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 11:53:55,111 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 11:53:55,111 - __main__ - INFO - Skipping full model checkpoint for iteration 5
2025-04-14 11:53:55,111 - __main__ - INFO - Iteration 5 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 5
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 5
Iteration 5 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 6
===========================================
Iteration 6 is in valid list - performing full update
Finding noisy pairs for iteration 6...
2025-04-14 11:54:16,122 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 11:54:16,672 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:54:16,819 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 11:54:16,833 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 11:54:16,833 - __main__ - INFO - Loading dataset
2025-04-14 11:54:17,775 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 11:54:39,565 - __main__ - INFO - Processing dataset
2025-04-14 11:54:39,813 - __main__ - INFO - Searching for noisy pairs starting from index 1345
2025-04-14 11:54:51,014 - __main__ - INFO - Processed 55 samples without finding a noisy pair
2025-04-14 11:55:11,654 - __main__ - INFO - Processed 155 samples without finding a noisy pair
2025-04-14 11:55:17,095 - __main__ - INFO - Noisy pair found at index 1530: chosen_logps=-360.0000, rejected_logps=-362.0000
2025-04-14 11:55:17,096 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 11:55:17,164 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_6
2025-04-14 11:55:17,165 - __main__ - INFO - New dataset offset: 1531
Updated dataset offset to: 1531 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 6
Worker node 1 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 11:55:32,680 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]2025-04-14 11:55:33,010 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]2025-04-14 11:55:33,010 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 11:55:33,226 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]2025-04-14 11:55:33,370 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:55:33,385 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 11:55:33,385 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 11:55:33,557 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:55:33,567 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 11:55:33,709 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 11:55:33,707 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:55:33,724 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 11:55:33,724 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 11:55:33,720 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 11:55:33,720 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 11:55:49,970 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 11:55:50,278 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 11:55:50,279 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 11:56:00,162 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 11:56:00,598 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 11:56:00,723 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 11:56:03,934 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 11:56:04,480 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 11:56:04,620 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 11:56:07,119 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 11:56:07,339 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 11:56:07,879 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 11:56:07,983 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 11:56:08,500 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 11:56:08,514 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 11:56:11,143 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 11:56:11,229 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 11:56:11,669 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 11:56:11,973 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 11:56:12,334 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 11:56:12,432 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 11:56:15,219 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 11:56:15,841 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 11:56:16,007 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 11:56:16,049 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 11:56:16,276 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 11:56:16,361 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 11:56:19,091 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 11:56:19,543 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 11:56:20,036 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 11:56:20,168 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 11:56:20,272 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 11:56:20,292 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 11:56:23,380 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 11:56:23,516 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 11:56:23,760 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 11:56:24,054 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 11:56:24,202 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 11:56:24,327 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 11:56:27,380 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 11:56:27,730 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 11:56:27,925 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 11:56:28,249 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 11:56:28,673 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 11:56:28,684 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 11:56:31,373 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 11:56:31,484 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 11:56:31,915 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 11:56:32,535 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 11:56:32,545 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 11:56:32,899 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 11:56:32,909 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 11:56:32,948 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 11:56:32,949 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 11:56:35,730 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 11:56:35,886 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 11:56:36,277 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 11:56:40,104 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 11:56:40,133 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 11:56:40,137 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 11:56:40,421 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 11:56:41,007 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 11:56:41,232 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 11:57:59,563 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 11:57:59,676 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 11:58:03,497 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 11:58:04,320 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 11:58:04,975 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 11:58:06,736 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 11:58:11,024 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 11:58:11,962 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 11:58:12,068 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 11:58:13,629 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 11:58:14,032 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 11:58:15,979 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 11:58:16,606 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 11:58:19,039 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 11:58:19,379 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 11:58:21,251 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 11:58:22,477 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 11:58:23,149 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 11:58:23,738 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 11:58:26,518 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 11:58:28,529 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 11:58:28,902 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 11:58:29,713 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 11:58:30,035 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 11:58:31,034 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 11:58:31,796 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 11:58:32,112 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 11:58:32,919 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 11:58:33,684 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 11:58:37,857 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 12:00:21.432038449 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7112878e3446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71123cbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71123cbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x71123cbcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x71123cbd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71123cbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x711287a4a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x711288494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x711288526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:00:21.452297044 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fbfb8b78446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7fbf6dfcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7fbf6dfcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7fbf6dfcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7fbf6dfd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fbf6dfd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7fbfb8cdf5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7fbfb9694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7fbfb9726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:00:21,774 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_0.npy
2025-04-14 12:00:21,774 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 91 negative comparisons
[E414 12:00:22.910727970 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70cd52510446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70cd077cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70cd077cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x70cd077cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x70cd077d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70cd077d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70cd526775c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70cd53094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70cd53126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:00:22.916667213 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fc82d431446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7fc7e27cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7fc7e27cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7fc7e27cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7fc7e27d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fc7e27d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7fc82d5985c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7fc82de94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7fc82df26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:00:22.929498575 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x769267e4e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76921d1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76921d1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x76921d1cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x76921d1d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76921d1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x769267fb55c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x769268a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x769268b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:00:22,246 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_1.npy
2025-04-14 12:00:22,246 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 92 negative comparisons
2025-04-14 12:00:24,844 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 12:00:25,180 - __main__ - INFO - Node 1: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 6
[E414 12:00:31.809532083 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ebb39adc446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ebaeedcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ebaeedcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7ebaeedcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7ebaeedd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ebaeedd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7ebb39c435c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7ebb3a694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7ebb3a726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:00:32,166 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_2.npy
2025-04-14 12:00:32,166 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 93 negative comparisons
2025-04-14 12:00:35,106 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 6
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 12:00:53,400 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 12:00:53,957 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:00:54,108 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:00:54,123 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 12:01:10,622 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 12:01:13,547 - __main__ - INFO - Loaded gradients from node 0: 91 negative comparisons
2025-04-14 12:01:13,807 - __main__ - INFO - Loaded gradients from node 1: 92 negative comparisons
2025-04-14 12:01:14,051 - __main__ - INFO - Loaded gradients from node 2: 93 negative comparisons
2025-04-14 12:01:14,212 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 12:01:14,403 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 12:01:14,595 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133911
2025-04-14 12:01:14,785 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107347548
2025-04-14 12:01:14,976 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84831187
2025-04-14 12:01:15,166 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47185924
2025-04-14 12:01:15,356 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22259194
2025-04-14 12:01:15,547 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806488
2025-04-14 12:01:15,737 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550782
2025-04-14 12:01:15,928 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555227
2025-04-14 12:01:16,275 - __main__ - INFO - Non-zero entries after thresholding: 47185924
2025-04-14 12:01:16,275 - __main__ - INFO - Using relaxed comparison (count_negative: 276)
2025-04-14 12:01:16,275 - __main__ - INFO - Testing stepsize: 0
2025-04-14 12:01:16,957 - __main__ - INFO -   chosen_logps: -360.31107, rejected_logps: -361.51080
2025-04-14 12:01:16,957 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 12:01:17,260 - __main__ - INFO -   chosen_logps: -360.05975, rejected_logps: -361.52951
2025-04-14 12:01:17,260 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 12:01:17,562 - __main__ - INFO -   chosen_logps: -357.78821, rejected_logps: -362.85138
2025-04-14 12:01:17,563 - __main__ - INFO - Testing stepsize: 1
2025-04-14 12:01:17,868 - __main__ - INFO -   chosen_logps: -354.44366, rejected_logps: -363.99457
2025-04-14 12:01:17,868 - __main__ - INFO - Testing stepsize: 2
2025-04-14 12:01:18,178 - __main__ - INFO -   chosen_logps: -350.03616, rejected_logps: -366.61975
2025-04-14 12:01:18,179 - __main__ - INFO - Testing stepsize: 5
2025-04-14 12:01:18,490 - __main__ - INFO -   chosen_logps: -336.62427, rejected_logps: -374.57245
2025-04-14 12:01:18,490 - __main__ - INFO - Testing stepsize: 10
2025-04-14 12:01:18,803 - __main__ - INFO -   chosen_logps: -316.66772, rejected_logps: -390.16809
2025-04-14 12:01:18,803 - __main__ - INFO - Testing stepsize: 15
2025-04-14 12:01:19,118 - __main__ - INFO -   chosen_logps: -301.53058, rejected_logps: -407.68887
2025-04-14 12:01:19,773 - __main__ - INFO - Update scale: 0.005366666666666667
2025-04-14 12:01:19,774 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 12:01:19,859 - __main__ - INFO - Model weights updated successfully
2025-04-14 12:01:21,175 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:01:21,217 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:01:22,102 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 12:01:22,138 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 12:01:22,138 - __main__ - INFO - Skipping full model checkpoint for iteration 6
2025-04-14 12:01:22,138 - __main__ - INFO - Iteration 6 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 6
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 6
Iteration 6 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 7
===========================================
Iteration 7 is in valid list - performing full update
Finding noisy pairs for iteration 7...
2025-04-14 12:01:43,083 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 12:01:43,641 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:01:43,799 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:01:43,814 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 12:01:43,814 - __main__ - INFO - Loading dataset
2025-04-14 12:01:44,594 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 12:02:05,996 - __main__ - INFO - Processing dataset
2025-04-14 12:02:06,237 - __main__ - INFO - Searching for noisy pairs starting from index 1531
2025-04-14 12:02:20,306 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 12:02:24,859 - __main__ - INFO - Noisy pair found at index 1624: chosen_logps=-127.5000, rejected_logps=-130.0000
2025-04-14 12:02:24,860 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 12:02:24,917 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_7
2025-04-14 12:02:24,917 - __main__ - INFO - New dataset offset: 1625
Updated dataset offset to: 1625 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 7
Worker node 2 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 12:02:40,167 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]2025-04-14 12:02:40,369 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 12:02:40,496 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.52it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]2025-04-14 12:02:40,720 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.71it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 12:02:40,876 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:02:40,892 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 12:02:40,892 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:02:40,914 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s]
2025-04-14 12:02:41,050 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:02:41,054 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:02:41,066 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 12:02:41,066 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:02:41,203 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:02:41,218 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 12:02:41,218 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:02:57,440 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 12:02:57,624 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 12:02:57,864 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 12:03:07,553 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 12:03:07,633 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 12:03:08,362 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 12:03:11,321 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 12:03:11,471 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 12:03:12,186 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 12:03:14,951 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 12:03:15,079 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 12:03:15,561 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 12:03:15,640 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 12:03:15,859 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 12:03:16,092 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 12:03:18,316 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 12:03:18,695 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 12:03:19,060 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 12:03:19,388 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 12:03:19,673 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 12:03:19,899 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 12:03:22,936 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 12:03:22,977 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 12:03:23,205 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 12:03:23,340 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 12:03:23,674 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 12:03:23,722 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 12:03:27,104 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 12:03:27,179 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 12:03:27,526 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 12:03:27,554 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 12:03:27,632 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 12:03:27,677 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 12:03:30,920 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 12:03:30,925 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 12:03:30,934 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 12:03:31,421 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 12:03:31,502 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 12:03:31,844 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 12:03:34,852 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 12:03:35,319 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 12:03:35,599 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 12:03:35,691 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 12:03:35,718 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 12:03:36,017 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 12:03:38,893 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 12:03:39,009 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 12:03:39,522 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 12:03:39,614 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 12:03:39,619 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 12:03:40,283 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 12:03:40,285 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 12:03:40,775 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 12:03:40,776 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 12:03:42,990 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 12:03:43,123 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 12:03:43,699 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 12:03:46,826 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 12:03:47,028 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 12:03:47,922 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 12:03:48,088 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 12:03:48,425 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 12:03:48,758 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 12:05:13,338 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 12:05:13,498 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 12:05:15,002 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 12:05:16,001 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 12:05:16,317 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 12:05:18,362 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 12:05:19,478 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 12:05:20,092 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 12:05:22,129 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 12:05:23,262 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 12:05:24,994 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 12:05:25,061 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 12:05:26,304 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 12:05:26,970 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 12:05:28,428 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 12:05:31,293 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 12:05:32,344 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 12:05:34,209 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 12:05:34,410 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 12:05:36,592 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 12:05:37,892 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 12:05:38,183 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 12:05:38,740 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 12:05:40,813 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 12:05:42,921 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 12:05:43,249 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 12:05:43,603 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 12:05:44,139 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 12:05:47,045 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 12:05:52,958 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 12:07:36,643 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_1.npy
2025-04-14 12:07:36,643 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 165 negative comparisons
[E414 12:07:36.529415586 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x744070515446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7440257cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7440257cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7440257cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7440257d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7440257d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74407067c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x744071094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x744071126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:07:39,819 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 12:07:42.916819381 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x732e3a34e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x732def7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x732def7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x732def7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x732def7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x732def7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x732e3a4b55c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x732e3b094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x732e3b126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:07:42.940163343 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x73d1cbb6c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x73d1813cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x73d1813cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x73d1813cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x73d1813d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x73d1813d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x73d1cc09f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x73d1cca94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x73d1ccb26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:07:42.940386236 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71e6c4e1e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71e67a1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71e67a1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x71e67a1cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x71e67a1d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71e67a1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x71e6c4f855c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x71e6c5a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x71e6c5b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:07:42.946098349 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e08b7278446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7e086c5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7e086c5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7e086c5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7e086c5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e086c5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7e08b73df5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7e08b7e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7e08b7f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:07:42.946269512 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d94d6310446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7d948b5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7d948b5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7d948b5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7d948b5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d948b5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7d94d64775c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7d94d6e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7d94d6f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:07:42,254 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_2.npy
2025-04-14 12:07:42,255 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 154 negative comparisons
2025-04-14 12:07:45,109 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 7
Worker node 2 successfully completed gradient computation for iteration 7
[E414 12:07:54.331471405 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x757de0f57446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x757d963cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x757d963cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x757d963cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x757d963d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x757d963d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x757de10be5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x757de1a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x757de1b26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:07:54.335752980 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78a638464446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78a5ed7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78a5ed7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x78a5ed7cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x78a5ed7d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78a5ed7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78a6385cb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78a639094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78a639126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:07:54,694 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_0.npy
2025-04-14 12:07:54,695 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 163 negative comparisons
2025-04-14 12:07:57,655 - __main__ - INFO - Node 0: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 12:08:16,366 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.24it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 12:08:16,916 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:08:17,074 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:08:17,089 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 12:08:33,650 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 12:08:36,603 - __main__ - INFO - Loaded gradients from node 0: 163 negative comparisons
2025-04-14 12:08:36,856 - __main__ - INFO - Loaded gradients from node 1: 165 negative comparisons
2025-04-14 12:08:37,103 - __main__ - INFO - Loaded gradients from node 2: 154 negative comparisons
2025-04-14 12:08:37,266 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 12:08:37,460 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 12:08:37,655 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119134034
2025-04-14 12:08:37,849 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107348388
2025-04-14 12:08:38,046 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84836896
2025-04-14 12:08:38,240 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47197442
2025-04-14 12:08:38,434 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22254770
2025-04-14 12:08:38,629 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8805420
2025-04-14 12:08:38,823 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550036
2025-04-14 12:08:39,018 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553760
2025-04-14 12:08:39,372 - __main__ - INFO - Non-zero entries after thresholding: 47197442
2025-04-14 12:08:39,372 - __main__ - INFO - Testing stepsize: 0
2025-04-14 12:08:40,124 - __main__ - INFO -   chosen_logps: -127.93242, rejected_logps: -129.48668
2025-04-14 12:08:40,124 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 12:08:40,499 - __main__ - INFO -   chosen_logps: -127.26112, rejected_logps: -129.48666
2025-04-14 12:08:40,499 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 12:08:40,871 - __main__ - INFO -   chosen_logps: -125.68410, rejected_logps: -129.96671
2025-04-14 12:08:40,871 - __main__ - INFO - Testing stepsize: 1
2025-04-14 12:08:41,246 - __main__ - INFO -   chosen_logps: -123.94350, rejected_logps: -130.27760
2025-04-14 12:08:41,246 - __main__ - INFO - Testing stepsize: 2
2025-04-14 12:08:41,627 - __main__ - INFO -   chosen_logps: -119.87563, rejected_logps: -131.02008
2025-04-14 12:08:41,627 - __main__ - INFO - Testing stepsize: 5
2025-04-14 12:08:42,010 - __main__ - INFO -   chosen_logps: -110.15823, rejected_logps: -133.18802
2025-04-14 12:08:42,010 - __main__ - INFO - Testing stepsize: 10
2025-04-14 12:08:42,393 - __main__ - INFO -   chosen_logps: -95.53926, rejected_logps: -137.10068
2025-04-14 12:08:42,393 - __main__ - INFO - Testing stepsize: 15
2025-04-14 12:08:42,776 - __main__ - INFO -   chosen_logps: -83.68451, rejected_logps: -141.11581
2025-04-14 12:08:43,527 - __main__ - INFO - Update scale: 0.009372222222222223
2025-04-14 12:08:43,615 - __main__ - INFO - Model weights updated successfully
2025-04-14 12:08:44,468 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:08:44,504 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:08:45,397 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 12:08:45,433 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 12:08:45,433 - __main__ - INFO - Skipping full model checkpoint for iteration 7
2025-04-14 12:08:45,433 - __main__ - INFO - Iteration 7 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 7
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 7
Iteration 7 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 8
===========================================
Iteration 8 is in valid list - performing full update
Finding noisy pairs for iteration 8...
2025-04-14 12:09:06,461 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.21it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
2025-04-14 12:09:07,016 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:09:07,174 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:09:07,188 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 12:09:07,189 - __main__ - INFO - Loading dataset
2025-04-14 12:09:08,191 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 12:09:29,857 - __main__ - INFO - Processing dataset
2025-04-14 12:09:30,120 - __main__ - INFO - Searching for noisy pairs starting from index 1625
2025-04-14 12:09:37,787 - __main__ - INFO - Noisy pair found at index 1663: chosen_logps=-49.7500, rejected_logps=-50.5000
2025-04-14 12:09:37,788 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 12:09:37,857 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_8
2025-04-14 12:09:37,858 - __main__ - INFO - New dataset offset: 1664
Updated dataset offset to: 1664 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 8
Worker node 2 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 12:09:52,360 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]2025-04-14 12:09:52,640 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 12:09:52,771 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 12:09:52,910 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]2025-04-14 12:09:53,066 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:09:53,081 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 12:09:53,081 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
2025-04-14 12:09:53,196 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.52it/s]
2025-04-14 12:09:53,322 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:09:53,337 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:09:53,350 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 12:09:53,350 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:09:53,475 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:09:53,489 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 12:09:53,490 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:10:09,612 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 12:10:09,879 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 12:10:10,080 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 12:10:19,929 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 12:10:20,122 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 12:10:20,170 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 12:10:23,810 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 12:10:23,958 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 12:10:23,965 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 12:10:26,791 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 12:10:26,888 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 12:10:27,289 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 12:10:27,909 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 12:10:27,937 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 12:10:27,989 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 12:10:31,175 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 12:10:31,285 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 12:10:31,678 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 12:10:31,814 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 12:10:31,922 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 12:10:32,025 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 12:10:35,114 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 12:10:35,237 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 12:10:35,392 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 12:10:35,821 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 12:10:35,833 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 12:10:35,853 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 12:10:39,078 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 12:10:39,275 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 12:10:39,454 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 12:10:39,886 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 12:10:39,918 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 12:10:40,034 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 12:10:42,940 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 12:10:42,974 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 12:10:43,289 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 12:10:43,797 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 12:10:43,958 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 12:10:44,469 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 12:10:47,216 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 12:10:47,530 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 12:10:47,775 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 12:10:48,067 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 12:10:48,177 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 12:10:48,579 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 12:10:51,037 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 12:10:51,586 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 12:10:51,692 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 12:10:52,418 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 12:10:52,422 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 12:10:52,826 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 12:10:52,826 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 12:10:53,136 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 12:10:53,145 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 12:10:55,539 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 12:10:55,736 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 12:10:56,068 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 12:11:00,038 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 12:11:00,422 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 12:11:00,468 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 12:11:00,543 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 12:11:00,568 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 12:11:01,201 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 12:12:22,492 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 12:12:23,044 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 12:12:24,969 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 12:12:26,101 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 12:12:26,127 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 12:12:30,365 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 12:12:30,478 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 12:12:31,245 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 12:12:31,998 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 12:12:35,539 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 12:12:36,209 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 12:12:37,935 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 12:12:38,291 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 12:12:38,870 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 12:12:43,367 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 12:12:43,529 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 12:12:44,120 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 12:12:45,595 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 12:12:46,290 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 12:12:46,675 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 12:12:46,865 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 12:12:50,524 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 12:12:51,216 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 12:12:51,389 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 12:12:55,184 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 12:12:55,901 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 12:12:55,912 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 12:12:56,354 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 12:12:58,498 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 12:12:59,948 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
[E414 12:14:48.012750372 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7be94a3be446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7be8ff7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7be8ff7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7be8ff7cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7be8ff7d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7be8ff7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7be94a5255c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7be94ae94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7be94af26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:14:48.014683147 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7dabb57ea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7dab6abcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7dab6abcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7dab6abcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7dab6abd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7dab6abd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7dabb59515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7dabb6294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7dabb6326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:14:48.022728145 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x732ecc62a446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x732e819cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x732e819cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x732e819cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x732e819d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x732e819d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x732ecc7915c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x732ecd094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x732ecd126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:14:48,345 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_0.npy
2025-04-14 12:14:48,346 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 70 negative comparisons
2025-04-14 12:14:48,438 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_1.npy
2025-04-14 12:14:48,438 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 69 negative comparisons
[E414 12:14:48.324698623 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ed296771446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ed24bbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ed24bbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7ed24bbcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7ed24bbd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ed24bbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7ed2968d85c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7ed297294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7ed297326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:14:51,292 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 12:14:51,567 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 12:14:56,235 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_2.npy
2025-04-14 12:14:56,235 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 78 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 8
Waiting for worker nodes to complete gradient computation...
2025-04-14 12:14:59,153 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 8
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 12:15:17,169 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.20it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
2025-04-14 12:15:17,719 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:15:17,881 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:15:17,896 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 12:15:34,367 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 12:15:37,333 - __main__ - INFO - Loaded gradients from node 0: 70 negative comparisons
2025-04-14 12:15:37,583 - __main__ - INFO - Loaded gradients from node 1: 69 negative comparisons
2025-04-14 12:15:37,826 - __main__ - INFO - Loaded gradients from node 2: 78 negative comparisons
2025-04-14 12:15:37,986 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 12:15:38,177 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 12:15:38,368 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119127869
2025-04-14 12:15:38,559 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107346412
2025-04-14 12:15:38,750 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84833186
2025-04-14 12:15:38,940 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47192396
2025-04-14 12:15:39,132 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22262102
2025-04-14 12:15:39,323 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803372
2025-04-14 12:15:39,514 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551364
2025-04-14 12:15:39,705 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553582
2025-04-14 12:15:40,054 - __main__ - INFO - Non-zero entries after thresholding: 47192396
2025-04-14 12:15:40,054 - __main__ - INFO - Using relaxed comparison (count_negative: 217)
2025-04-14 12:15:40,054 - __main__ - INFO - Testing stepsize: 0
2025-04-14 12:15:40,797 - __main__ - INFO -   chosen_logps: -49.78368, rejected_logps: -50.12823
2025-04-14 12:15:40,797 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 12:15:41,162 - __main__ - INFO -   chosen_logps: -49.78979, rejected_logps: -50.12738
2025-04-14 12:15:41,163 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 12:15:41,527 - __main__ - INFO -   chosen_logps: -49.52060, rejected_logps: -50.34393
2025-04-14 12:15:41,527 - __main__ - INFO - Testing stepsize: 1
2025-04-14 12:15:41,894 - __main__ - INFO -   chosen_logps: -49.11811, rejected_logps: -50.86668
2025-04-14 12:15:41,894 - __main__ - INFO - Testing stepsize: 2
2025-04-14 12:15:42,267 - __main__ - INFO -   chosen_logps: -48.04060, rejected_logps: -51.30357
2025-04-14 12:15:42,267 - __main__ - INFO - Testing stepsize: 5
2025-04-14 12:15:42,640 - __main__ - INFO -   chosen_logps: -45.33694, rejected_logps: -53.01073
2025-04-14 12:15:42,641 - __main__ - INFO - Testing stepsize: 10
2025-04-14 12:15:43,017 - __main__ - INFO -   chosen_logps: -41.45486, rejected_logps: -55.87057
2025-04-14 12:15:43,017 - __main__ - INFO - Testing stepsize: 15
2025-04-14 12:15:43,393 - __main__ - INFO -   chosen_logps: -38.30948, rejected_logps: -58.98142
2025-04-14 12:15:44,126 - __main__ - INFO - Update scale: 0.004219444444444445
2025-04-14 12:15:44,128 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 12:15:44,215 - __main__ - INFO - Model weights updated successfully
2025-04-14 12:15:45,071 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:15:45,114 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:15:46,170 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 12:15:46,210 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 12:15:46,210 - __main__ - INFO - Skipping full model checkpoint for iteration 8
2025-04-14 12:15:46,210 - __main__ - INFO - Iteration 8 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 8
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 8
Iteration 8 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 9
===========================================
Iteration 9 is in valid list - performing full update
Finding noisy pairs for iteration 9...
2025-04-14 12:16:07,598 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]
2025-04-14 12:16:08,156 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:16:08,314 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:16:08,328 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 12:16:08,328 - __main__ - INFO - Loading dataset
2025-04-14 12:16:09,679 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 12:16:31,133 - __main__ - INFO - Processing dataset
2025-04-14 12:16:31,379 - __main__ - INFO - Searching for noisy pairs starting from index 1664
2025-04-14 12:16:39,856 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 12:16:48,625 - __main__ - INFO - Noisy pair found at index 1746: chosen_logps=-23.8750, rejected_logps=-24.2500
2025-04-14 12:16:48,626 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 12:16:48,694 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_9
2025-04-14 12:16:48,694 - __main__ - INFO - New dataset offset: 1747
Updated dataset offset to: 1747 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 9
Worker node 2 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 12:17:03,873 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]2025-04-14 12:17:04,100 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 12:17:04,106 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.24it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 12:17:04,421 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.55it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 12:17:04,579 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:17:04,594 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 12:17:04,594 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:17:04,652 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:17:04,647 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:17:04,788 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:17:04,800 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 12:17:04,800 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:17:04,804 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:17:04,819 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 12:17:04,819 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:17:21,152 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 12:17:21,370 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 12:17:21,404 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 12:17:31,342 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 12:17:31,504 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 12:17:31,602 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 12:17:35,138 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 12:17:35,262 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 12:17:35,470 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 12:17:38,322 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 12:17:38,329 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 12:17:38,856 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 12:17:39,152 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 12:17:39,196 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 12:17:40,144 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 12:17:42,275 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 12:17:42,412 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 12:17:42,596 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 12:17:42,969 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 12:17:43,274 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 12:17:44,527 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 12:17:46,524 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 12:17:46,828 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 12:17:46,916 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 12:17:47,257 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 12:17:47,308 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 12:17:48,869 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 12:17:50,223 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 12:17:50,494 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 12:17:50,724 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 12:17:51,490 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 12:17:51,788 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 12:17:52,885 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 12:17:54,016 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 12:17:54,490 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 12:17:54,841 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 12:17:55,724 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 12:17:56,142 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 12:17:57,196 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 12:17:58,258 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 12:17:59,099 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 12:17:59,227 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 12:18:00,072 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 12:18:00,175 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 12:18:01,576 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 12:18:02,583 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 12:18:03,270 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 12:18:03,660 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 12:18:03,666 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 12:18:04,125 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 12:18:04,126 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 12:18:04,665 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 12:18:05,969 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 12:18:05,976 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 12:18:06,603 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 12:18:07,424 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 12:18:08,751 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 12:18:11,638 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 12:18:11,730 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 12:18:11,871 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 12:18:11,895 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 12:18:13,427 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 12:18:13,483 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 12:19:35,644 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 12:19:35,755 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 12:19:38,673 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 12:19:39,762 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 12:19:39,847 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 12:19:42,561 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 12:19:42,867 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 12:19:45,971 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 12:19:47,590 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 12:19:48,560 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 12:19:48,650 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 12:19:49,155 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 12:19:50,996 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 12:19:51,175 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 12:19:55,694 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 12:19:56,718 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 12:19:57,316 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 12:19:57,437 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 12:19:59,721 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 12:19:59,747 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 12:20:01,639 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 12:20:03,901 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 12:20:06,164 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 12:20:06,588 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 12:20:07,913 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 12:20:09,553 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 12:20:10,661 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 12:20:12,152 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 12:20:12,304 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 12:20:12,970 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 12:22:04.242706636 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7277ff77d446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7277b4bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7277b4bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7277b4bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7277b4bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7277b4bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7277ff8e45c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x727800294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x727800326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:22:04,680 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_1.npy
2025-04-14 12:22:04,680 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 104 negative comparisons
2025-04-14 12:22:07,501 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 12:22:09.599039091 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x71be0e804446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x71bdc3bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x71bdc3bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x71bdc3bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x71bdc3bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x71bdc3bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x71be0e96b5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x71be0f294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x71be0f326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:22:09,918 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_0.npy
2025-04-14 12:22:09,919 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 102 negative comparisons
2025-04-14 12:22:11,196 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_2.npy
2025-04-14 12:22:11,197 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 99 negative comparisons
2025-04-14 12:22:12,719 - __main__ - INFO - Node 0: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 9
2025-04-14 12:22:14,252 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 9
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 12:22:32,653 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.24it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.40it/s]
2025-04-14 12:22:33,211 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:22:33,371 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:22:33,386 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 12:22:49,878 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 12:22:52,906 - __main__ - INFO - Loaded gradients from node 0: 102 negative comparisons
2025-04-14 12:22:53,162 - __main__ - INFO - Loaded gradients from node 1: 104 negative comparisons
2025-04-14 12:22:53,412 - __main__ - INFO - Loaded gradients from node 2: 99 negative comparisons
2025-04-14 12:22:53,578 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 12:22:53,776 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 12:22:53,973 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119128052
2025-04-14 12:22:54,170 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107349550
2025-04-14 12:22:54,368 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84836440
2025-04-14 12:22:54,565 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47190327
2025-04-14 12:22:54,762 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22258121
2025-04-14 12:22:54,959 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806560
2025-04-14 12:22:55,158 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1549956
2025-04-14 12:22:55,354 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 553204
2025-04-14 12:22:55,709 - __main__ - INFO - Non-zero entries after thresholding: 47190327
2025-04-14 12:22:55,709 - __main__ - INFO - Using relaxed comparison (count_negative: 305)
2025-04-14 12:22:55,710 - __main__ - INFO - Testing stepsize: 0
2025-04-14 12:22:56,477 - __main__ - INFO -   chosen_logps: -23.92811, rejected_logps: -23.81994
2025-04-14 12:22:56,477 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 12:22:56,870 - __main__ - INFO -   chosen_logps: -23.88343, rejected_logps: -23.80080
2025-04-14 12:22:56,870 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 12:22:57,262 - __main__ - INFO -   chosen_logps: -23.49772, rejected_logps: -24.05428
2025-04-14 12:22:57,262 - __main__ - INFO - Testing stepsize: 1
2025-04-14 12:22:57,655 - __main__ - INFO -   chosen_logps: -23.09232, rejected_logps: -24.32421
2025-04-14 12:22:57,655 - __main__ - INFO - Testing stepsize: 2
2025-04-14 12:22:58,056 - __main__ - INFO -   chosen_logps: -22.12971, rejected_logps: -24.97643
2025-04-14 12:22:58,056 - __main__ - INFO - Testing stepsize: 5
2025-04-14 12:22:58,458 - __main__ - INFO -   chosen_logps: -19.49197, rejected_logps: -27.04996
2025-04-14 12:22:58,458 - __main__ - INFO - Testing stepsize: 10
2025-04-14 12:22:58,863 - __main__ - INFO -   chosen_logps: -15.61988, rejected_logps: -30.41073
2025-04-14 12:22:58,864 - __main__ - INFO - Testing stepsize: 15
2025-04-14 12:22:59,261 - __main__ - INFO -   chosen_logps: -12.86890, rejected_logps: -33.87326
2025-04-14 12:23:00,044 - __main__ - INFO - Update scale: 0.005930555555555556
2025-04-14 12:23:00,045 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 12:23:00,132 - __main__ - INFO - Model weights updated successfully
2025-04-14 12:23:00,990 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:23:01,028 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:23:01,921 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 12:23:01,958 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 12:23:01,959 - __main__ - INFO - Skipping full model checkpoint for iteration 9
2025-04-14 12:23:01,959 - __main__ - INFO - Iteration 9 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 9
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 9
Iteration 9 completed successfully
Cleaning up iteration files...
Multi-node training completed successfully!
Copying scripts to worker nodes...
Starting with dataset offset: 0
===========================================
Processing iteration 0
===========================================
Iteration 0 is in valid list - performing full update
Finding noisy pairs for iteration 0...
2025-04-14 12:23:26,448 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 12:23:26,998 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:23:27,157 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:23:27,171 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 12:23:27,171 - __main__ - INFO - Loading dataset
2025-04-14 12:23:28,125 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 12:23:49,658 - __main__ - INFO - Processing dataset
2025-04-14 12:23:49,904 - __main__ - INFO - Searching for noisy pairs starting from index 0
2025-04-14 12:23:50,539 - __main__ - INFO - Processed 0 samples without finding a noisy pair
2025-04-14 12:24:10,916 - __main__ - INFO - Processed 100 samples without finding a noisy pair
2025-04-14 12:24:22,175 - __main__ - INFO - Noisy pair found at index 167: chosen_logps=-51.2500, rejected_logps=-51.5000
2025-04-14 12:24:22,176 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 12:24:22,244 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_0
2025-04-14 12:24:22,245 - __main__ - INFO - New dataset offset: 168
Updated dataset offset to: 168 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 0
Worker node 1 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 12:24:38,029 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]2025-04-14 12:24:38,377 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 12:24:38,374 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.40it/s]2025-04-14 12:24:38,579 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.49it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.34it/s]2025-04-14 12:24:38,736 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:24:38,751 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 12:24:38,751 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 12:24:38,922 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:24:38,930 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:24:39,074 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 12:24:39,071 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:24:39,090 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 12:24:39,090 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:24:39,086 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 12:24:39,086 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:24:55,253 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 12:24:55,636 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 12:24:55,635 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 12:25:05,413 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 12:25:05,738 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 12:25:05,785 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 12:25:09,212 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 12:25:09,645 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 12:25:09,671 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 12:25:12,316 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 12:25:12,421 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 12:25:12,533 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 12:25:13,184 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 12:25:13,561 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 12:25:13,674 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 12:25:16,489 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 12:25:16,812 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 12:25:17,015 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 12:25:17,079 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 12:25:17,526 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 12:25:17,805 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 12:25:20,322 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 12:25:20,932 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 12:25:21,156 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 12:25:21,373 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 12:25:21,472 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 12:25:21,756 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 12:25:23,902 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 12:25:24,328 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 12:25:25,244 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 12:25:25,279 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 12:25:25,628 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 12:25:25,920 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 12:25:28,451 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 12:25:28,859 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 12:25:29,150 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 12:25:29,174 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 12:25:29,889 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 12:25:29,991 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 12:25:32,396 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 12:25:33,166 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 12:25:33,315 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 12:25:33,545 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 12:25:34,048 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 12:25:34,116 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 12:25:36,281 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 12:25:37,081 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 12:25:37,339 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 12:25:37,755 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 12:25:37,765 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 12:25:38,534 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 12:25:38,544 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 12:25:38,745 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 12:25:38,753 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 12:25:40,807 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 12:25:41,398 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 12:25:41,461 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 12:25:44,813 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 12:25:44,884 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 12:25:45,897 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 12:25:46,289 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 12:25:46,510 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 12:25:47,043 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 12:27:05,161 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 12:27:05,278 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 12:27:07,185 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 12:27:08,177 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 12:27:08,700 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 12:27:11,009 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 12:27:12,854 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 12:27:13,742 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 12:27:15,687 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 12:27:16,088 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 12:27:17,353 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 12:27:20,254 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 12:27:21,953 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 12:27:22,384 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 12:27:24,062 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 12:27:25,421 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 12:27:25,827 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 12:27:28,808 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 12:27:28,835 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 12:27:28,945 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 12:27:30,108 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 12:27:31,716 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 12:27:33,246 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 12:27:33,886 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 12:27:36,956 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 12:27:37,684 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 12:27:38,030 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 12:27:41,615 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 12:27:41,882 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 12:27:42,122 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
[E414 12:29:28.096543843 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fec923e3446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7fec477cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7fec477cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7fec477cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7fec477d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fec477d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7fec9254a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7fec92e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7fec92f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:29:28,354 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_1.npy
2025-04-14 12:29:28,354 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 150 negative comparisons
2025-04-14 12:29:31,248 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 12:29:34.001914033 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f7d5c123446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f7d113cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f7d113cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f7d113cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f7d113d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f7d113d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f7d5c28a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f7d5cc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f7d5cd26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:29:34.012235772 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f4ea61f1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f4e5b5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f4e5b5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7f4e5b5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f4e5b5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f4e5b5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f4ea63585c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f4ea6c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f4ea6d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:29:34.026118767 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x781f37128446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x781eec3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x781eec3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x781eec3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x781eec3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x781eec3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x781f3728f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x781f37c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x781f37d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:29:34,323 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_0.npy
2025-04-14 12:29:34,323 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 163 negative comparisons
[E414 12:29:34.701431066 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7686b6f68446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76866c3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76866c3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x76866c3cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x76866c3d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76866c3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7686b70cf5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7686b7c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7686b7d26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:29:34.704486479 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7cf0e1646446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7cf0969cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7cf0969cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7cf0969cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7cf0969d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7cf0969d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7cf0e17ad5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7cf0e2294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7cf0e2326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:29:34.725126000 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ff7c9b15446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ff77edcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ff77edcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7ff77edcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7ff77edd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ff77edd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7ff7c9c7c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7ff7ca694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7ff7ca726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:29:35,007 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_0/node_grad_vector_2.npy
2025-04-14 12:29:35,007 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 123 negative comparisons
2025-04-14 12:29:37,240 - __main__ - INFO - Node 0: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 0
2025-04-14 12:29:37,919 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 0
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 12:29:55,581 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 12:29:56,130 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:29:56,289 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:29:56,301 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_0/noisy_batch_0.pt
2025-04-14 12:30:12,817 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 12:30:15,828 - __main__ - INFO - Loaded gradients from node 0: 163 negative comparisons
2025-04-14 12:30:16,074 - __main__ - INFO - Loaded gradients from node 1: 150 negative comparisons
2025-04-14 12:30:16,320 - __main__ - INFO - Loaded gradients from node 2: 123 negative comparisons
2025-04-14 12:30:16,487 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 12:30:16,684 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 12:30:16,881 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119130076
2025-04-14 12:30:17,079 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107351531
2025-04-14 12:30:17,277 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84833334
2025-04-14 12:30:17,474 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47194436
2025-04-14 12:30:17,672 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22251830
2025-04-14 12:30:17,869 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802325
2025-04-14 12:30:18,066 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552251
2025-04-14 12:30:18,265 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555676
2025-04-14 12:30:18,622 - __main__ - INFO - Non-zero entries after thresholding: 47194436
2025-04-14 12:30:18,622 - __main__ - INFO - Testing stepsize: 0
2025-04-14 12:30:19,311 - __main__ - INFO -   chosen_logps: -51.81276, rejected_logps: -51.05098
2025-04-14 12:30:19,311 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 12:30:19,618 - __main__ - INFO -   chosen_logps: -51.77745, rejected_logps: -51.11347
2025-04-14 12:30:19,618 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 12:30:19,927 - __main__ - INFO -   chosen_logps: -51.22326, rejected_logps: -51.53264
2025-04-14 12:30:19,927 - __main__ - INFO - Testing stepsize: 1
2025-04-14 12:30:20,237 - __main__ - INFO -   chosen_logps: -50.83402, rejected_logps: -52.25791
2025-04-14 12:30:20,237 - __main__ - INFO - Testing stepsize: 2
2025-04-14 12:30:20,552 - __main__ - INFO -   chosen_logps: -49.77309, rejected_logps: -53.53894
2025-04-14 12:30:20,552 - __main__ - INFO - Testing stepsize: 5
2025-04-14 12:30:20,868 - __main__ - INFO -   chosen_logps: -46.87643, rejected_logps: -57.40288
2025-04-14 12:30:20,868 - __main__ - INFO - Testing stepsize: 10
2025-04-14 12:30:21,185 - __main__ - INFO -   chosen_logps: -42.09648, rejected_logps: -63.75605
2025-04-14 12:30:21,185 - __main__ - INFO - Testing stepsize: 15
2025-04-14 12:30:21,503 - __main__ - INFO -   chosen_logps: -37.50578, rejected_logps: -70.07899
2025-04-14 12:30:22,176 - __main__ - INFO - Update scale: 0.008477777777777779
2025-04-14 12:30:22,260 - __main__ - INFO - Model weights updated successfully
2025-04-14 12:30:23,107 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:30:23,151 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:30:24,049 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 12:30:24,081 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter0.pt
2025-04-14 12:30:24,081 - __main__ - INFO - Skipping full model checkpoint for iteration 0
2025-04-14 12:30:24,081 - __main__ - INFO - Iteration 0 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 0 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 0
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 0
Iteration 0 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 1
===========================================
Iteration 1 is in valid list - performing full update
Finding noisy pairs for iteration 1...
2025-04-14 12:30:45,401 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 12:30:45,950 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:30:46,095 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:30:46,109 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 12:30:46,109 - __main__ - INFO - Loading dataset
2025-04-14 12:30:46,929 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 12:31:08,468 - __main__ - INFO - Processing dataset
2025-04-14 12:31:08,710 - __main__ - INFO - Searching for noisy pairs starting from index 168
2025-04-14 12:31:15,770 - __main__ - INFO - Processed 32 samples without finding a noisy pair
2025-04-14 12:31:34,581 - __main__ - INFO - Processed 132 samples without finding a noisy pair
2025-04-14 12:31:41,251 - __main__ - INFO - Noisy pair found at index 337: chosen_logps=-90.0000, rejected_logps=-92.0000
2025-04-14 12:31:41,252 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 12:31:41,318 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_1
2025-04-14 12:31:41,319 - __main__ - INFO - New dataset offset: 338
Updated dataset offset to: 338 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 1
Worker node 2 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 12:31:56,464 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 12:31:56,753 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.53it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]2025-04-14 12:31:57,011 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:31:57,063 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 12:31:57,154 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:31:57,169 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 12:31:57,169 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  6.86it/s]2025-04-14 12:31:57,297 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.36it/s]2025-04-14 12:31:57,450 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:31:57,465 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 12:31:57,465 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.46it/s]
2025-04-14 12:31:57,616 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:31:57,757 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:31:57,770 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 12:31:57,770 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:32:13,713 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 12:32:14,010 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 12:32:14,325 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 12:32:23,941 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 12:32:24,103 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 12:32:24,380 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 12:32:27,762 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 12:32:28,328 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 12:32:28,575 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 12:32:31,039 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 12:32:31,083 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 12:32:31,170 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 12:32:31,881 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 12:32:32,217 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 12:32:32,532 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 12:32:34,635 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 12:32:35,499 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 12:32:35,735 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 12:32:35,820 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 12:32:35,996 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 12:32:36,799 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 12:32:39,212 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 12:32:39,467 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 12:32:39,640 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 12:32:39,756 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 12:32:39,899 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 12:32:40,815 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 12:32:42,922 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 12:32:42,957 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 12:32:43,895 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 12:32:44,019 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 12:32:44,307 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 12:32:44,915 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 12:32:46,719 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 12:32:46,901 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 12:32:47,900 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 12:32:48,278 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 12:32:48,928 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 12:32:48,963 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 12:32:51,205 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 12:32:51,525 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 12:32:52,014 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 12:32:52,114 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 12:32:52,467 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 12:32:53,246 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 12:32:54,966 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 12:32:55,398 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 12:32:56,037 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 12:32:56,407 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 12:32:56,412 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 12:32:57,001 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 12:32:57,004 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 12:32:57,599 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 12:32:57,605 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 12:32:59,224 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 12:32:59,746 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 12:33:00,496 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 12:33:03,949 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 12:33:03,951 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 12:33:04,036 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 12:33:04,094 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 12:33:05,277 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 12:33:05,371 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 12:34:21,095 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 12:34:21,746 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 12:34:26,190 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 12:34:27,944 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 12:34:28,929 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 12:34:29,463 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 12:34:29,478 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 12:34:30,855 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 12:34:32,773 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 12:34:34,184 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 12:34:34,507 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 12:34:36,353 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 12:34:36,524 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 12:34:38,219 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 12:34:39,759 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 12:34:40,801 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 12:34:40,978 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 12:34:43,466 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 12:34:44,354 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 12:34:46,248 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 12:34:46,664 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 12:34:49,177 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 12:34:49,239 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 12:34:49,264 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 12:34:53,429 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 12:34:53,788 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 12:34:54,231 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 12:34:54,468 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 12:34:54,711 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 12:34:57,841 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 12:36:39.787477695 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75c6f5c84446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75c6aafcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75c6aafcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x75c6aafcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x75c6aafd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75c6aafd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x75c6f5deb5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x75c6f6894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x75c6f6926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:36:40,158 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_1.npy
2025-04-14 12:36:40,159 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 160 negative comparisons
[E414 12:36:41.614920613 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bdba2557446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7bdb579cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7bdb579cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7bdb579cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7bdb579d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bdb579d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7bdba26be5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7bdba3094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7bdba3126850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:36:41,917 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_0.npy
2025-04-14 12:36:41,917 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 173 negative comparisons
2025-04-14 12:36:43,028 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 12:36:44,844 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 12:36:48,440 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_1/node_grad_vector_2.npy
2025-04-14 12:36:48,441 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 157 negative comparisons
[E414 12:36:48.408103929 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7126ded91446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7126941cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7126941cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7126941cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7126941d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7126941d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7126deef85c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7126dfa94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7126dfb26850 in /lib/x86_64-linux-gnu/libc.so.6)

Worker node 1 successfully completed gradient computation for iteration 1
Waiting for worker nodes to complete gradient computation...
2025-04-14 12:36:51,702 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 1
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 12:37:08,301 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
2025-04-14 12:37:08,847 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:37:08,987 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:37:08,999 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_1/noisy_batch_0.pt
2025-04-14 12:37:25,451 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 12:37:28,437 - __main__ - INFO - Loaded gradients from node 0: 173 negative comparisons
2025-04-14 12:37:28,675 - __main__ - INFO - Loaded gradients from node 1: 160 negative comparisons
2025-04-14 12:37:28,913 - __main__ - INFO - Loaded gradients from node 2: 157 negative comparisons
2025-04-14 12:37:29,075 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 12:37:29,268 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 12:37:29,461 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119133185
2025-04-14 12:37:29,654 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352407
2025-04-14 12:37:29,847 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84830837
2025-04-14 12:37:30,041 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47194948
2025-04-14 12:37:30,234 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22251890
2025-04-14 12:37:30,428 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8807045
2025-04-14 12:37:30,621 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551855
2025-04-14 12:37:30,814 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555300
2025-04-14 12:37:31,166 - __main__ - INFO - Non-zero entries after thresholding: 47194948
2025-04-14 12:37:31,166 - __main__ - INFO - Testing stepsize: 0
2025-04-14 12:37:31,804 - __main__ - INFO -   chosen_logps: -90.05226, rejected_logps: -91.92722
2025-04-14 12:37:31,804 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 12:37:32,073 - __main__ - INFO -   chosen_logps: -89.63415, rejected_logps: -92.11443
2025-04-14 12:37:32,073 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 12:37:32,342 - __main__ - INFO -   chosen_logps: -88.75664, rejected_logps: -92.72580
2025-04-14 12:37:32,342 - __main__ - INFO - Testing stepsize: 1
2025-04-14 12:37:32,614 - __main__ - INFO -   chosen_logps: -87.53758, rejected_logps: -93.44342
2025-04-14 12:37:32,615 - __main__ - INFO - Testing stepsize: 2
2025-04-14 12:37:32,892 - __main__ - INFO -   chosen_logps: -85.48730, rejected_logps: -94.77576
2025-04-14 12:37:32,892 - __main__ - INFO - Testing stepsize: 5
2025-04-14 12:37:33,170 - __main__ - INFO -   chosen_logps: -78.70672, rejected_logps: -99.24473
2025-04-14 12:37:33,170 - __main__ - INFO - Testing stepsize: 10
2025-04-14 12:37:33,450 - __main__ - INFO -   chosen_logps: -68.23800, rejected_logps: -106.85106
2025-04-14 12:37:33,450 - __main__ - INFO - Testing stepsize: 15
2025-04-14 12:37:33,731 - __main__ - INFO -   chosen_logps: -59.40245, rejected_logps: -114.20615
2025-04-14 12:37:34,339 - __main__ - INFO - Update scale: 0.009527777777777777
2025-04-14 12:37:34,420 - __main__ - INFO - Model weights updated successfully
2025-04-14 12:37:35,258 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:37:35,294 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:37:36,205 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 12:37:36,239 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter1.pt
2025-04-14 12:37:36,240 - __main__ - INFO - Skipping full model checkpoint for iteration 1
2025-04-14 12:37:36,240 - __main__ - INFO - Iteration 1 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 1 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 1
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 1
Iteration 1 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 2
===========================================
Iteration 2 is in valid list - performing full update
Finding noisy pairs for iteration 2...
2025-04-14 12:37:57,587 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 12:37:58,135 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:37:58,280 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:37:58,295 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 12:37:58,295 - __main__ - INFO - Loading dataset
2025-04-14 12:37:59,066 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 12:38:20,437 - __main__ - INFO - Processing dataset
2025-04-14 12:38:20,678 - __main__ - INFO - Searching for noisy pairs starting from index 338
2025-04-14 12:38:27,392 - __main__ - INFO - Noisy pair found at index 369: chosen_logps=-153.0000, rejected_logps=-154.0000
2025-04-14 12:38:27,393 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 12:38:27,456 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_2
2025-04-14 12:38:27,456 - __main__ - INFO - New dataset offset: 370
Updated dataset offset to: 370 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 2
Worker node 2 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 12:38:41,564 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.21it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]2025-04-14 12:38:41,919 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
2025-04-14 12:38:42,113 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.68it/s]2025-04-14 12:38:42,258 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:38:42,256 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:38:42,273 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 12:38:42,273 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
2025-04-14 12:38:42,462 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  6.23it/s]2025-04-14 12:38:42,616 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  6.92it/s]/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:38:42,631 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 12:38:42,631 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.23it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.06it/s]
2025-04-14 12:38:42,834 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:38:42,992 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:38:43,004 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 12:38:43,004 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:38:58,822 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 12:38:59,162 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 12:38:59,561 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 12:39:09,038 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 12:39:09,468 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 12:39:09,922 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 12:39:12,859 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 12:39:13,254 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 12:39:13,970 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 12:39:16,044 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 12:39:16,385 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 12:39:16,434 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 12:39:16,721 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 12:39:17,161 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 12:39:17,953 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 12:39:20,302 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 12:39:20,335 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 12:39:20,652 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 12:39:20,837 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 12:39:21,423 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 12:39:21,949 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 12:39:23,903 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 12:39:24,166 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 12:39:24,734 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 12:39:24,962 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 12:39:25,404 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 12:39:25,872 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 12:39:27,910 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 12:39:28,502 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 12:39:28,574 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 12:39:29,038 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 12:39:29,360 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 12:39:29,805 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 12:39:32,252 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 12:39:32,553 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 12:39:32,813 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 12:39:33,296 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 12:39:33,572 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 12:39:33,832 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 12:39:36,104 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 12:39:36,526 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 12:39:36,896 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 12:39:37,533 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 12:39:37,629 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 12:39:38,256 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 12:39:40,529 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 12:39:40,653 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 12:39:41,015 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 12:39:41,688 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 12:39:41,695 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 12:39:42,085 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 12:39:42,090 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 12:39:42,278 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 12:39:42,285 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 12:39:44,608 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 12:39:44,688 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 12:39:45,083 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 12:39:48,819 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 12:39:48,847 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 12:39:49,407 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 12:39:49,602 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 12:39:49,728 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 12:39:49,886 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 12:41:06,203 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 12:41:07,438 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 12:41:07,847 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 12:41:10,580 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 12:41:11,737 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 12:41:12,347 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 12:41:13,512 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 12:41:14,279 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 12:41:14,932 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 12:41:18,536 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 12:41:18,721 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 12:41:19,032 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 12:41:22,786 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 12:41:22,931 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 12:41:23,596 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 12:41:25,607 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 12:41:26,672 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 12:41:27,594 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 12:41:29,393 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 12:41:29,629 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 12:41:31,687 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 12:41:33,378 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 12:41:33,575 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 12:41:34,181 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 12:41:38,485 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 12:41:38,791 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 12:41:39,097 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 12:41:40,038 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 12:41:40,077 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 12:41:41,137 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
[E414 12:43:26.566329057 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x733dda491446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x733d8f7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x733d8f7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x733d8f7cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x733d8f7d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x733d8f7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x733dda5f85c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x733ddb094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x733ddb126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:43:26.568493710 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75ba1c078446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75b9d13cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75b9d13cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x75b9d13cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x75b9d13d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75b9d13d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x75ba1c1df5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x75ba1cc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x75ba1cd26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:43:26,929 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_1.npy
2025-04-14 12:43:26,929 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 130 negative comparisons
2025-04-14 12:43:28,315 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_2.npy
2025-04-14 12:43:28,316 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 117 negative comparisons
[E414 12:43:28.652242663 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b14f0afd446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7b14a5dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7b14a5dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7b14a5dcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7b14a5dd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7b14a5dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7b14f0c645c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7b14f1694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7b14f1726850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:43:28.655301477 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a4bde0b2446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7a4b933cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7a4b933cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7a4b933cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7a4b933d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a4b933d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7a4bde2195c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7a4bdec94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7a4bded26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:43:28.662159045 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x716b2b176446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x716ae05cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x716ae05cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x716ae05cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x716ae05d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x716ae05d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x716b2b2dd5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x716b2bc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x716b2bd26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:43:28,913 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_2/node_grad_vector_0.npy
2025-04-14 12:43:28,913 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 132 negative comparisons
2025-04-14 12:43:29,667 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 12:43:31,145 - __main__ - INFO - Node 2: Gradient computation completed successfully
2025-04-14 12:43:31,952 - __main__ - INFO - Node 0: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 2
Worker node 2 successfully completed gradient computation for iteration 2
Waiting for worker nodes to complete gradient computation...
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 12:43:49,337 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 12:43:49,885 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:43:50,025 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:43:50,038 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_2/noisy_batch_0.pt
2025-04-14 12:44:06,468 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 12:44:09,417 - __main__ - INFO - Loaded gradients from node 0: 132 negative comparisons
2025-04-14 12:44:09,651 - __main__ - INFO - Loaded gradients from node 1: 130 negative comparisons
2025-04-14 12:44:09,884 - __main__ - INFO - Loaded gradients from node 2: 117 negative comparisons
2025-04-14 12:44:10,044 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 12:44:10,234 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 12:44:10,424 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119131606
2025-04-14 12:44:10,613 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107350536
2025-04-14 12:44:10,803 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84827772
2025-04-14 12:44:10,993 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47196450
2025-04-14 12:44:11,183 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252940
2025-04-14 12:44:11,373 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806381
2025-04-14 12:44:11,562 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1552011
2025-04-14 12:44:11,752 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554633
2025-04-14 12:44:12,098 - __main__ - INFO - Non-zero entries after thresholding: 47196450
2025-04-14 12:44:12,098 - __main__ - INFO - Testing stepsize: 0
2025-04-14 12:44:12,727 - __main__ - INFO -   chosen_logps: -152.85400, rejected_logps: -153.89148
2025-04-14 12:44:12,727 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 12:44:12,988 - __main__ - INFO -   chosen_logps: -152.78247, rejected_logps: -154.04767
2025-04-14 12:44:12,988 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 12:44:13,249 - __main__ - INFO -   chosen_logps: -152.12552, rejected_logps: -154.34843
2025-04-14 12:44:13,249 - __main__ - INFO - Testing stepsize: 1
2025-04-14 12:44:13,513 - __main__ - INFO -   chosen_logps: -151.60806, rejected_logps: -154.72366
2025-04-14 12:44:13,513 - __main__ - INFO - Testing stepsize: 2
2025-04-14 12:44:13,781 - __main__ - INFO -   chosen_logps: -150.43002, rejected_logps: -156.03218
2025-04-14 12:44:13,781 - __main__ - INFO - Testing stepsize: 5
2025-04-14 12:44:14,051 - __main__ - INFO -   chosen_logps: -147.23074, rejected_logps: -158.91074
2025-04-14 12:44:14,051 - __main__ - INFO - Testing stepsize: 10
2025-04-14 12:44:14,321 - __main__ - INFO -   chosen_logps: -142.10049, rejected_logps: -164.20396
2025-04-14 12:44:14,322 - __main__ - INFO - Testing stepsize: 15
2025-04-14 12:44:14,595 - __main__ - INFO -   chosen_logps: -138.46216, rejected_logps: -169.50560
2025-04-14 12:44:15,189 - __main__ - INFO - Update scale: 0.007369444444444445
2025-04-14 12:44:15,270 - __main__ - INFO - Model weights updated successfully
2025-04-14 12:44:16,503 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:44:16,541 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:44:17,441 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 12:44:17,476 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter2.pt
2025-04-14 12:44:17,476 - __main__ - INFO - Skipping full model checkpoint for iteration 2
2025-04-14 12:44:17,476 - __main__ - INFO - Iteration 2 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 2 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 2
Worker node 2 successfully applied weights for iteration 2
Iteration 2 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 3
===========================================
Iteration 3 is in valid list - performing full update
Finding noisy pairs for iteration 3...
2025-04-14 12:44:38,439 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 12:44:38,988 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:44:39,133 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:44:39,148 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 12:44:39,148 - __main__ - INFO - Loading dataset
2025-04-14 12:44:51,668 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 12:45:13,240 - __main__ - INFO - Processing dataset
2025-04-14 12:45:13,482 - __main__ - INFO - Searching for noisy pairs starting from index 370
2025-04-14 12:45:19,647 - __main__ - INFO - Processed 30 samples without finding a noisy pair
2025-04-14 12:45:40,174 - __main__ - INFO - Processed 130 samples without finding a noisy pair
2025-04-14 12:46:01,659 - __main__ - INFO - Processed 230 samples without finding a noisy pair
2025-04-14 12:46:20,989 - __main__ - INFO - Processed 330 samples without finding a noisy pair
2025-04-14 12:46:41,255 - __main__ - INFO - Processed 430 samples without finding a noisy pair
2025-04-14 12:47:00,913 - __main__ - INFO - Processed 530 samples without finding a noisy pair
2025-04-14 12:47:19,569 - __main__ - INFO - Processed 630 samples without finding a noisy pair
2025-04-14 12:47:25,051 - __main__ - INFO - Noisy pair found at index 1030: chosen_logps=-280.0000, rejected_logps=-282.0000
2025-04-14 12:47:25,056 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 12:47:25,123 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_3
2025-04-14 12:47:25,124 - __main__ - INFO - New dataset offset: 1031
Updated dataset offset to: 1031 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 3
Worker node 2 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 12:47:40,478 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]2025-04-14 12:47:40,697 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 12:47:40,734 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]2025-04-14 12:47:41,035 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.50it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]
2025-04-14 12:47:41,179 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:47:41,194 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 12:47:41,194 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 12:47:41,242 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:47:41,290 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:47:41,396 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:47:41,411 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 12:47:41,411 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:47:41,438 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:47:41,451 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 12:47:41,451 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:47:57,765 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 12:47:57,963 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 12:47:58,010 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 12:48:07,907 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 12:48:08,069 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 12:48:08,235 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 12:48:11,844 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 12:48:11,866 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 12:48:11,981 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 12:48:15,005 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 12:48:15,087 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 12:48:15,138 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 12:48:15,752 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 12:48:15,768 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 12:48:15,895 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 12:48:18,999 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 12:48:19,003 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 12:48:19,210 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 12:48:19,600 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 12:48:19,635 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 12:48:19,692 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 12:48:22,837 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 12:48:22,980 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 12:48:23,254 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 12:48:23,566 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 12:48:23,703 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 12:48:24,006 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 12:48:26,729 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 12:48:26,807 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 12:48:27,043 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 12:48:27,672 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 12:48:27,855 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 12:48:28,118 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 12:48:30,745 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 12:48:31,004 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 12:48:31,033 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 12:48:31,825 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 12:48:32,171 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 12:48:32,316 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 12:48:34,906 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 12:48:35,167 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 12:48:35,285 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 12:48:36,143 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 12:48:36,327 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 12:48:36,470 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 12:48:39,226 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 12:48:39,252 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 12:48:39,250 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 12:48:40,202 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 12:48:40,213 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 12:48:40,601 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 12:48:40,612 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 12:48:40,740 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 12:48:40,741 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 12:48:43,504 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 12:48:43,625 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 12:48:43,701 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 12:48:47,701 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 12:48:47,745 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 12:48:47,875 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 12:48:47,913 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 12:48:47,954 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 12:48:48,046 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 12:50:03,895 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 12:50:04,821 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 12:50:05,204 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 12:50:08,526 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 12:50:08,941 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 12:50:12,113 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 12:50:12,755 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 12:50:13,520 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 12:50:15,027 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 12:50:16,170 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 12:50:16,598 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 12:50:19,427 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 12:50:20,417 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 12:50:22,120 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 12:50:22,282 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 12:50:24,310 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 12:50:24,610 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 12:50:25,144 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 12:50:27,676 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 12:50:28,842 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 12:50:29,130 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 12:50:35,311 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 12:50:35,603 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 12:50:36,030 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 12:50:36,774 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 12:50:36,955 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 12:50:37,056 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 12:50:39,964 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 12:50:40,488 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 12:50:41,737 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 12:52:25,406 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_0.npy
2025-04-14 12:52:25,406 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 115 negative comparisons
[E414 12:52:25.361957426 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x70c4e7817446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x70c49cbcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x70c49cbcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x70c49cbcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x70c49cbd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x70c49cbd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x70c4e797e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x70c4e8294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x70c4e8326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 12:52:25.360668286 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76831a3f1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7682cf7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7682cf7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7682cf7cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7682cf7d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7682cf7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x76831a5585c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x76831ae94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x76831af26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:52:28,541 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 12:52:30,815 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_2.npy
2025-04-14 12:52:30,815 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 129 negative comparisons
2025-04-14 12:52:33,683 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
2025-04-14 12:52:36,779 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_3/node_grad_vector_1.npy
2025-04-14 12:52:36,779 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 123 negative comparisons
2025-04-14 12:52:39,632 - __main__ - INFO - Node 1: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 3
Worker node 1 successfully completed gradient computation for iteration 3
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 12:52:57,127 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 12:52:57,675 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:52:57,817 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:52:57,830 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_3/noisy_batch_0.pt
2025-04-14 12:53:14,260 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 12:53:17,123 - __main__ - INFO - Loaded gradients from node 0: 115 negative comparisons
2025-04-14 12:53:17,358 - __main__ - INFO - Loaded gradients from node 1: 123 negative comparisons
2025-04-14 12:53:17,590 - __main__ - INFO - Loaded gradients from node 2: 129 negative comparisons
2025-04-14 12:53:17,748 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 12:53:17,937 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 12:53:18,125 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119134444
2025-04-14 12:53:18,314 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107358088
2025-04-14 12:53:18,502 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84840303
2025-04-14 12:53:18,689 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47195909
2025-04-14 12:53:18,877 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22252195
2025-04-14 12:53:19,064 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806081
2025-04-14 12:53:19,251 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551990
2025-04-14 12:53:19,439 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554644
2025-04-14 12:53:19,780 - __main__ - INFO - Non-zero entries after thresholding: 47195909
2025-04-14 12:53:19,780 - __main__ - INFO - Using relaxed comparison (count_negative: 367)
2025-04-14 12:53:19,780 - __main__ - INFO - Testing stepsize: 0
2025-04-14 12:53:20,395 - __main__ - INFO -   chosen_logps: -279.80569, rejected_logps: -281.87384
2025-04-14 12:53:20,395 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 12:53:20,643 - __main__ - INFO -   chosen_logps: -279.42920, rejected_logps: -282.00836
2025-04-14 12:53:20,643 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 12:53:20,891 - __main__ - INFO -   chosen_logps: -277.70331, rejected_logps: -283.15674
2025-04-14 12:53:20,892 - __main__ - INFO - Testing stepsize: 1
2025-04-14 12:53:21,142 - __main__ - INFO -   chosen_logps: -276.36420, rejected_logps: -284.33615
2025-04-14 12:53:21,142 - __main__ - INFO - Testing stepsize: 2
2025-04-14 12:53:21,397 - __main__ - INFO -   chosen_logps: -272.82587, rejected_logps: -286.54184
2025-04-14 12:53:21,397 - __main__ - INFO - Testing stepsize: 5
2025-04-14 12:53:21,653 - __main__ - INFO -   chosen_logps: -262.40411, rejected_logps: -294.08966
2025-04-14 12:53:21,654 - __main__ - INFO - Testing stepsize: 10
2025-04-14 12:53:21,912 - __main__ - INFO -   chosen_logps: -247.85818, rejected_logps: -306.75928
2025-04-14 12:53:21,912 - __main__ - INFO - Testing stepsize: 15
2025-04-14 12:53:22,171 - __main__ - INFO -   chosen_logps: -235.05374, rejected_logps: -319.63522
2025-04-14 12:53:22,738 - __main__ - INFO - Update scale: 0.007136111111111112
2025-04-14 12:53:22,739 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 12:53:22,818 - __main__ - INFO - Model weights updated successfully
2025-04-14 12:53:23,790 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:53:23,825 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:53:25,104 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 12:53:25,138 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter3.pt
2025-04-14 12:53:25,139 - __main__ - INFO - Skipping full model checkpoint for iteration 3
2025-04-14 12:53:25,139 - __main__ - INFO - Iteration 3 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 3 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 3
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 3
Iteration 3 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 4
===========================================
Iteration 4 is in valid list - performing full update
Finding noisy pairs for iteration 4...
2025-04-14 12:53:46,047 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 12:53:46,596 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:53:46,740 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 12:53:46,754 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 12:53:46,754 - __main__ - INFO - Loading dataset
2025-04-14 12:53:47,731 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 12:54:09,438 - __main__ - INFO - Processing dataset
2025-04-14 12:54:09,687 - __main__ - INFO - Searching for noisy pairs starting from index 1031
2025-04-14 12:54:24,630 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 12:54:38,229 - __main__ - INFO - Noisy pair found at index 1163: chosen_logps=-147.0000, rejected_logps=-149.0000
2025-04-14 12:54:38,234 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 12:54:38,300 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_4
2025-04-14 12:54:38,301 - __main__ - INFO - New dataset offset: 1164
Updated dataset offset to: 1164 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 4
Worker node 2 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 12:54:53,403 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.20it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.52it/s]2025-04-14 12:54:53,748 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 12:54:53,814 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.68it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]2025-04-14 12:54:53,952 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.67it/s]2025-04-14 12:54:54,097 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:54:54,112 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 12:54:54,112 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 12:54:54,293 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:54:54,369 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 12:54:54,446 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:54:54,461 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 12:54:54,461 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:54:54,510 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 12:54:54,523 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 12:54:54,523 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 12:55:10,612 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 12:55:11,010 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 12:55:11,065 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 12:55:20,828 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 12:55:21,059 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 12:55:21,237 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 12:55:24,581 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 12:55:24,979 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 12:55:25,063 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 12:55:28,195 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 12:55:28,208 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 12:55:28,465 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 12:55:28,520 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 12:55:28,817 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 12:55:29,499 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 12:55:32,042 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 12:55:32,194 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 12:55:32,335 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 12:55:32,527 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 12:55:32,601 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 12:55:33,441 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 12:55:36,177 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 12:55:36,343 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 12:55:36,454 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 12:55:36,654 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 12:55:36,943 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 12:55:37,829 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 12:55:39,605 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 12:55:39,950 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 12:55:40,412 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 12:55:40,920 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 12:55:41,027 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 12:55:41,813 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 12:55:43,898 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 12:55:44,080 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 12:55:44,776 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 12:55:45,016 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 12:55:45,251 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 12:55:46,074 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 12:55:48,086 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 12:55:48,503 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 12:55:48,972 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 12:55:49,453 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 12:55:49,464 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 12:55:50,283 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 12:55:52,390 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 12:55:52,418 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 12:55:53,206 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 12:55:53,212 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 12:55:53,834 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 12:55:53,925 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 12:55:53,932 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 12:55:54,403 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 12:55:54,413 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 12:55:56,703 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 12:55:56,992 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 12:55:57,846 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 12:56:00,598 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 12:56:00,761 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 12:56:01,437 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 12:56:01,498 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 12:56:01,970 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 12:56:02,132 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 12:57:24,848 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 12:57:26,030 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 12:57:27,548 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 12:57:28,864 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 12:57:29,907 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 12:57:31,681 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 12:57:31,802 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 12:57:31,963 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 12:57:35,695 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 12:57:36,453 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 12:57:36,707 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 12:57:37,269 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 12:57:40,765 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 12:57:41,823 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 12:57:45,746 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 12:57:45,777 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 12:57:46,039 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 12:57:46,473 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 12:57:50,074 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 12:57:50,171 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 12:57:50,664 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 12:57:53,953 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 12:57:54,972 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 12:57:57,054 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 12:57:57,499 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 12:57:57,518 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 12:57:57,747 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 12:57:58,299 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 12:57:58,462 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 12:58:01,452 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 12:59:51,752 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_0.npy
2025-04-14 12:59:51,752 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 138 negative comparisons
2025-04-14 12:59:52,618 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_1.npy
2025-04-14 12:59:52,618 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 143 negative comparisons
2025-04-14 12:59:54,735 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 12:59:55,392 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 12:59:58.185491944 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78de062e3446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78ddbb5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78ddbb5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x78ddbb5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x78ddbb5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78ddbb5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x78de0644a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x78de06e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x78de06f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 12:59:58,513 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_4/node_grad_vector_2.npy
2025-04-14 12:59:58,513 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 166 negative comparisons
2025-04-14 13:00:01,494 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 4
Worker node 2 successfully completed gradient computation for iteration 4
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 13:00:19,879 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 13:00:20,429 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:00:20,570 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:00:20,582 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_4/noisy_batch_0.pt
2025-04-14 13:00:37,054 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 13:00:39,984 - __main__ - INFO - Loaded gradients from node 0: 138 negative comparisons
2025-04-14 13:00:40,220 - __main__ - INFO - Loaded gradients from node 1: 143 negative comparisons
2025-04-14 13:00:40,456 - __main__ - INFO - Loaded gradients from node 2: 166 negative comparisons
2025-04-14 13:00:40,617 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 13:00:40,810 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 13:00:41,003 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119137999
2025-04-14 13:00:41,195 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107350465
2025-04-14 13:00:41,388 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84828607
2025-04-14 13:00:41,581 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47194535
2025-04-14 13:00:41,774 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22260207
2025-04-14 13:00:41,966 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8802928
2025-04-14 13:00:42,159 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550299
2025-04-14 13:00:42,352 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554662
2025-04-14 13:00:42,702 - __main__ - INFO - Non-zero entries after thresholding: 47194535
2025-04-14 13:00:42,702 - __main__ - INFO - Testing stepsize: 0
2025-04-14 13:00:43,451 - __main__ - INFO -   chosen_logps: -147.81654, rejected_logps: -147.74109
2025-04-14 13:00:43,451 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 13:00:43,834 - __main__ - INFO -   chosen_logps: -147.49300, rejected_logps: -147.88171
2025-04-14 13:00:43,834 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 13:00:44,218 - __main__ - INFO -   chosen_logps: -145.85576, rejected_logps: -148.31880
2025-04-14 13:00:44,219 - __main__ - INFO - Testing stepsize: 1
2025-04-14 13:00:44,606 - __main__ - INFO -   chosen_logps: -144.38138, rejected_logps: -149.02078
2025-04-14 13:00:44,606 - __main__ - INFO - Testing stepsize: 2
2025-04-14 13:00:44,997 - __main__ - INFO -   chosen_logps: -141.43483, rejected_logps: -150.35132
2025-04-14 13:00:44,997 - __main__ - INFO - Testing stepsize: 5
2025-04-14 13:00:45,390 - __main__ - INFO -   chosen_logps: -133.08203, rejected_logps: -154.00798
2025-04-14 13:00:45,390 - __main__ - INFO - Testing stepsize: 10
2025-04-14 13:00:45,784 - __main__ - INFO -   chosen_logps: -121.92009, rejected_logps: -160.16077
2025-04-14 13:00:45,784 - __main__ - INFO - Testing stepsize: 15
2025-04-14 13:00:46,179 - __main__ - INFO -   chosen_logps: -114.03669, rejected_logps: -166.71550
2025-04-14 13:00:46,946 - __main__ - INFO - Update scale: 0.008691666666666667
2025-04-14 13:00:47,028 - __main__ - INFO - Model weights updated successfully
2025-04-14 13:00:47,878 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:00:47,912 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:00:48,806 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 13:00:48,840 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter4.pt
2025-04-14 13:00:48,840 - __main__ - INFO - Skipping full model checkpoint for iteration 4
2025-04-14 13:00:48,840 - __main__ - INFO - Iteration 4 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 4 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 4
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 4
Iteration 4 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 5
===========================================
Iteration 5 is in valid list - performing full update
Finding noisy pairs for iteration 5...
2025-04-14 13:01:09,679 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 13:01:10,227 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:01:10,372 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:01:10,387 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 13:01:10,387 - __main__ - INFO - Loading dataset
2025-04-14 13:01:11,133 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 13:01:32,708 - __main__ - INFO - Processing dataset
2025-04-14 13:01:32,951 - __main__ - INFO - Searching for noisy pairs starting from index 1164
2025-04-14 13:01:39,878 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 13:02:01,252 - __main__ - INFO - Processed 136 samples without finding a noisy pair
2025-04-14 13:02:11,386 - __main__ - INFO - Noisy pair found at index 1344: chosen_logps=-268.0000, rejected_logps=-270.0000
2025-04-14 13:02:11,387 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 13:02:11,454 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_5
2025-04-14 13:02:11,454 - __main__ - INFO - New dataset offset: 1345
Updated dataset offset to: 1345 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 5
Worker node 1 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 13:02:26,778 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.31it/s]2025-04-14 13:02:27,078 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]2025-04-14 13:02:27,106 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.63it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 13:02:27,325 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.46it/s]2025-04-14 13:02:27,468 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:02:27,483 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 13:02:27,483 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 13:02:27,624 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:02:27,662 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:02:27,776 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:02:27,791 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 13:02:27,791 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 13:02:27,809 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:02:27,828 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 13:02:27,828 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 13:02:44,075 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 13:02:44,310 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 13:02:44,522 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 13:02:54,264 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 13:02:54,391 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 13:02:54,594 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 13:02:58,104 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 13:02:58,167 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 13:02:58,369 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 13:03:01,183 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 13:03:01,686 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 13:03:01,727 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 13:03:02,058 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 13:03:02,340 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 13:03:02,673 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 13:03:04,777 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 13:03:05,044 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 13:03:05,472 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 13:03:05,841 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 13:03:06,215 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 13:03:06,949 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 13:03:09,195 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 13:03:09,613 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 13:03:10,000 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 13:03:10,122 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 13:03:10,186 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 13:03:10,797 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 13:03:12,871 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 13:03:13,629 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 13:03:14,100 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 13:03:14,111 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 13:03:14,296 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 13:03:14,731 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 13:03:17,386 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 13:03:17,543 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 13:03:18,143 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 13:03:18,221 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 13:03:18,317 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 13:03:19,076 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 13:03:21,215 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 13:03:21,543 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 13:03:21,908 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 13:03:22,482 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 13:03:22,523 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 13:03:23,584 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 13:03:25,482 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 13:03:25,645 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 13:03:26,108 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 13:03:27,089 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 13:03:27,098 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 13:03:27,107 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 13:03:27,115 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 13:03:27,602 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 13:03:27,610 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 13:03:29,503 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 13:03:30,349 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 13:03:30,466 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 13:03:34,274 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 13:03:34,430 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 13:03:34,594 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 13:03:34,967 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 13:03:34,984 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 13:03:35,220 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 13:04:52,037 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 13:04:55,737 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 13:04:57,139 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 13:04:57,137 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 13:04:57,271 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 13:04:59,383 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 13:05:00,475 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 13:05:02,112 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 13:05:04,282 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 13:05:05,453 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 13:05:05,699 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 13:05:05,876 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 13:05:09,220 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 13:05:09,628 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 13:05:11,183 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 13:05:11,904 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 13:05:12,300 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 13:05:15,794 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 13:05:15,924 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 13:05:15,986 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 13:05:16,694 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 13:05:19,620 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 13:05:20,769 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 13:05:21,359 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 13:05:24,969 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 13:05:25,466 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 13:05:26,480 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 13:05:26,512 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 13:05:28,411 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 13:05:28,955 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
[E414 13:07:15.906507687 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7261c7a50446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72617cdcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72617cdcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x72617cdcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x72617cdd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72617cdd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7261c7bb75c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7261c8694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7261c8726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:07:15,167 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_0.npy
2025-04-14 13:07:15,167 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 229 negative comparisons
[E414 13:07:15.384409114 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x781525df6446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7814db1cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7814db1cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7814db1cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7814db1d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7814db1d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x781525f5d5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x781526894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x781526926850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:07:15,770 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_1.npy
2025-04-14 13:07:15,771 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 196 negative comparisons
2025-04-14 13:07:18,146 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 13:07:18,626 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 13:07:19.529092660 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79ecae6ce446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79ec639cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79ec639cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x79ec639cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x79ec639d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79ec639d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79ecae8355c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x79ecaf294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x79ecaf326850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank8]:[E414 13:07:19.539493856 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7856b8fa9446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78566e3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78566e3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x78566e3cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x78566e3d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78566e3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7856b91105c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7856b9c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7856b9d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:07:19,896 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_5/node_grad_vector_2.npy
2025-04-14 13:07:19,896 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 209 negative comparisons
2025-04-14 13:07:22,885 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 5
Worker node 2 successfully completed gradient computation for iteration 5
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 13:07:39,760 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 13:07:40,309 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:07:40,450 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:07:40,462 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_5/noisy_batch_0.pt
2025-04-14 13:07:56,954 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 13:07:59,829 - __main__ - INFO - Loaded gradients from node 0: 229 negative comparisons
2025-04-14 13:08:00,061 - __main__ - INFO - Loaded gradients from node 1: 196 negative comparisons
2025-04-14 13:08:00,294 - __main__ - INFO - Loaded gradients from node 2: 209 negative comparisons
2025-04-14 13:08:00,453 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 13:08:00,642 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 13:08:00,832 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119137974
2025-04-14 13:08:01,020 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107353497
2025-04-14 13:08:01,210 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84837390
2025-04-14 13:08:01,399 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47193661
2025-04-14 13:08:01,588 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22251918
2025-04-14 13:08:01,777 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8806333
2025-04-14 13:08:01,966 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1550539
2025-04-14 13:08:02,155 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555377
2025-04-14 13:08:02,503 - __main__ - INFO - Non-zero entries after thresholding: 47193661
2025-04-14 13:08:02,503 - __main__ - INFO - Testing stepsize: 0
2025-04-14 13:08:03,153 - __main__ - INFO -   chosen_logps: -267.74493, rejected_logps: -269.52414
2025-04-14 13:08:03,153 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 13:08:03,437 - __main__ - INFO -   chosen_logps: -267.53696, rejected_logps: -269.82074
2025-04-14 13:08:03,437 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 13:08:03,722 - __main__ - INFO -   chosen_logps: -265.44727, rejected_logps: -271.02457
2025-04-14 13:08:03,722 - __main__ - INFO - Testing stepsize: 1
2025-04-14 13:08:04,009 - __main__ - INFO -   chosen_logps: -263.04034, rejected_logps: -273.46399
2025-04-14 13:08:04,009 - __main__ - INFO - Testing stepsize: 2
2025-04-14 13:08:04,300 - __main__ - INFO -   chosen_logps: -259.43860, rejected_logps: -276.77313
2025-04-14 13:08:04,300 - __main__ - INFO - Testing stepsize: 5
2025-04-14 13:08:04,593 - __main__ - INFO -   chosen_logps: -246.84674, rejected_logps: -287.45914
2025-04-14 13:08:04,593 - __main__ - INFO - Testing stepsize: 10
2025-04-14 13:08:04,889 - __main__ - INFO -   chosen_logps: -228.99590, rejected_logps: -307.63910
2025-04-14 13:08:04,889 - __main__ - INFO - Testing stepsize: 15
2025-04-14 13:08:05,184 - __main__ - INFO -   chosen_logps: -212.89905, rejected_logps: -330.10672
2025-04-14 13:08:05,809 - __main__ - INFO - Update scale: 0.01232777777777778
2025-04-14 13:08:05,890 - __main__ - INFO - Model weights updated successfully
2025-04-14 13:08:06,724 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:08:06,756 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:08:07,641 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 13:08:07,686 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter5.pt
2025-04-14 13:08:07,686 - __main__ - INFO - Skipping full model checkpoint for iteration 5
2025-04-14 13:08:07,687 - __main__ - INFO - Iteration 5 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 5 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 5
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 5
Iteration 5 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 6
===========================================
Iteration 6 is in valid list - performing full update
Finding noisy pairs for iteration 6...
2025-04-14 13:08:29,052 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 13:08:29,600 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:08:29,744 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:08:29,759 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 13:08:29,759 - __main__ - INFO - Loading dataset
2025-04-14 13:08:40,530 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 13:09:02,192 - __main__ - INFO - Processing dataset
2025-04-14 13:09:02,437 - __main__ - INFO - Searching for noisy pairs starting from index 1345
2025-04-14 13:09:13,635 - __main__ - INFO - Processed 55 samples without finding a noisy pair
2025-04-14 13:09:34,290 - __main__ - INFO - Processed 155 samples without finding a noisy pair
2025-04-14 13:09:39,744 - __main__ - INFO - Noisy pair found at index 1530: chosen_logps=-360.0000, rejected_logps=-362.0000
2025-04-14 13:09:39,745 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 13:09:39,812 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_6
2025-04-14 13:09:39,813 - __main__ - INFO - New dataset offset: 1531
Updated dataset offset to: 1531 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 6
Worker node 2 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 13:09:55,463 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 13:09:55,500 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 13:09:55,582 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.38it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.26it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.64it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.13it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.64it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 13:09:56,010 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 13:09:56,057 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:09:56,135 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:09:56,162 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:09:56,177 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 13:09:56,177 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 13:09:56,198 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:09:56,210 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 13:09:56,210 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 13:09:56,280 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:09:56,295 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 13:09:56,295 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 13:10:12,742 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 13:10:12,785 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 13:10:12,829 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 13:10:22,914 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 13:10:22,979 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 13:10:23,266 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 13:10:26,765 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 13:10:26,807 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 13:10:27,027 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 13:10:29,684 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 13:10:30,022 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 13:10:30,380 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 13:10:30,705 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 13:10:30,754 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 13:10:30,980 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 13:10:34,173 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 13:10:34,195 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 13:10:34,236 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 13:10:34,590 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 13:10:34,621 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 13:10:34,824 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 13:10:37,929 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 13:10:38,017 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 13:10:38,094 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 13:10:38,551 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 13:10:38,637 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 13:10:38,834 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 13:10:41,801 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 13:10:42,133 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 13:10:42,144 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 13:10:42,583 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 13:10:42,779 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 13:10:43,057 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 13:10:45,947 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 13:10:46,014 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 13:10:46,127 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 13:10:46,847 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 13:10:46,922 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 13:10:47,360 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 13:10:49,757 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 13:10:49,867 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 13:10:50,350 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 13:10:51,188 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 13:10:51,592 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 13:10:51,775 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 13:10:53,974 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 13:10:54,201 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 13:10:54,753 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 13:10:55,582 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 13:10:55,588 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 13:10:56,102 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 13:10:56,112 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 13:10:56,191 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 13:10:56,194 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 13:10:58,293 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 13:10:58,922 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 13:10:59,648 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 13:11:02,644 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 13:11:02,843 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 13:11:03,496 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 13:11:03,736 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 13:11:03,893 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 13:11:04,129 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 13:12:22,546 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 13:12:22,879 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 13:12:24,581 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 13:12:25,836 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 13:12:28,142 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 13:12:30,092 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 13:12:30,466 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 13:12:32,227 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 13:12:32,447 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 13:12:34,561 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 13:12:35,924 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 13:12:37,830 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 13:12:38,333 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 13:12:40,151 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 13:12:41,365 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 13:12:43,003 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 13:12:43,047 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 13:12:45,569 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 13:12:45,700 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 13:12:45,994 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 13:12:46,629 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 13:12:50,738 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 13:12:51,623 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 13:12:53,013 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 13:12:54,404 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 13:12:54,629 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 13:12:54,843 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 13:12:55,383 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 13:12:55,944 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 13:12:58,873 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 13:14:45.322830914 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7324a2850446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x732457bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x732457bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x732457bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x732457bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x732457bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7324a29b75c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7324a3494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7324a3526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:14:45,552 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_0.npy
2025-04-14 13:14:45,553 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 86 negative comparisons
[E414 13:14:46.933518739 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d80353b7446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7d7fea7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7d7fea7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7d7fea7cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7d7fea7d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d7fea7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7d803551e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7d8035e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7d8035f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 13:14:46.938604499 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a841b2a5446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7a83d05cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7a83d05cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7a83d05cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7a83d05d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7a83d05d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7a841b40c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7a841be94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7a841bf26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:14:46,250 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_1.npy
2025-04-14 13:14:46,250 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 101 negative comparisons
2025-04-14 13:14:48,410 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 13:14:49,128 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 13:14:51.242255098 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ac39e304446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ac3535cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ac3535cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7ac3535cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7ac3535d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ac3535d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7ac39e46b5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7ac39ee94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7ac39ef26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 13:14:51.254642905 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7317e298f446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x731797dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x731797dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x731797dcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x731797dd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x731797dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7317e2af65c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7317e3694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7317e3726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:14:51,491 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_6/node_grad_vector_2.npy
2025-04-14 13:14:51,492 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 91 negative comparisons
Waiting for worker nodes to complete gradient computation...
2025-04-14 13:14:54,379 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 1 successfully completed gradient computation for iteration 6
Worker node 2 successfully completed gradient computation for iteration 6
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 13:15:11,788 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
2025-04-14 13:15:12,336 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:15:12,477 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:15:12,490 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_6/noisy_batch_0.pt
2025-04-14 13:15:29,021 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 13:15:31,931 - __main__ - INFO - Loaded gradients from node 0: 86 negative comparisons
2025-04-14 13:15:32,167 - __main__ - INFO - Loaded gradients from node 1: 101 negative comparisons
2025-04-14 13:15:32,403 - __main__ - INFO - Loaded gradients from node 2: 91 negative comparisons
2025-04-14 13:15:32,564 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 13:15:32,758 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 13:15:32,952 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119137881
2025-04-14 13:15:33,146 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107354627
2025-04-14 13:15:33,340 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84841396
2025-04-14 13:15:33,534 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47195151
2025-04-14 13:15:33,727 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22257982
2025-04-14 13:15:33,920 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803947
2025-04-14 13:15:34,112 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551497
2025-04-14 13:15:34,306 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554820
2025-04-14 13:15:34,655 - __main__ - INFO - Non-zero entries after thresholding: 47195151
2025-04-14 13:15:34,655 - __main__ - INFO - Using relaxed comparison (count_negative: 278)
2025-04-14 13:15:34,655 - __main__ - INFO - Testing stepsize: 0
2025-04-14 13:15:35,333 - __main__ - INFO -   chosen_logps: -360.30994, rejected_logps: -361.51227
2025-04-14 13:15:35,333 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 13:15:35,637 - __main__ - INFO -   chosen_logps: -359.85858, rejected_logps: -361.50919
2025-04-14 13:15:35,638 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 13:15:35,942 - __main__ - INFO -   chosen_logps: -357.69992, rejected_logps: -362.97894
2025-04-14 13:15:35,942 - __main__ - INFO - Testing stepsize: 1
2025-04-14 13:15:36,249 - __main__ - INFO -   chosen_logps: -355.52707, rejected_logps: -363.84851
2025-04-14 13:15:36,249 - __main__ - INFO - Testing stepsize: 2
2025-04-14 13:15:36,561 - __main__ - INFO -   chosen_logps: -350.41296, rejected_logps: -366.31824
2025-04-14 13:15:36,561 - __main__ - INFO - Testing stepsize: 5
2025-04-14 13:15:36,874 - __main__ - INFO -   chosen_logps: -336.54236, rejected_logps: -373.28033
2025-04-14 13:15:36,874 - __main__ - INFO - Testing stepsize: 10
2025-04-14 13:15:37,189 - __main__ - INFO -   chosen_logps: -314.51965, rejected_logps: -387.79288
2025-04-14 13:15:37,189 - __main__ - INFO - Testing stepsize: 15
2025-04-14 13:15:37,503 - __main__ - INFO -   chosen_logps: -296.38577, rejected_logps: -404.10144
2025-04-14 13:15:38,163 - __main__ - INFO - Update scale: 0.005405555555555556
2025-04-14 13:15:38,164 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 13:15:38,244 - __main__ - INFO - Model weights updated successfully
2025-04-14 13:15:39,350 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:15:39,388 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:15:40,565 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 13:15:40,600 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter6.pt
2025-04-14 13:15:40,601 - __main__ - INFO - Skipping full model checkpoint for iteration 6
2025-04-14 13:15:40,601 - __main__ - INFO - Iteration 6 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 6 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 6
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 6
Iteration 6 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 7
===========================================
Iteration 7 is in valid list - performing full update
Finding noisy pairs for iteration 7...
2025-04-14 13:16:01,661 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.61it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
2025-04-14 13:16:02,210 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:16:02,354 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:16:02,369 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 13:16:02,369 - __main__ - INFO - Loading dataset
2025-04-14 13:16:03,402 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 13:16:25,117 - __main__ - INFO - Processing dataset
2025-04-14 13:16:25,366 - __main__ - INFO - Searching for noisy pairs starting from index 1531
2025-04-14 13:16:39,457 - __main__ - INFO - Processed 69 samples without finding a noisy pair
2025-04-14 13:16:44,007 - __main__ - INFO - Noisy pair found at index 1624: chosen_logps=-127.5000, rejected_logps=-130.0000
2025-04-14 13:16:44,009 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 13:16:44,076 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_7
2025-04-14 13:16:44,077 - __main__ - INFO - New dataset offset: 1625
Updated dataset offset to: 1625 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 7
Worker node 2 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
Worker node 1 starting mode compute for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
2025-04-14 13:16:59,072 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 13:16:59,263 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.28it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.30it/s]2025-04-14 13:16:59,472 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.61it/s]2025-04-14 13:16:59,629 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.77it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
2025-04-14 13:16:59,774 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:16:59,788 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 13:16:59,789 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 13:16:59,806 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.24it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]
2025-04-14 13:16:59,959 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:16:59,974 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 13:16:59,974 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 13:17:00,027 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:17:00,168 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:17:00,180 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 13:17:00,180 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 13:17:16,355 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 13:17:16,555 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 13:17:16,757 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 13:17:26,528 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 13:17:26,562 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 13:17:26,676 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 13:17:30,315 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 13:17:30,366 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 13:17:30,934 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 13:17:33,677 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 13:17:33,701 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 13:17:33,771 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 13:17:34,382 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 13:17:34,472 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 13:17:34,840 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 13:17:37,541 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 13:17:37,618 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 13:17:38,213 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 13:17:38,550 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 13:17:38,626 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 13:17:39,066 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 13:17:41,604 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 13:17:41,710 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 13:17:42,103 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 13:17:42,388 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 13:17:42,700 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 13:17:42,942 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 13:17:45,927 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 13:17:45,932 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 13:17:45,933 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 13:17:46,594 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 13:17:46,639 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 13:17:46,922 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 13:17:49,416 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 13:17:49,862 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 13:17:49,872 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 13:17:50,369 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 13:17:50,471 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 13:17:50,847 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 13:17:53,626 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 13:17:53,851 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 13:17:54,186 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 13:17:54,485 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 13:17:54,768 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 13:17:54,847 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 13:17:57,252 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 13:17:57,832 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 13:17:58,272 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 13:17:58,274 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 13:17:58,311 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 13:17:58,560 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 13:17:58,570 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 13:17:59,021 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 13:17:59,030 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 13:18:01,705 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 13:18:02,244 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 13:18:02,796 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 13:18:05,650 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 13:18:05,802 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 13:18:05,877 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 13:18:05,982 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 13:18:06,762 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 13:18:06,878 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 13:19:29,866 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 13:19:30,944 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 13:19:32,285 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 13:19:32,930 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 13:19:33,817 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 13:19:34,327 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 13:19:37,583 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 13:19:38,307 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 13:19:42,668 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 13:19:42,783 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 13:19:42,930 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 13:19:43,758 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 13:19:45,655 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 13:19:45,863 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 13:19:50,703 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 13:19:51,044 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 13:19:51,066 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 13:19:54,920 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 13:19:55,001 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 13:19:56,784 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 13:19:58,349 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 13:19:58,973 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 13:19:59,419 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 13:20:01,050 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 13:20:01,536 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 13:20:02,216 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 13:20:02,284 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 13:20:02,767 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 13:20:02,914 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 13:20:05,872 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
[E414 13:21:55.324695477 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bee7336c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7bee28bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7bee28bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7bee28bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7bee28bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bee28bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7bee738985c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7bee74294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7bee74326850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 13:21:55.335539366 ProcessGroupNCCL.cpp:542] [Rank 4] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7eb2af4d7446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7eb2647cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7eb2647cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7eb2647cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7eb2647d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7eb2647d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7eb2af63e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7eb2b0094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7eb2b0126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 13:21:55.335812022 ProcessGroupNCCL.cpp:542] [Rank 6] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bc2121ea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7bc1c75cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7bc1c75cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7bc1c75cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7bc1c75d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bc1c75d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7bc2123515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7bc212c94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7bc212d26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:21:55,638 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_0.npy
2025-04-14 13:21:55,638 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 154 negative comparisons
[E414 13:21:56.245051790 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x75ddd4417446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x75dd897cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x75dd897cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x75dd897cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x75dd897d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x75dd897d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x75ddd457e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x75ddd4e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x75ddd4f26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 13:21:56.255615317 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79b7c392a446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79b778bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79b778bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x79b778bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x79b778bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79b778bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79b7c3a915c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x79b7c4494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x79b7c4526850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:21:56,583 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_1.npy
2025-04-14 13:21:56,583 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 176 negative comparisons
2025-04-14 13:21:58,733 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 13:21:59,677 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 13:22:00.276966518 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x72b625be3446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x72b5dafcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x72b5dafcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x72b5dafcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x72b5dafd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x72b5dafd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x72b625d4a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x72b626894ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x72b626926850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 13:22:00.277448143 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f19b53f1446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f196a7cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f196a7cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7f196a7cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f196a7d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f196a7d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f19b55585c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f19b6094ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f19b6126850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 13:22:00.280352619 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f99332a5446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f98e85cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f98e85cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7f98e85cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f98e85d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f98e85d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x7f993340c5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x7f9933e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f9933f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:22:00,506 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_7/node_grad_vector_2.npy
2025-04-14 13:22:00,507 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 155 negative comparisons
2025-04-14 13:22:03,530 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 7
Worker node 2 successfully completed gradient computation for iteration 7
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 13:22:22,356 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  6.03it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  6.57it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  6.95it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  6.78it/s]
2025-04-14 13:22:22,952 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:22:23,119 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:22:23,135 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_7/noisy_batch_0.pt
2025-04-14 13:22:39,685 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 13:22:43,167 - __main__ - INFO - Loaded gradients from node 0: 154 negative comparisons
2025-04-14 13:22:43,443 - __main__ - INFO - Loaded gradients from node 1: 176 negative comparisons
2025-04-14 13:22:43,720 - __main__ - INFO - Loaded gradients from node 2: 155 negative comparisons
2025-04-14 13:22:43,885 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 13:22:44,081 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 13:22:44,277 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119136007
2025-04-14 13:22:44,474 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107352864
2025-04-14 13:22:44,670 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84840285
2025-04-14 13:22:44,867 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47197143
2025-04-14 13:22:45,064 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22255506
2025-04-14 13:22:45,260 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803096
2025-04-14 13:22:45,456 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551121
2025-04-14 13:22:45,653 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555259
2025-04-14 13:22:46,011 - __main__ - INFO - Non-zero entries after thresholding: 47197143
2025-04-14 13:22:46,012 - __main__ - INFO - Testing stepsize: 0
2025-04-14 13:22:46,755 - __main__ - INFO -   chosen_logps: -127.93252, rejected_logps: -129.48647
2025-04-14 13:22:46,755 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 13:22:47,126 - __main__ - INFO -   chosen_logps: -127.35114, rejected_logps: -129.48668
2025-04-14 13:22:47,126 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 13:22:47,500 - __main__ - INFO -   chosen_logps: -125.56921, rejected_logps: -129.96404
2025-04-14 13:22:47,500 - __main__ - INFO - Testing stepsize: 1
2025-04-14 13:22:47,874 - __main__ - INFO -   chosen_logps: -123.48451, rejected_logps: -130.27576
2025-04-14 13:22:47,874 - __main__ - INFO - Testing stepsize: 2
2025-04-14 13:22:48,254 - __main__ - INFO -   chosen_logps: -120.37204, rejected_logps: -131.01834
2025-04-14 13:22:48,255 - __main__ - INFO - Testing stepsize: 5
2025-04-14 13:22:48,637 - __main__ - INFO -   chosen_logps: -108.89735, rejected_logps: -133.57547
2025-04-14 13:22:48,637 - __main__ - INFO - Testing stepsize: 10
2025-04-14 13:22:49,020 - __main__ - INFO -   chosen_logps: -94.02901, rejected_logps: -137.61047
2025-04-14 13:22:49,020 - __main__ - INFO - Testing stepsize: 15
2025-04-14 13:22:49,405 - __main__ - INFO -   chosen_logps: -82.48524, rejected_logps: -141.62044
2025-04-14 13:22:50,165 - __main__ - INFO - Update scale: 0.009430555555555557
2025-04-14 13:22:50,247 - __main__ - INFO - Model weights updated successfully
2025-04-14 13:22:51,089 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:22:51,124 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:22:52,041 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 13:22:52,073 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter7.pt
2025-04-14 13:22:52,073 - __main__ - INFO - Skipping full model checkpoint for iteration 7
2025-04-14 13:22:52,073 - __main__ - INFO - Iteration 7 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 7 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 7
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 7
Iteration 7 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 8
===========================================
Iteration 8 is in valid list - performing full update
Finding noisy pairs for iteration 8...
2025-04-14 13:23:13,203 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 13:23:13,753 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:23:13,906 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:23:13,921 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 13:23:13,921 - __main__ - INFO - Loading dataset
2025-04-14 13:23:16,843 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 13:23:38,337 - __main__ - INFO - Processing dataset
2025-04-14 13:23:38,580 - __main__ - INFO - Searching for noisy pairs starting from index 1625
2025-04-14 13:23:46,181 - __main__ - INFO - Noisy pair found at index 1663: chosen_logps=-49.7500, rejected_logps=-50.5000
2025-04-14 13:23:46,182 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 13:23:46,248 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_8
2025-04-14 13:23:46,249 - __main__ - INFO - New dataset offset: 1664
Updated dataset offset to: 1664 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 8
Worker node 1 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 13:24:00,708 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]2025-04-14 13:24:00,992 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]
2025-04-14 13:24:01,010 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.48it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.41it/s]

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]2025-04-14 13:24:01,266 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.51it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.53it/s]2025-04-14 13:24:01,410 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:24:01,425 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 13:24:01,425 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.43it/s]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]
2025-04-14 13:24:01,547 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:24:01,563 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:24:01,690 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:24:01,703 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 13:24:01,703 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 13:24:01,716 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:24:01,731 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 13:24:01,731 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 13:24:18,000 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 13:24:18,245 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 13:24:18,265 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 13:24:28,034 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 13:24:28,287 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 13:24:28,360 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 13:24:32,027 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 13:24:32,403 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 13:24:32,420 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 13:24:35,214 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 13:24:35,354 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 13:24:35,569 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 13:24:35,970 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 13:24:36,392 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 13:24:36,429 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 13:24:39,106 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 13:24:39,476 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 13:24:39,888 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 13:24:40,097 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 13:24:40,277 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 13:24:40,681 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 13:24:43,686 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 13:24:43,797 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 13:24:43,844 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 13:24:43,899 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 13:24:44,384 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 13:24:44,523 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 13:24:47,443 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 13:24:47,983 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 13:24:48,071 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 13:24:48,573 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 13:24:48,572 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 13:24:48,628 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 13:24:51,375 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 13:24:51,729 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 13:24:51,816 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 13:24:52,142 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 13:24:52,568 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 13:24:52,999 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 13:24:55,454 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 13:24:55,783 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 13:24:56,155 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 13:24:56,442 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 13:24:56,761 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 13:24:57,277 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 13:24:59,557 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 13:24:59,624 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 13:25:00,393 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 13:25:00,515 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 13:25:00,516 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 13:25:01,248 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 13:25:01,258 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 13:25:01,393 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 13:25:01,397 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 13:25:03,950 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 13:25:04,125 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 13:25:04,650 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 13:25:08,285 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 13:25:08,419 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 13:25:08,621 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 13:25:08,709 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 13:25:08,949 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 13:25:08,992 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 13:26:31,122 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 13:26:32,104 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 13:26:32,817 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 13:26:34,018 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 13:26:36,796 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 13:26:38,260 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 13:26:39,293 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 13:26:39,741 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 13:26:42,838 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 13:26:44,644 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 13:26:47,726 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 13:26:47,856 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 13:26:48,959 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 13:26:49,523 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 13:26:50,219 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 13:26:50,932 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 13:26:51,146 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 13:26:51,909 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 13:26:55,094 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 13:26:55,720 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 13:26:58,117 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 13:26:58,725 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 13:26:59,363 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 13:27:00,243 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 13:27:03,943 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
2025-04-14 13:27:04,530 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 13:27:04,581 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 13:27:04,816 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 13:27:04,990 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 13:27:07,411 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 13:28:57,258 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_1.npy
2025-04-14 13:28:57,258 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 85 negative comparisons
[E414 13:28:57.770943863 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x787f7b746446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x787f30bcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x787f30bcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x787f30bcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x787f30bd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x787f30bd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x787f7b8ad5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x787f7c294ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x787f7c326850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:28:58,034 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_0.npy
2025-04-14 13:28:58,034 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 68 negative comparisons
2025-04-14 13:29:00,242 - __main__ - INFO - Node 1: Gradient computation completed successfully
2025-04-14 13:29:00,951 - __main__ - INFO - Node 0: Gradient computation completed successfully
[E414 13:29:03.053852544 ProcessGroupNCCL.cpp:542] [Rank 7] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x736345323446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7362fa5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7362fa5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7362fa5cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7362fa5d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7362fa5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x73634548a5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x736345e94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x736345f26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:29:03,371 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_8/node_grad_vector_2.npy
2025-04-14 13:29:03,371 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 66 negative comparisons
Worker node 1 successfully completed gradient computation for iteration 8
2025-04-14 13:29:06,236 - __main__ - INFO - Node 2: Gradient computation completed successfully
Waiting for worker nodes to complete gradient computation...
Worker node 2 successfully completed gradient computation for iteration 8
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 13:29:23,867 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.58it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
2025-04-14 13:29:24,417 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:29:24,559 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:29:24,571 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_8/noisy_batch_0.pt
2025-04-14 13:29:41,102 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 13:29:43,984 - __main__ - INFO - Loaded gradients from node 0: 68 negative comparisons
2025-04-14 13:29:44,224 - __main__ - INFO - Loaded gradients from node 1: 85 negative comparisons
2025-04-14 13:29:44,463 - __main__ - INFO - Loaded gradients from node 2: 66 negative comparisons
2025-04-14 13:29:44,623 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 13:29:44,814 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 13:29:45,004 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119131731
2025-04-14 13:29:45,194 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107342597
2025-04-14 13:29:45,385 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84835286
2025-04-14 13:29:45,576 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47189797
2025-04-14 13:29:45,765 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22251649
2025-04-14 13:29:45,954 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8804461
2025-04-14 13:29:46,144 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551879
2025-04-14 13:29:46,334 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 555600
2025-04-14 13:29:46,678 - __main__ - INFO - Non-zero entries after thresholding: 47189797
2025-04-14 13:29:46,678 - __main__ - INFO - Using relaxed comparison (count_negative: 219)
2025-04-14 13:29:46,678 - __main__ - INFO - Testing stepsize: 0
2025-04-14 13:29:47,410 - __main__ - INFO -   chosen_logps: -49.78374, rejected_logps: -50.12824
2025-04-14 13:29:47,410 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 13:29:47,771 - __main__ - INFO -   chosen_logps: -49.80797, rejected_logps: -50.12750
2025-04-14 13:29:47,771 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 13:29:48,132 - __main__ - INFO -   chosen_logps: -49.49622, rejected_logps: -50.54282
2025-04-14 13:29:48,132 - __main__ - INFO - Testing stepsize: 1
2025-04-14 13:29:48,498 - __main__ - INFO -   chosen_logps: -48.97044, rejected_logps: -50.85954
2025-04-14 13:29:48,498 - __main__ - INFO - Testing stepsize: 2
2025-04-14 13:29:48,867 - __main__ - INFO -   chosen_logps: -48.27665, rejected_logps: -51.63913
2025-04-14 13:29:48,867 - __main__ - INFO - Testing stepsize: 5
2025-04-14 13:29:49,238 - __main__ - INFO -   chosen_logps: -45.80294, rejected_logps: -54.01822
2025-04-14 13:29:49,238 - __main__ - INFO - Testing stepsize: 10
2025-04-14 13:29:49,610 - __main__ - INFO -   chosen_logps: -42.53643, rejected_logps: -58.14227
2025-04-14 13:29:49,610 - __main__ - INFO - Testing stepsize: 15
2025-04-14 13:29:49,983 - __main__ - INFO -   chosen_logps: -40.10067, rejected_logps: -62.74301
2025-04-14 13:29:50,712 - __main__ - INFO - Update scale: 0.004258333333333334
2025-04-14 13:29:50,713 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 13:29:50,793 - __main__ - INFO - Model weights updated successfully
2025-04-14 13:29:52,113 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:29:52,152 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:29:53,042 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 13:29:53,077 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter8.pt
2025-04-14 13:29:53,077 - __main__ - INFO - Skipping full model checkpoint for iteration 8
2025-04-14 13:29:53,077 - __main__ - INFO - Iteration 8 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 8 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 8
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 8
Iteration 8 completed successfully
Cleaning up iteration files...
===========================================
Processing iteration 9
===========================================
Iteration 9 is in valid list - performing full update
Finding noisy pairs for iteration 9...
2025-04-14 13:30:14,091 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.62it/s]
2025-04-14 13:30:14,638 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:30:14,782 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:30:14,797 - __main__ - INFO - Finding noisy pairs for this iteration
2025-04-14 13:30:14,797 - __main__ - INFO - Loading dataset
2025-04-14 13:30:15,983 - __main__ - INFO - Creating DPO config and trainer
2025-04-14 13:30:37,514 - __main__ - INFO - Processing dataset
2025-04-14 13:30:37,761 - __main__ - INFO - Searching for noisy pairs starting from index 1664
2025-04-14 13:30:46,228 - __main__ - INFO - Processed 36 samples without finding a noisy pair
2025-04-14 13:30:55,020 - __main__ - INFO - Noisy pair found at index 1746: chosen_logps=-23.8750, rejected_logps=-24.2500
2025-04-14 13:30:55,021 - __main__ - INFO - Saved noisy batch to /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 13:30:55,088 - __main__ - INFO - Found 1 noisy batch(es), saved to /home/user/dpo/compo/noisy_batches/iter_9
2025-04-14 13:30:55,088 - __main__ - INFO - New dataset offset: 1747
Updated dataset offset to: 1747 for next iteration
Copying noisy batches and weights to worker nodes...
Starting worker nodes for gradient computation...
Starting master node computation for iteration 9
Worker node 1 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode compute for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
2025-04-14 13:31:10,170 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.33it/s]2025-04-14 13:31:10,453 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.57it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-14 13:31:10,574 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.56it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.53it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.25it/s]2025-04-14 13:31:10,721 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.34it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.50it/s]2025-04-14 13:31:10,865 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:31:10,879 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 13:31:10,879 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.32it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.44it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.42it/s]
2025-04-14 13:31:11,010 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)

Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.51it/s]
2025-04-14 13:31:11,125 - __main__ - INFO - Loading weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:31:11,150 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker2_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:31:11,163 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 13:31:11,163 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 13:31:11,278 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/worker_weights/worker1_weights.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:31:11,293 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 13:31:11,293 - __main__ - INFO - Loaded 1 noisy batch(es) for gradient computation
2025-04-14 13:31:27,418 - __main__ - INFO - Node 0: Using 10 GPUs for gradient computation
2025-04-14 13:31:27,704 - __main__ - INFO - Node 2: Using 10 GPUs for gradient computation
2025-04-14 13:31:27,894 - __main__ - INFO - Node 1: Using 10 GPUs for gradient computation
2025-04-14 13:31:37,742 - __mp_main__ - INFO - Node 0, GPU 1: Initialized process group
2025-04-14 13:31:37,938 - __mp_main__ - INFO - Node 2, GPU 1: Initialized process group
2025-04-14 13:31:38,129 - __mp_main__ - INFO - Node 1, GPU 1: Initialized process group
2025-04-14 13:31:41,586 - __mp_main__ - INFO - Node 0, GPU 2: Initialized process group
2025-04-14 13:31:41,757 - __mp_main__ - INFO - Node 2, GPU 2: Initialized process group
2025-04-14 13:31:42,612 - __mp_main__ - INFO - Node 1, GPU 2: Initialized process group
2025-04-14 13:31:44,651 - __mp_main__ - INFO - Node 0, GPU 1: Processed 0 perturbations
2025-04-14 13:31:44,845 - __mp_main__ - INFO - Node 2, GPU 1: Processed 0 perturbations
2025-04-14 13:31:45,227 - __mp_main__ - INFO - Node 1, GPU 1: Processed 0 perturbations
2025-04-14 13:31:45,693 - __mp_main__ - INFO - Node 2, GPU 3: Initialized process group
2025-04-14 13:31:45,944 - __mp_main__ - INFO - Node 0, GPU 3: Initialized process group
2025-04-14 13:31:46,537 - __mp_main__ - INFO - Node 1, GPU 3: Initialized process group
2025-04-14 13:31:48,445 - __mp_main__ - INFO - Node 0, GPU 2: Processed 0 perturbations
2025-04-14 13:31:49,498 - __mp_main__ - INFO - Node 2, GPU 2: Processed 0 perturbations
2025-04-14 13:31:49,531 - __mp_main__ - INFO - Node 2, GPU 4: Initialized process group
2025-04-14 13:31:49,908 - __mp_main__ - INFO - Node 1, GPU 2: Processed 0 perturbations
2025-04-14 13:31:50,204 - __mp_main__ - INFO - Node 0, GPU 4: Initialized process group
2025-04-14 13:31:50,367 - __mp_main__ - INFO - Node 1, GPU 4: Initialized process group
2025-04-14 13:31:53,122 - __mp_main__ - INFO - Node 2, GPU 3: Processed 0 perturbations
2025-04-14 13:31:53,403 - __mp_main__ - INFO - Node 2, GPU 5: Initialized process group
2025-04-14 13:31:53,562 - __mp_main__ - INFO - Node 0, GPU 3: Processed 0 perturbations
2025-04-14 13:31:53,949 - __mp_main__ - INFO - Node 1, GPU 3: Processed 0 perturbations
2025-04-14 13:31:54,121 - __mp_main__ - INFO - Node 0, GPU 5: Initialized process group
2025-04-14 13:31:54,223 - __mp_main__ - INFO - Node 1, GPU 5: Initialized process group
2025-04-14 13:31:56,985 - __mp_main__ - INFO - Node 2, GPU 4: Processed 0 perturbations
2025-04-14 13:31:57,348 - __mp_main__ - INFO - Node 2, GPU 6: Initialized process group
2025-04-14 13:31:57,877 - __mp_main__ - INFO - Node 0, GPU 4: Processed 0 perturbations
2025-04-14 13:31:57,976 - __mp_main__ - INFO - Node 1, GPU 4: Processed 0 perturbations
2025-04-14 13:31:58,510 - __mp_main__ - INFO - Node 1, GPU 6: Initialized process group
2025-04-14 13:31:58,689 - __mp_main__ - INFO - Node 0, GPU 6: Initialized process group
2025-04-14 13:32:00,702 - __mp_main__ - INFO - Node 2, GPU 5: Processed 0 perturbations
2025-04-14 13:32:01,353 - __mp_main__ - INFO - Node 1, GPU 5: Processed 0 perturbations
2025-04-14 13:32:01,446 - __mp_main__ - INFO - Node 2, GPU 7: Initialized process group
2025-04-14 13:32:01,696 - __mp_main__ - INFO - Node 0, GPU 5: Processed 0 perturbations
2025-04-14 13:32:02,584 - __mp_main__ - INFO - Node 1, GPU 7: Initialized process group
2025-04-14 13:32:02,895 - __mp_main__ - INFO - Node 0, GPU 7: Initialized process group
2025-04-14 13:32:04,955 - __mp_main__ - INFO - Node 2, GPU 6: Processed 0 perturbations
2025-04-14 13:32:05,469 - __mp_main__ - INFO - Node 2, GPU 8: Initialized process group
2025-04-14 13:32:05,839 - __mp_main__ - INFO - Node 1, GPU 6: Processed 0 perturbations
2025-04-14 13:32:06,230 - __mp_main__ - INFO - Node 0, GPU 6: Processed 0 perturbations
2025-04-14 13:32:06,971 - __mp_main__ - INFO - Node 0, GPU 8: Initialized process group
2025-04-14 13:32:07,133 - __mp_main__ - INFO - Node 1, GPU 8: Initialized process group
2025-04-14 13:32:08,598 - __mp_main__ - INFO - Node 2, GPU 7: Processed 0 perturbations
2025-04-14 13:32:09,688 - __mp_main__ - INFO - Node 2, GPU 9: Initialized process group
2025-04-14 13:32:09,695 - __mp_main__ - INFO - Node 2, GPU 0: Initialized process group
2025-04-14 13:32:10,212 - __mp_main__ - INFO - Node 1, GPU 7: Processed 0 perturbations
2025-04-14 13:32:10,292 - __mp_main__ - INFO - Node 0, GPU 7: Processed 0 perturbations
2025-04-14 13:32:11,066 - __mp_main__ - INFO - Node 0, GPU 9: Initialized process group
2025-04-14 13:32:11,072 - __mp_main__ - INFO - Node 0, GPU 0: Initialized process group
2025-04-14 13:32:11,345 - __mp_main__ - INFO - Node 1, GPU 9: Initialized process group
2025-04-14 13:32:11,347 - __mp_main__ - INFO - Node 1, GPU 0: Initialized process group
2025-04-14 13:32:13,039 - __mp_main__ - INFO - Node 2, GPU 8: Processed 0 perturbations
2025-04-14 13:32:14,240 - __mp_main__ - INFO - Node 1, GPU 8: Processed 0 perturbations
2025-04-14 13:32:14,512 - __mp_main__ - INFO - Node 0, GPU 8: Processed 0 perturbations
2025-04-14 13:32:17,963 - __mp_main__ - INFO - Node 2, GPU 0: Processed 0 perturbations
2025-04-14 13:32:18,026 - __mp_main__ - INFO - Node 2, GPU 9: Processed 0 perturbations
2025-04-14 13:32:18,344 - __mp_main__ - INFO - Node 0, GPU 9: Processed 0 perturbations
2025-04-14 13:32:18,445 - __mp_main__ - INFO - Node 0, GPU 0: Processed 0 perturbations
2025-04-14 13:32:18,650 - __mp_main__ - INFO - Node 1, GPU 0: Processed 0 perturbations
2025-04-14 13:32:18,957 - __mp_main__ - INFO - Node 1, GPU 9: Processed 0 perturbations
2025-04-14 13:33:42,936 - __mp_main__ - INFO - Node 0, GPU 1: Processed 30 perturbations
2025-04-14 13:33:43,013 - __mp_main__ - INFO - Node 1, GPU 1: Processed 30 perturbations
2025-04-14 13:33:44,668 - __mp_main__ - INFO - Node 0, GPU 2: Processed 30 perturbations
2025-04-14 13:33:45,225 - __mp_main__ - INFO - Node 2, GPU 1: Processed 30 perturbations
2025-04-14 13:33:47,011 - __mp_main__ - INFO - Node 1, GPU 2: Processed 30 perturbations
2025-04-14 13:33:49,343 - __mp_main__ - INFO - Node 2, GPU 2: Processed 30 perturbations
2025-04-14 13:33:49,843 - __mp_main__ - INFO - Node 0, GPU 3: Processed 30 perturbations
2025-04-14 13:33:51,806 - __mp_main__ - INFO - Node 1, GPU 3: Processed 30 perturbations
2025-04-14 13:33:53,966 - __mp_main__ - INFO - Node 2, GPU 3: Processed 30 perturbations
2025-04-14 13:33:55,906 - __mp_main__ - INFO - Node 0, GPU 4: Processed 30 perturbations
2025-04-14 13:33:56,261 - __mp_main__ - INFO - Node 1, GPU 4: Processed 30 perturbations
2025-04-14 13:33:57,584 - __mp_main__ - INFO - Node 2, GPU 4: Processed 30 perturbations
2025-04-14 13:33:57,907 - __mp_main__ - INFO - Node 2, GPU 5: Processed 30 perturbations
2025-04-14 13:34:00,015 - __mp_main__ - INFO - Node 1, GPU 5: Processed 30 perturbations
2025-04-14 13:34:02,220 - __mp_main__ - INFO - Node 2, GPU 6: Processed 30 perturbations
2025-04-14 13:34:02,990 - __mp_main__ - INFO - Node 1, GPU 6: Processed 30 perturbations
2025-04-14 13:34:03,052 - __mp_main__ - INFO - Node 0, GPU 5: Processed 30 perturbations
2025-04-14 13:34:05,721 - __mp_main__ - INFO - Node 0, GPU 6: Processed 30 perturbations
2025-04-14 13:34:06,261 - __mp_main__ - INFO - Node 0, GPU 7: Processed 30 perturbations
2025-04-14 13:34:07,717 - __mp_main__ - INFO - Node 2, GPU 7: Processed 30 perturbations
2025-04-14 13:34:08,219 - __mp_main__ - INFO - Node 1, GPU 7: Processed 30 perturbations
2025-04-14 13:34:09,736 - __mp_main__ - INFO - Node 2, GPU 8: Processed 30 perturbations
2025-04-14 13:34:09,970 - __mp_main__ - INFO - Node 1, GPU 8: Processed 30 perturbations
2025-04-14 13:34:11,013 - __mp_main__ - INFO - Node 0, GPU 8: Processed 30 perturbations
2025-04-14 13:34:15,173 - __mp_main__ - INFO - Node 0, GPU 9: Processed 30 perturbations
2025-04-14 13:34:15,441 - __mp_main__ - INFO - Node 1, GPU 0: Processed 30 perturbations
2025-04-14 13:34:15,930 - __mp_main__ - INFO - Node 0, GPU 0: Processed 30 perturbations
2025-04-14 13:34:16,169 - __mp_main__ - INFO - Node 1, GPU 9: Processed 30 perturbations
2025-04-14 13:34:18,116 - __mp_main__ - INFO - Node 2, GPU 0: Processed 30 perturbations
2025-04-14 13:34:18,314 - __mp_main__ - INFO - Node 2, GPU 9: Processed 30 perturbations
[E414 13:36:09.374859757 ProcessGroupNCCL.cpp:542] [Rank 9] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74e6a4987446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x74e659dcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x74e659dcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x74e659dcc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x74e659dd3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x74e659dd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x74e6a4aee5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x74e6a5494ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x74e6a5526850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 13:36:09.379613954 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=2, OpType=REDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x791684fd7446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x79163a3cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x79163a3cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x79163a3cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x79163a3d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79163a3d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x79168513e5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x791685a94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x791685b26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:36:09,658 - __mp_main__ - INFO - Node 0: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_0.npy
2025-04-14 13:36:09,658 - __mp_main__ - INFO - Node 0: Processed total 600 samples with 87 negative comparisons
2025-04-14 13:36:09,949 - __mp_main__ - INFO - Node 1: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_1.npy
2025-04-14 13:36:09,949 - __mp_main__ - INFO - Node 1: Processed total 600 samples with 110 negative comparisons
[E414 13:36:10.846301841 ProcessGroupNCCL.cpp:542] [Rank 5] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x76d5acbea446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x76d561fcbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x76d561fcc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x76d561fcc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x76d561fd4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x76d561fd561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x76d5acd515c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x76d5ad694ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x76d5ad726850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:36:12,541 - __main__ - INFO - Node 0: Gradient computation completed successfully
2025-04-14 13:36:13,036 - __main__ - INFO - Node 1: Gradient computation completed successfully
[E414 13:36:16.307664203 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x791aaa31e446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x791a5f5cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x791a5f5cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x791a5f5cc4a0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x791a5f5d4378 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x791a5f5d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x791aaa4855c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x791aaae94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x791aaaf26850 in /lib/x86_64-linux-gnu/libc.so.6)

[E414 13:36:16.320315038 ProcessGroupNCCL.cpp:542] [Rank 8] Collective WorkNCCL(SeqNum=1, OpType=REDUCE, NumelIn=131072000, NumelOut=131072000, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:

Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x744a0ad6c446 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7449c05cbf80 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7449c05cc1cc in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7449c05cc3e0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7449c05d3b5a in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7449c05d561d in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x145c0 (0x744a0b29f5c0 in /home/user/dpo/env/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #7: <unknown function> + 0x94ac3 (0x744a0bc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x744a0bd26850 in /lib/x86_64-linux-gnu/libc.so.6)

2025-04-14 13:36:16,610 - __mp_main__ - INFO - Node 2: Saved aggregated gradient vector to /home/user/dpo/compo/gradients/iter_9/node_grad_vector_2.npy
2025-04-14 13:36:16,610 - __mp_main__ - INFO - Node 2: Processed total 600 samples with 103 negative comparisons
Waiting for worker nodes to complete gradient computation...
Worker node 1 successfully completed gradient computation for iteration 9
2025-04-14 13:36:19,316 - __main__ - INFO - Node 2: Gradient computation completed successfully
Worker node 2 successfully completed gradient computation for iteration 9
WARNING: Missing gradient files for node 1
WARNING: Missing gradient files for node 2
WARNING: Some gradient files are missing. Attempting to retrieve from worker nodes...
Aggregating gradients and updating model...
2025-04-14 13:36:36,412 - __main__ - INFO - Will save checkpoints for iterations: [36, 40, 45]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.59it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.57it/s]
2025-04-14 13:36:36,960 - __main__ - INFO - Loading weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:701: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(file_path)
2025-04-14 13:36:37,100 - __main__ - INFO - Successfully applied weights from /home/user/dpo/compo/temp_weights/current_lm_head.pt
/home/user/dpo/compo/compute_gradients.py:474: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch_data = torch.load(file_path)
2025-04-14 13:36:37,112 - __main__ - INFO - Loaded noisy batch from /home/user/dpo/compo/noisy_batches/iter_9/noisy_batch_0.pt
2025-04-14 13:36:53,556 - __main__ - INFO - Aggregating gradients using device cuda:0
2025-04-14 13:36:56,470 - __main__ - INFO - Loaded gradients from node 0: 87 negative comparisons
2025-04-14 13:36:56,708 - __main__ - INFO - Loaded gradients from node 1: 110 negative comparisons
2025-04-14 13:36:56,948 - __main__ - INFO - Loaded gradients from node 2: 103 negative comparisons
2025-04-14 13:36:57,111 - __main__ - INFO - Number of entries in g_hat: 131072000
2025-04-14 13:36:57,304 - __main__ - INFO - 0 - Non-zero entries after thresholding: 131072000
2025-04-14 13:36:57,498 - __main__ - INFO - 1e-05 - Non-zero entries after thresholding: 119129046
2025-04-14 13:36:57,691 - __main__ - INFO - 2e-05 - Non-zero entries after thresholding: 107346576
2025-04-14 13:36:57,884 - __main__ - INFO - 4e-05 - Non-zero entries after thresholding: 84831230
2025-04-14 13:36:58,077 - __main__ - INFO - 8e-05 - Non-zero entries after thresholding: 47193116
2025-04-14 13:36:58,269 - __main__ - INFO - 0.00012 - Non-zero entries after thresholding: 22257306
2025-04-14 13:36:58,462 - __main__ - INFO - 0.00016 - Non-zero entries after thresholding: 8803440
2025-04-14 13:36:58,654 - __main__ - INFO - 0.00022 - Non-zero entries after thresholding: 1551899
2025-04-14 13:36:58,846 - __main__ - INFO - 0.00025 - Non-zero entries after thresholding: 554305
2025-04-14 13:36:59,195 - __main__ - INFO - Non-zero entries after thresholding: 47193116
2025-04-14 13:36:59,195 - __main__ - INFO - Using relaxed comparison (count_negative: 300)
2025-04-14 13:36:59,195 - __main__ - INFO - Testing stepsize: 0
2025-04-14 13:36:59,946 - __main__ - INFO -   chosen_logps: -23.92811, rejected_logps: -23.81994
2025-04-14 13:36:59,946 - __main__ - INFO - Testing stepsize: 0.1
2025-04-14 13:37:00,331 - __main__ - INFO -   chosen_logps: -23.90104, rejected_logps: -23.80727
2025-04-14 13:37:00,331 - __main__ - INFO - Testing stepsize: 0.5
2025-04-14 13:37:00,717 - __main__ - INFO -   chosen_logps: -23.51377, rejected_logps: -24.10437
2025-04-14 13:37:00,717 - __main__ - INFO - Testing stepsize: 1
2025-04-14 13:37:01,105 - __main__ - INFO -   chosen_logps: -23.03496, rejected_logps: -24.42006
2025-04-14 13:37:01,105 - __main__ - INFO - Testing stepsize: 2
2025-04-14 13:37:01,498 - __main__ - INFO -   chosen_logps: -22.23171, rejected_logps: -25.03327
2025-04-14 13:37:01,498 - __main__ - INFO - Testing stepsize: 5
2025-04-14 13:37:01,891 - __main__ - INFO -   chosen_logps: -19.68331, rejected_logps: -26.89964
2025-04-14 13:37:01,891 - __main__ - INFO - Testing stepsize: 10
2025-04-14 13:37:02,286 - __main__ - INFO -   chosen_logps: -16.17327, rejected_logps: -30.15381
2025-04-14 13:37:02,286 - __main__ - INFO - Testing stepsize: 15
2025-04-14 13:37:02,683 - __main__ - INFO -   chosen_logps: -13.74407, rejected_logps: -33.48358
2025-04-14 13:37:03,454 - __main__ - INFO - Update scale: 0.005833333333333334
2025-04-14 13:37:03,456 - __main__ - INFO - Didn't pass, keeping original weights
2025-04-14 13:37:03,536 - __main__ - INFO - Model weights updated successfully
2025-04-14 13:37:04,402 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:37:04,443 - __main__ - INFO - Saved weights to /home/user/dpo/compo/temp_weights/current_lm_head.pt
2025-04-14 13:37:05,330 - __main__ - INFO - Saved lm_head weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 13:37:05,364 - __main__ - INFO - Saved temp weights to /home/user/dpo/compo/temp_weights/lm_head_weights_iter9.pt
2025-04-14 13:37:05,364 - __main__ - INFO - Skipping full model checkpoint for iteration 9
2025-04-14 13:37:05,364 - __main__ - INFO - Iteration 9 completed successfully
Updated temp weights file for next iteration
Transferring updated weights to worker nodes...
Worker node 1 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker1_weights.pt
Worker node 2 starting mode apply_weights for iteration 9 with weights file /home/user/dpo/compo/worker_weights/worker2_weights.pt
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 1 successfully applied weights for iteration 9
usage: compute_gradients.py [-h] --model_path MODEL_PATH --output_path
                            OUTPUT_PATH --grad_dir GRAD_DIR
                            [--noisy_batch_dir NOISY_BATCH_DIR]
                            [--weights_file WEIGHTS_FILE] [--run_all_iters]
                            [--step_size STEP_SIZE]
                            [--num_perturbation NUM_PERTURBATION]
                            [--batch_size BATCH_SIZE]
                            [--noise_threshold NOISE_THRESHOLD]
                            [--save_freq SAVE_FREQ] --node_rank NODE_RANK
                            --mode
                            {find_pairs,compute,aggregate,apply_weights,full}
                            --iteration ITERATION
                            [--dataset_offset DATASET_OFFSET]
                            [--checkpoint_iters CHECKPOINT_ITERS]
                            [--temp_weights_file TEMP_WEIGHTS_FILE]
compute_gradients.py: error: the following arguments are required: --grad_dir
Worker node 2 successfully applied weights for iteration 9
Iteration 9 completed successfully
Cleaning up iteration files...
Multi-node training completed successfully!
